{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermVectors:\n",
    "    def __init__(self):\n",
    "        self.termVectorDict=dict()\n",
    "        self.terms=None\n",
    "        self.vectors=None\n",
    "\n",
    "    def parse_term_embedding_file(self,file_path):\n",
    "        with open(file_path) as f:\n",
    "            lines=f.readlines()\n",
    "            self.vectors=lines[1:] # 第一个是term的数量，不是termID\n",
    "            terms = []\n",
    "            for line in lines[1:]:\n",
    "                term=line.split()[0]\n",
    "                terms.append(term)\n",
    "            self.terms = terms\n",
    "\n",
    "    def str_to_vector(self,Str):\n",
    "        Str = Str.strip('\\n')\n",
    "        nums = Str.split()\n",
    "        vec = []\n",
    "        for num in nums:\n",
    "            vec.append(float(num))\n",
    "        return np.array(vec)\n",
    "\n",
    "    def construct_term_vector_dict(self):\n",
    "        for term in self.terms:\n",
    "            termindex = self.terms.index(term)\n",
    "            line=self.vectors[termindex]\n",
    "            s =line[len(term):].lstrip(' ')\n",
    "            s_vec =self.str_to_vector(s)\n",
    "            self.termVectorDict[term]=s_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29699\n",
      "4202\n",
      "11148\n"
     ]
    }
   ],
   "source": [
    "# construct Node2Vec termVector\n",
    "BP_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/BP.emb'\n",
    "CC_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/CC.emb'\n",
    "MF_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/MF.emb'\n",
    "Node2Vec_dim = 300\n",
    "\n",
    "\n",
    "BPTermVectors = TermVectors()\n",
    "BPTermVectors.parse_term_embedding_file(BP_TERM_EMB_FILE_PATH)\n",
    "BPTermVectors.construct_term_vector_dict()\n",
    "print(len(BPTermVectors.termVectorDict))\n",
    "\n",
    "CCTermVectors = TermVectors()\n",
    "CCTermVectors.parse_term_embedding_file(CC_TERM_EMB_FILE_PATH)\n",
    "CCTermVectors.construct_term_vector_dict()\n",
    "print(len(CCTermVectors.termVectorDict))\n",
    "\n",
    "MFTermVectors = TermVectors()\n",
    "MFTermVectors.parse_term_embedding_file(MF_TERM_EMB_FILE_PATH)\n",
    "MFTermVectors.construct_term_vector_dict()\n",
    "print(len(MFTermVectors.termVectorDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read go.obo obtain ontology type\n",
    "id_type_dicts = {}\n",
    "obo_file = '../cross-species/go.obo'\n",
    "fp=open(obo_file,'r')\n",
    "obo_txt=fp.read()\n",
    "fp.close()\n",
    "obo_txt=obo_txt[obo_txt.find(\"[Term]\")-1:]\n",
    "obo_txt=obo_txt[:obo_txt.find(\"[Typedef]\")]\n",
    "# obo_dict=parse_obo_txt(obo_txt)\n",
    "id_type_dicts = {}\n",
    "for Term_txt in obo_txt.split(\"[Term]\\n\"):\n",
    "    if not Term_txt.strip():\n",
    "        continue\n",
    "    name = ''\n",
    "    ids = []\n",
    "    for line in Term_txt.splitlines():\n",
    "        if   line.startswith(\"id: \"):\n",
    "            ids.append(line[len(\"id: \"):])     \n",
    "        elif line.startswith(\"namespace: \"):\n",
    "             name=line[len(\"namespace: \"):]\n",
    "        elif line.startswith(\"alt_id: \"):\n",
    "            ids.append(line[len(\"alt_id: \"):])\n",
    "    \n",
    "    for t_id in ids:\n",
    "        id_type_dicts[t_id] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 512\n",
    "max_seq_len = 1000\n",
    "max_node_len = 256\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "\n",
    "protein2go = load_dict('MMprot2go.pkl')\n",
    "prot2nodevec = {}\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,Node2Vec_dim))\n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        if go.startswith('GO'):\n",
    "            if id_type_dicts[go] == 'biological_process':\n",
    "                termVectors = BPTermVectors\n",
    "                term_ID=go[3:].lstrip('0')\n",
    "                if term_ID in termVectors.termVectorDict.keys():\n",
    "                    feature = termVectors.termVectorDict[term_ID].reshape(1, Node2Vec_dim)\n",
    "                else:\n",
    "                    feature = np.zeros((1,Node2Vec_dim))\n",
    "                \n",
    "            elif id_type_dicts[go] == 'cellular_component':\n",
    "                termVectors = CCTermVectors\n",
    "                term_ID=go[3:].lstrip('0')\n",
    "                if term_ID in termVectors.termVectorDict.keys():\n",
    "                    feature = termVectors.termVectorDict[term_ID].reshape(1, Node2Vec_dim)\n",
    "                else:\n",
    "                    feature = np.zeros((1,Node2Vec_dim))\n",
    "            elif id_type_dicts[go] == 'molecular_function':\n",
    "                termVectors = MFTermVectors\n",
    "                term_ID=go[3:].lstrip('0')\n",
    "                if term_ID in termVectors.termVectorDict.keys():\n",
    "                    feature = termVectors.termVectorDict[term_ID].reshape(1, Node2Vec_dim)\n",
    "                else:\n",
    "                    feature = np.zeros((1,Node2Vec_dim))\n",
    "            else:\n",
    "                 feature = np.zeros((1,Node2Vec_dim))\n",
    "\n",
    "\n",
    "            if count + feature.shape[0] > max_node_len:\n",
    "                break\n",
    "            X_go1 = np.concatenate((X_go1,feature ))    \n",
    "            count += feature.shape[0]\n",
    "    prot2nodevec[key] =  X_go1[1:]  \n",
    "\n",
    "    \n",
    "    \n",
    "prot2emb = {}\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,768))\n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        if go.startswith('GO'):\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            if count + feature.shape[0] > max_go_len:\n",
    "                break\n",
    "            X_go1 = np.concatenate((X_go1,feature ))    \n",
    "            count += feature.shape[0]\n",
    "    prot2emb[key] =  X_go1[1:]  \n",
    "    \n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.max_node_len = max_node_len\n",
    "        self.protein2go =  load_dict('MMprot2go.pkl')\n",
    "        self.protein2seq = load_dict('MMprot2seq.pkl')\n",
    "        self.read_ppi()\n",
    "        self.prot2emb =  prot2emb\n",
    "#         self.prot2embedding()\n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.prot2nodevec = prot2nodevec\n",
    "#         self.prot2nodevec_fun()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "#     def prot2nodevec_fun(self):\n",
    "#         for key, value in self.protein2go.items():\n",
    "#             X_go1 =  np.zeros((1,Node2Vec_dim))\n",
    "#             allgos = value.split(',') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 if go.startswith('GO'):\n",
    "#                     if id_type_dicts[go] == '':\n",
    "#                         termVectors = BPTermVectors\n",
    "#                         term_ID=go[3:].lstrip('0')\n",
    "#                         feature = termVectors.termVectorDict[term_ID]\n",
    "#                     elif id_type_dicts[go] == '':\n",
    "#                         termVectors = CCTermVectors\n",
    "#                         term_ID=go[3:].lstrip('0')\n",
    "#                         feature = termVectors.termVectorDict[term_ID]\n",
    "#                     elif id_type_dicts[go] == '':\n",
    "#                         termVectors = MFTermVectors\n",
    "#                         term_ID=go[3:].lstrip('0')\n",
    "#                         feature = termVectors.termVectorDict[term_ID]\n",
    "#                     else:\n",
    "#                         feature = np.zeros((1,Node2Vec_dim))\n",
    "                    \n",
    "                     \n",
    "#                     if count + feature.shape[0] > max_go_len:\n",
    "#                         break\n",
    "#                     X_go1 = np.concatenate((X_go1,feature ))    \n",
    "#                     count += feature.shape[0]\n",
    "#             self.prot2nodevec[key] =  X_go1[1:]   \n",
    "        \n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "            \n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "#     def prot2embedding(self):\n",
    "#         for key, value in self.protein2go.items():\n",
    "#             X_go1 =  np.zeros((1,768))\n",
    "#             allgos = value.split(',') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 if go.startswith('GO'):\n",
    "#                     feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                     if count + feature.shape[0] > max_go_len:\n",
    "#                         break\n",
    "#                     X_go1 = np.concatenate((X_go1,feature ))    \n",
    "#                     count += feature.shape[0]\n",
    "#             self.prot2emb[key] =  X_go1[1:]   \n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        y = np.empty((self.batch_size))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "        X_node1 = np.empty((self.batch_size, self.max_node_len,Node2Vec_dim))\n",
    "        X_node2 = np.empty((self.batch_size, self.max_node_len,Node2Vec_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_node = self.prot2nodevec[p1]\n",
    "            X_node1[i,:prot1emb_node.shape[0]] = prot1emb_node\n",
    "            \n",
    "            prot2emb_node = self.prot2nodevec[p2]\n",
    "            X_node2[i,:prot2emb_node.shape[0]] = prot2emb_node\n",
    "            \n",
    "            \n",
    "            \n",
    "     \n",
    "        return [X_go1,X_go2,  X_node1, X_node2, X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "         \n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "        X_node1 = np.empty((len(list_IDs_temp), self.max_node_len,Node2Vec_dim))\n",
    "        X_node2 = np.empty((len(list_IDs_temp), self.max_node_len,Node2Vec_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_node = self.prot2nodevec[p1]\n",
    "            X_node1[i,:prot1emb_node.shape[0]] = prot1emb_node\n",
    "            \n",
    "            prot2emb_node = self.prot2nodevec[p2]\n",
    "            X_node2[i,:prot2emb_node.shape[0]] = prot2emb_node\n",
    "            \n",
    "  \n",
    "        return [X_go1,X_go2, X_node1, X_node2, X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 32)      73760       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 512, 32)      24608       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 512, 32)      73760       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 512, 32)      24608       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 256, 32)      28832       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 256, 32)      9632        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 256, 32)      28832       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 256, 32)      9632        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 32)     1952        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 32)     672         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 32)     1952        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 32)     672         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 512, 32)      5152        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 512, 32)      3104        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 512, 32)      73760       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 512, 32)      24608       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 512, 32)      5152        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 512, 32)      3104        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 32)      73760       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 32)      24608       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 256, 32)      5152        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 256, 32)      3104        conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 256, 32)      28832       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 256, 32)      9632        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 256, 32)      5152        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 256, 32)      3104        conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 256, 32)      28832       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 256, 32)      9632        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 32)     5152        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 32)     3104        conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 32)     1952        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 32)     672         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 32)     5152        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 32)     3104        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 32)     1952        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 32)     672         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512, 128)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 128)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256, 128)     0           conv1d_26[0][0]                  \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "                                                                 conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256, 128)     0           conv1d_32[0][0]                  \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "                                                                 conv1d_35[0][0]                  \n",
      "                                                                 conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 128)    0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 128)    0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 256, 128)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 256, 128)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 128, 128)     0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 128, 128)     0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 500, 128)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 500, 128)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256, 128)     0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256, 128)     0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128, 128)     0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 128, 128)     0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 500, 128)     0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 500, 128)     0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 256, 128)     74496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 256, 128)     74496       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 128, 128)     74496       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 128, 128)     74496       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 500, 128)     74496       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 500, 128)     74496       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 256, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128, 128)     0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 128, 128)     0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 500, 128)     0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 500, 128)     0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 128)          384         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          384         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 128)          384         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          384         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 128)          256         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          256         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 128)          256         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          256         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 128)          628         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          628         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 128)          628         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          628         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 768)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 768)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 768)          0           global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 768)          0           global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 768)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 768)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          196864      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          196864      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          196864      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          196864      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          196864      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1536)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         1573888     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         1049600     dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          524800      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            513         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,389,393\n",
      "Trainable params: 5,389,393\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "    y = MaxPooling1D(2)(y)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    " \n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([ x_a, x_b, x_c,  x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_cnn_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    \n",
    "    x = Concatenate()([ x_a, x_b, x_c])\n",
    "    x = Dense(256)(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "    left_input_node = Input(shape=(max_node_len,Node2Vec_dim))\n",
    "    right_input_node = Input(shape=(max_node_len,Node2Vec_dim))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "\n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters, gru_units)\n",
    "    \n",
    "    left_x_node = build_cnn_gru_model(left_input_node, con_filters, gru_units)\n",
    "    right_x_node = build_cnn_gru_model(right_input_node, con_filters,gru_units)\n",
    "    \n",
    "   \n",
    "    \n",
    "   \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go, left_x_node,  right_x_node, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go, left_input_node, right_input_node,     left_input_seq, right_input_seq], output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "# siamese_a = create_share_model()\n",
    "# siamese_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 1.0644 - acc: 0.4948 - val_loss: 0.7446 - val_acc: 0.4062\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 7s 615ms/step - loss: 0.7103 - acc: 0.5182 - val_loss: 0.6808 - val_acc: 0.6250\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 7s 603ms/step - loss: 0.6826 - acc: 0.5560 - val_loss: 0.7907 - val_acc: 0.4219\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 7s 592ms/step - loss: 0.6474 - acc: 0.6211 - val_loss: 1.3419 - val_acc: 0.4219\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 7s 621ms/step - loss: 0.5962 - acc: 0.6719 - val_loss: 0.7249 - val_acc: 0.5469\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.3885 - acc: 0.8125 - val_loss: 0.6483 - val_acc: 0.6719\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.3477 - acc: 0.8451 - val_loss: 1.2133 - val_acc: 0.4219\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 7s 592ms/step - loss: 0.7121 - acc: 0.6237 - val_loss: 0.7474 - val_acc: 0.4688\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 7s 595ms/step - loss: 0.3948 - acc: 0.8555 - val_loss: 0.6805 - val_acc: 0.6875\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.1590 - acc: 0.9375 - val_loss: 2.5547 - val_acc: 0.5781\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 0.3733 - acc: 0.8451 - val_loss: 0.7010 - val_acc: 0.6406\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 7s 604ms/step - loss: 0.1563 - acc: 0.9492 - val_loss: 1.1204 - val_acc: 0.6094\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.3148 - acc: 0.9049 - val_loss: 2.0159 - val_acc: 0.5625\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.5592 - acc: 0.8828 - val_loss: 0.6921 - val_acc: 0.5781\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.1256 - acc: 0.9583 - val_loss: 1.1821 - val_acc: 0.6406\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.1536 - acc: 0.9401 - val_loss: 0.6762 - val_acc: 0.5156\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 7s 602ms/step - loss: 0.0773 - acc: 0.9818 - val_loss: 0.9794 - val_acc: 0.5312\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.0240 - acc: 0.9935 - val_loss: 0.8603 - val_acc: 0.6094\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.4625 - acc: 0.8529 - val_loss: 1.0480 - val_acc: 0.7344\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 7s 606ms/step - loss: 0.2218 - acc: 0.9128 - val_loss: 1.1295 - val_acc: 0.4688\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 572ms/step - loss: 0.1946 - acc: 0.9154 - val_loss: 0.7615 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 7s 595ms/step - loss: 0.0772 - acc: 0.9753 - val_loss: 0.7979 - val_acc: 0.6562\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.0556 - acc: 0.9779 - val_loss: 1.0345 - val_acc: 0.6406\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.0576 - acc: 0.9857 - val_loss: 1.0448 - val_acc: 0.6250\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.0607 - acc: 0.9844 - val_loss: 1.1392 - val_acc: 0.6719\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 7s 578ms/step - loss: 0.0803 - acc: 0.9740 - val_loss: 1.1625 - val_acc: 0.6562\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.0281 - acc: 0.9935 - val_loss: 1.6981 - val_acc: 0.5000\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 7s 596ms/step - loss: 0.1787 - acc: 0.9414 - val_loss: 0.7479 - val_acc: 0.5938\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.0420 - acc: 0.9818 - val_loss: 2.2183 - val_acc: 0.7031\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 7s 575ms/step - loss: 0.1432 - acc: 0.9479 - val_loss: 1.0948 - val_acc: 0.4844\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 7s 595ms/step - loss: 0.1051 - acc: 0.9518 - val_loss: 1.3379 - val_acc: 0.5000\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.0218 - acc: 0.9909 - val_loss: 1.6832 - val_acc: 0.5156\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 8s 647ms/step - loss: 0.0194 - acc: 0.9961 - val_loss: 1.6688 - val_acc: 0.6562\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.0579 - acc: 0.9740 - val_loss: 1.4324 - val_acc: 0.5781\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.0689 - acc: 0.9714 - val_loss: 0.8137 - val_acc: 0.6719\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 7s 601ms/step - loss: 0.0674 - acc: 0.9740 - val_loss: 0.9373 - val_acc: 0.6875\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0396 - acc: 0.9831 - val_loss: 1.3466 - val_acc: 0.5781\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 0.0476 - acc: 0.9831 - val_loss: 1.0463 - val_acc: 0.6719\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.0904 - acc: 0.9648 - val_loss: 1.3568 - val_acc: 0.6719\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.965545\n",
      "ACC: 0.830000\n",
      "MCC : 0.687899\n",
      "TPR:0.666667\n",
      "FPR:0.019231\n",
      "Pre:0.969697\n",
      "F1:0.790123\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 21s 2s/step - loss: 1.1416 - acc: 0.5065 - val_loss: 0.6972 - val_acc: 0.4844\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 8s 630ms/step - loss: 0.6990 - acc: 0.5247 - val_loss: 0.7745 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.6387 - acc: 0.6341 - val_loss: 0.6897 - val_acc: 0.5156\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 7s 597ms/step - loss: 0.5928 - acc: 0.6953 - val_loss: 0.6767 - val_acc: 0.5625\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 7s 574ms/step - loss: 0.4644 - acc: 0.7695 - val_loss: 0.7236 - val_acc: 0.5781\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 7s 596ms/step - loss: 0.4393 - acc: 0.8060 - val_loss: 1.1438 - val_acc: 0.4844\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 7s 577ms/step - loss: 0.5243 - acc: 0.7461 - val_loss: 0.7414 - val_acc: 0.5469\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 7s 606ms/step - loss: 0.3195 - acc: 0.8724 - val_loss: 1.0644 - val_acc: 0.5625\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 7s 579ms/step - loss: 0.2769 - acc: 0.8906 - val_loss: 0.9453 - val_acc: 0.5312\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 7s 602ms/step - loss: 0.2440 - acc: 0.8945 - val_loss: 0.9855 - val_acc: 0.5938\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.1406 - acc: 0.9440 - val_loss: 1.2691 - val_acc: 0.5781\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 7s 557ms/step - loss: 0.2103 - acc: 0.9154 - val_loss: 1.1779 - val_acc: 0.5000\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.3199 - acc: 0.8763 - val_loss: 0.8189 - val_acc: 0.4844\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 7s 611ms/step - loss: 0.1192 - acc: 0.9531 - val_loss: 1.8478 - val_acc: 0.5625\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.1242 - acc: 0.9518 - val_loss: 1.6300 - val_acc: 0.4844\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 7s 579ms/step - loss: 0.1045 - acc: 0.9661 - val_loss: 1.2640 - val_acc: 0.4375\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0908 - acc: 0.9688 - val_loss: 1.6237 - val_acc: 0.4844\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 7s 616ms/step - loss: 0.0890 - acc: 0.9714 - val_loss: 1.3814 - val_acc: 0.5156\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.1416 - acc: 0.9453 - val_loss: 1.1491 - val_acc: 0.5000\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0766 - acc: 0.9701 - val_loss: 2.5114 - val_acc: 0.4844\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.0401 - acc: 0.9857 - val_loss: 1.5214 - val_acc: 0.5312\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 597ms/step - loss: 0.0329 - acc: 0.9883 - val_loss: 1.4322 - val_acc: 0.6406\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.0193 - acc: 0.9948 - val_loss: 1.6835 - val_acc: 0.5625\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 7s 584ms/step - loss: 0.0377 - acc: 0.9857 - val_loss: 1.6119 - val_acc: 0.6406\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0220 - acc: 0.9922 - val_loss: 1.1925 - val_acc: 0.6406\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 7s 574ms/step - loss: 0.1587 - acc: 0.9479 - val_loss: 0.9751 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 7s 571ms/step - loss: 0.0921 - acc: 0.9622 - val_loss: 2.4802 - val_acc: 0.5625\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.0701 - acc: 0.9779 - val_loss: 1.1952 - val_acc: 0.5312\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 7s 568ms/step - loss: 0.0236 - acc: 0.9922 - val_loss: 1.3179 - val_acc: 0.5938\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 7s 592ms/step - loss: 0.0571 - acc: 0.9844 - val_loss: 2.0879 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 7s 577ms/step - loss: 0.0466 - acc: 0.9818 - val_loss: 2.8471 - val_acc: 0.5156\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 7s 604ms/step - loss: 0.1760 - acc: 0.9440 - val_loss: 0.7881 - val_acc: 0.5781\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 8s 633ms/step - loss: 0.0513 - acc: 0.9831 - val_loss: 2.5124 - val_acc: 0.6406\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.1509 - acc: 0.9492 - val_loss: 1.3194 - val_acc: 0.5312\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0590 - acc: 0.9727 - val_loss: 2.2042 - val_acc: 0.5156\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 7s 594ms/step - loss: 0.0687 - acc: 0.9766 - val_loss: 2.2838 - val_acc: 0.5781\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 7s 610ms/step - loss: 0.0291 - acc: 0.9922 - val_loss: 2.8430 - val_acc: 0.5938\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0815 - acc: 0.9701 - val_loss: 2.4688 - val_acc: 0.6094\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.1057 - acc: 0.9648 - val_loss: 2.1144 - val_acc: 0.6094\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 7s 581ms/step - loss: 0.0512 - acc: 0.9896 - val_loss: 1.8237 - val_acc: 0.5312\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.0126 - acc: 0.9948 - val_loss: 2.0950 - val_acc: 0.5469\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.2246 - acc: 0.9193 - val_loss: 1.3500 - val_acc: 0.5781\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.907600\n",
      "ACC: 0.880000\n",
      "MCC : 0.760000\n",
      "TPR:0.880000\n",
      "FPR:0.120000\n",
      "Pre:0.880000\n",
      "F1:0.880000\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 23s 2s/step - loss: 0.9855 - acc: 0.5312 - val_loss: 0.6926 - val_acc: 0.5469\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.7272 - acc: 0.5117 - val_loss: 0.6310 - val_acc: 0.6719\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 7s 613ms/step - loss: 0.6929 - acc: 0.5495 - val_loss: 0.6942 - val_acc: 0.5625\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 7s 600ms/step - loss: 0.6068 - acc: 0.6784 - val_loss: 0.6783 - val_acc: 0.5781\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 7s 592ms/step - loss: 0.4670 - acc: 0.7591 - val_loss: 0.5530 - val_acc: 0.7344\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.3851 - acc: 0.8359 - val_loss: 0.5674 - val_acc: 0.6875\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.3454 - acc: 0.8503 - val_loss: 0.6467 - val_acc: 0.6562\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.2318 - acc: 0.9193 - val_loss: 1.1097 - val_acc: 0.6406\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.1390 - acc: 0.9414 - val_loss: 0.8215 - val_acc: 0.7188\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 7s 606ms/step - loss: 0.1429 - acc: 0.9492 - val_loss: 1.9936 - val_acc: 0.6719\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.6112 - acc: 0.7891 - val_loss: 0.6293 - val_acc: 0.6562\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.3291 - acc: 0.8529 - val_loss: 0.7725 - val_acc: 0.6719\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.1107 - acc: 0.9570 - val_loss: 1.5365 - val_acc: 0.6250\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.1650 - acc: 0.9310 - val_loss: 3.5379 - val_acc: 0.4844\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 7s 579ms/step - loss: 0.5719 - acc: 0.7695 - val_loss: 0.9228 - val_acc: 0.5781\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 7s 598ms/step - loss: 0.3937 - acc: 0.8125 - val_loss: 0.7003 - val_acc: 0.6719\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 7s 598ms/step - loss: 0.3201 - acc: 0.8659 - val_loss: 0.6448 - val_acc: 0.6719\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 7s 595ms/step - loss: 0.2204 - acc: 0.8906 - val_loss: 1.6812 - val_acc: 0.6562\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.0436 - acc: 0.9870 - val_loss: 1.9314 - val_acc: 0.6719\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.1373 - acc: 0.9505 - val_loss: 1.3396 - val_acc: 0.6094\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 594ms/step - loss: 0.0916 - acc: 0.9635 - val_loss: 0.7454 - val_acc: 0.7031\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0920 - acc: 0.9701 - val_loss: 0.9070 - val_acc: 0.6719\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.0402 - acc: 0.9870 - val_loss: 1.8756 - val_acc: 0.6562\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.0193 - acc: 0.9896 - val_loss: 1.5649 - val_acc: 0.6250\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.1056 - acc: 0.9714 - val_loss: 1.0981 - val_acc: 0.5781\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.899758\n",
      "ACC: 0.800000\n",
      "MCC : 0.605624\n",
      "TPR:0.847826\n",
      "FPR:0.240741\n",
      "Pre:0.750000\n",
      "F1:0.795918\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 25s 2s/step - loss: 1.1150 - acc: 0.5039 - val_loss: 0.7181 - val_acc: 0.4219\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 7s 581ms/step - loss: 0.7228 - acc: 0.5182 - val_loss: 0.6809 - val_acc: 0.6094\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 7s 618ms/step - loss: 0.6900 - acc: 0.5169 - val_loss: 0.7094 - val_acc: 0.4062\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 7s 612ms/step - loss: 0.6484 - acc: 0.6211 - val_loss: 0.5786 - val_acc: 0.7500\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 8s 652ms/step - loss: 0.6066 - acc: 0.6849 - val_loss: 0.7319 - val_acc: 0.4688\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.5710 - acc: 0.6810 - val_loss: 0.6313 - val_acc: 0.5781\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 7s 597ms/step - loss: 0.4170 - acc: 0.8190 - val_loss: 0.5874 - val_acc: 0.7500\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 7s 611ms/step - loss: 0.4527 - acc: 0.7799 - val_loss: 0.7377 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 7s 569ms/step - loss: 0.4086 - acc: 0.7865 - val_loss: 0.5566 - val_acc: 0.7031\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 7s 563ms/step - loss: 0.2456 - acc: 0.8815 - val_loss: 0.7490 - val_acc: 0.7031\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 7s 551ms/step - loss: 0.2056 - acc: 0.9128 - val_loss: 0.6440 - val_acc: 0.7656\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 7s 568ms/step - loss: 0.1441 - acc: 0.9505 - val_loss: 0.9558 - val_acc: 0.6719\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 8s 629ms/step - loss: 0.1250 - acc: 0.9531 - val_loss: 1.1383 - val_acc: 0.5781\n",
      "Epoch 14/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 632ms/step - loss: 0.1959 - acc: 0.9219 - val_loss: 0.6571 - val_acc: 0.7656\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 7s 621ms/step - loss: 0.2285 - acc: 0.9219 - val_loss: 0.5513 - val_acc: 0.7188\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 7s 609ms/step - loss: 0.1376 - acc: 0.9518 - val_loss: 1.7264 - val_acc: 0.5469\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 7s 619ms/step - loss: 0.1534 - acc: 0.9401 - val_loss: 0.9092 - val_acc: 0.5938\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0945 - acc: 0.9635 - val_loss: 1.0673 - val_acc: 0.6094\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 604ms/step - loss: 0.1352 - acc: 0.9453 - val_loss: 0.8520 - val_acc: 0.6719\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 7s 579ms/step - loss: 0.0544 - acc: 0.9753 - val_loss: 0.9773 - val_acc: 0.6875\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.0540 - acc: 0.9831 - val_loss: 1.0202 - val_acc: 0.6875\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.0261 - acc: 0.9909 - val_loss: 2.0032 - val_acc: 0.6094\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.0590 - acc: 0.9857 - val_loss: 0.8373 - val_acc: 0.7500\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0507 - acc: 0.9870 - val_loss: 1.1254 - val_acc: 0.6719\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 584ms/step - loss: 0.0789 - acc: 0.9648 - val_loss: 1.1996 - val_acc: 0.7344\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 7s 561ms/step - loss: 0.2021 - acc: 0.9154 - val_loss: 0.6186 - val_acc: 0.7344\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.0663 - acc: 0.9740 - val_loss: 1.0060 - val_acc: 0.6719\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 7s 565ms/step - loss: 0.0525 - acc: 0.9766 - val_loss: 1.1302 - val_acc: 0.6250\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.1216 - acc: 0.9492 - val_loss: 0.9791 - val_acc: 0.7344\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 7s 557ms/step - loss: 0.0932 - acc: 0.9701 - val_loss: 1.3196 - val_acc: 0.6094\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.1033 - acc: 0.9792 - val_loss: 2.7875 - val_acc: 0.5469\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.905333\n",
      "ACC: 0.870000\n",
      "MCC : 0.735645\n",
      "TPR:0.878049\n",
      "FPR:0.135593\n",
      "Pre:0.818182\n",
      "F1:0.847059\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 29s 2s/step - loss: 0.9922 - acc: 0.5039 - val_loss: 0.7296 - val_acc: 0.5156\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 9s 785ms/step - loss: 0.7011 - acc: 0.5430 - val_loss: 0.6981 - val_acc: 0.5156\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 9s 785ms/step - loss: 0.6585 - acc: 0.5781 - val_loss: 0.6871 - val_acc: 0.5781\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 9s 762ms/step - loss: 0.6304 - acc: 0.6406 - val_loss: 0.6975 - val_acc: 0.5938\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 9s 777ms/step - loss: 0.5294 - acc: 0.7253 - val_loss: 0.6237 - val_acc: 0.6094\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 9s 791ms/step - loss: 0.4326 - acc: 0.7969 - val_loss: 0.6741 - val_acc: 0.6250\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 9s 755ms/step - loss: 0.4102 - acc: 0.8086 - val_loss: 0.9219 - val_acc: 0.5781\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 9s 734ms/step - loss: 0.3411 - acc: 0.8464 - val_loss: 0.7754 - val_acc: 0.5781\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 9s 711ms/step - loss: 0.2692 - acc: 0.8776 - val_loss: 0.9340 - val_acc: 0.6250\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 9s 783ms/step - loss: 0.3658 - acc: 0.8438 - val_loss: 0.9355 - val_acc: 0.5156\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 9s 763ms/step - loss: 0.1755 - acc: 0.9375 - val_loss: 2.4227 - val_acc: 0.5625\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 9s 773ms/step - loss: 0.1632 - acc: 0.9375 - val_loss: 1.5092 - val_acc: 0.5469\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 9s 751ms/step - loss: 0.1266 - acc: 0.9505 - val_loss: 0.7781 - val_acc: 0.6875\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 9s 785ms/step - loss: 0.0635 - acc: 0.9766 - val_loss: 2.3373 - val_acc: 0.5312\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 9s 749ms/step - loss: 0.0504 - acc: 0.9779 - val_loss: 2.0765 - val_acc: 0.5625\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 7s 559ms/step - loss: 0.2812 - acc: 0.9049 - val_loss: 1.0084 - val_acc: 0.6406\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 7s 578ms/step - loss: 0.0809 - acc: 0.9740 - val_loss: 1.5946 - val_acc: 0.6719\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 7s 558ms/step - loss: 0.4654 - acc: 0.8385 - val_loss: 0.6054 - val_acc: 0.6719\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 579ms/step - loss: 0.1543 - acc: 0.9479 - val_loss: 1.1295 - val_acc: 0.6875\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 7s 592ms/step - loss: 0.0616 - acc: 0.9805 - val_loss: 1.2075 - val_acc: 0.6719\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 570ms/step - loss: 0.0724 - acc: 0.9714 - val_loss: 1.1261 - val_acc: 0.5781\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 7s 548ms/step - loss: 0.1054 - acc: 0.9609 - val_loss: 0.9939 - val_acc: 0.6562\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 7s 552ms/step - loss: 0.0469 - acc: 0.9805 - val_loss: 1.2600 - val_acc: 0.7031\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 7s 594ms/step - loss: 0.0317 - acc: 0.9870 - val_loss: 1.7703 - val_acc: 0.6562\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 574ms/step - loss: 0.0232 - acc: 0.9909 - val_loss: 1.2585 - val_acc: 0.6406\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 7s 554ms/step - loss: 0.0248 - acc: 0.9948 - val_loss: 1.2755 - val_acc: 0.6250\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 7s 576ms/step - loss: 0.0304 - acc: 0.9909 - val_loss: 1.8724 - val_acc: 0.6719\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 7s 560ms/step - loss: 0.0381 - acc: 0.9870 - val_loss: 1.2392 - val_acc: 0.6094\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 7s 554ms/step - loss: 0.0740 - acc: 0.9688 - val_loss: 1.1323 - val_acc: 0.7031\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 7s 577ms/step - loss: 0.0542 - acc: 0.9805 - val_loss: 2.0646 - val_acc: 0.5938\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 7s 573ms/step - loss: 0.0763 - acc: 0.9766 - val_loss: 1.0728 - val_acc: 0.6406\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 7s 552ms/step - loss: 0.0449 - acc: 0.9818 - val_loss: 1.4820 - val_acc: 0.6406\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 7s 555ms/step - loss: 0.0181 - acc: 0.9935 - val_loss: 1.5936 - val_acc: 0.6562\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 6s 531ms/step - loss: 0.0190 - acc: 0.9935 - val_loss: 1.9537 - val_acc: 0.6094\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 6s 523ms/step - loss: 0.1439 - acc: 0.9570 - val_loss: 1.3888 - val_acc: 0.6094\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 6s 541ms/step - loss: 0.0451 - acc: 0.9909 - val_loss: 1.6249 - val_acc: 0.6094\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 7s 567ms/step - loss: 0.0154 - acc: 0.9935 - val_loss: 2.1143 - val_acc: 0.6406\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 7s 544ms/step - loss: 0.0202 - acc: 0.9948 - val_loss: 2.1860 - val_acc: 0.5938\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 7s 550ms/step - loss: 0.0128 - acc: 0.9948 - val_loss: 2.3357 - val_acc: 0.6250\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 6s 538ms/step - loss: 0.0883 - acc: 0.9661 - val_loss: 1.4641 - val_acc: 0.5781\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 6s 531ms/step - loss: 0.0256 - acc: 0.9948 - val_loss: 2.0084 - val_acc: 0.5781\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 6s 539ms/step - loss: 0.0191 - acc: 0.9909 - val_loss: 2.3597 - val_acc: 0.6719\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 6s 509ms/step - loss: 0.0434 - acc: 0.9883 - val_loss: 1.6780 - val_acc: 0.6406\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.969400\n",
      "ACC: 0.940000\n",
      "MCC : 0.881858\n",
      "TPR:0.976744\n",
      "FPR:0.087719\n",
      "Pre:0.893617\n",
      "F1:0.933333\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 28s 2s/step - loss: 1.0055 - acc: 0.5000 - val_loss: 0.6865 - val_acc: 0.5312\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.7164 - acc: 0.5078 - val_loss: 0.7185 - val_acc: 0.5312\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 7s 577ms/step - loss: 0.6831 - acc: 0.5443 - val_loss: 0.6695 - val_acc: 0.5625\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 7s 600ms/step - loss: 0.6165 - acc: 0.6758 - val_loss: 0.6232 - val_acc: 0.6562\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 7s 565ms/step - loss: 0.5897 - acc: 0.6927 - val_loss: 0.5988 - val_acc: 0.6250\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 6s 501ms/step - loss: 0.4579 - acc: 0.7917 - val_loss: 1.1778 - val_acc: 0.5781\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 6s 461ms/step - loss: 0.4201 - acc: 0.8164 - val_loss: 0.5891 - val_acc: 0.6406\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.2399 - acc: 0.9062 - val_loss: 0.7772 - val_acc: 0.6562\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.2023 - acc: 0.9102 - val_loss: 0.9494 - val_acc: 0.6094\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 7s 571ms/step - loss: 0.1869 - acc: 0.9180 - val_loss: 0.8615 - val_acc: 0.6406\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.1676 - acc: 0.9310 - val_loss: 0.9135 - val_acc: 0.5938\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 7s 613ms/step - loss: 0.1965 - acc: 0.9141 - val_loss: 1.1061 - val_acc: 0.5625\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 7s 578ms/step - loss: 0.2148 - acc: 0.9115 - val_loss: 0.9471 - val_acc: 0.5938\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 7s 551ms/step - loss: 0.0923 - acc: 0.9648 - val_loss: 1.0803 - val_acc: 0.5938\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 6s 506ms/step - loss: 0.0549 - acc: 0.9792 - val_loss: 1.2363 - val_acc: 0.6094\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 6s 494ms/step - loss: 0.0322 - acc: 0.9935 - val_loss: 1.5699 - val_acc: 0.5781\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 6s 480ms/step - loss: 0.0780 - acc: 0.9740 - val_loss: 1.3861 - val_acc: 0.5781\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 6s 509ms/step - loss: 0.0983 - acc: 0.9531 - val_loss: 1.3646 - val_acc: 0.6406\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 603ms/step - loss: 0.0521 - acc: 0.9831 - val_loss: 1.2710 - val_acc: 0.7031\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.0148 - acc: 0.9948 - val_loss: 1.7299 - val_acc: 0.5938\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 603ms/step - loss: 0.1076 - acc: 0.9583 - val_loss: 1.5329 - val_acc: 0.5781\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 7s 569ms/step - loss: 0.2306 - acc: 0.9089 - val_loss: 1.3981 - val_acc: 0.6250\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 6s 508ms/step - loss: 0.1356 - acc: 0.9570 - val_loss: 1.7749 - val_acc: 0.5625\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 6s 495ms/step - loss: 0.0796 - acc: 0.9661 - val_loss: 1.2654 - val_acc: 0.6406\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 557ms/step - loss: 0.0531 - acc: 0.9779 - val_loss: 1.7036 - val_acc: 0.6094\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 7s 582ms/step - loss: 0.1861 - acc: 0.9505 - val_loss: 0.7342 - val_acc: 0.6719\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 6s 535ms/step - loss: 0.0439 - acc: 0.9870 - val_loss: 1.2319 - val_acc: 0.6719\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 7s 603ms/step - loss: 0.0164 - acc: 0.9961 - val_loss: 1.9355 - val_acc: 0.6406\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 7s 584ms/step - loss: 0.1505 - acc: 0.9635 - val_loss: 0.7388 - val_acc: 0.6406\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.0209 - acc: 0.9948 - val_loss: 1.5445 - val_acc: 0.6250\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0851 - acc: 0.9688 - val_loss: 1.0389 - val_acc: 0.6406\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0962 - acc: 0.9635 - val_loss: 0.9327 - val_acc: 0.6562\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0258 - acc: 0.9883 - val_loss: 2.4990 - val_acc: 0.6250\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 0.0134 - acc: 0.9948 - val_loss: 2.5133 - val_acc: 0.6094\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 7s 576ms/step - loss: 0.0443 - acc: 0.9870 - val_loss: 2.0982 - val_acc: 0.6250\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0186 - acc: 0.9948 - val_loss: 1.0550 - val_acc: 0.6562\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.0690 - acc: 0.9727 - val_loss: 1.8768 - val_acc: 0.5938\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0403 - acc: 0.9818 - val_loss: 1.2082 - val_acc: 0.6719\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 0.0199 - acc: 0.9922 - val_loss: 2.3556 - val_acc: 0.6719\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.975758\n",
      "ACC: 0.940000\n",
      "MCC : 0.879020\n",
      "TPR:0.963636\n",
      "FPR:0.088889\n",
      "Pre:0.929825\n",
      "F1:0.946429\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 30s 3s/step - loss: 1.1516 - acc: 0.4974 - val_loss: 0.7250 - val_acc: 0.4062\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 7s 602ms/step - loss: 0.7086 - acc: 0.5378 - val_loss: 0.6700 - val_acc: 0.5938\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.7167 - acc: 0.5078 - val_loss: 0.7162 - val_acc: 0.4219\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.6568 - acc: 0.5964 - val_loss: 0.7193 - val_acc: 0.5312\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.5702 - acc: 0.7031 - val_loss: 0.5982 - val_acc: 0.6719\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.4788 - acc: 0.7773 - val_loss: 1.1452 - val_acc: 0.5938\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.5922 - acc: 0.7044 - val_loss: 0.6427 - val_acc: 0.5938\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.3534 - acc: 0.8451 - val_loss: 1.1568 - val_acc: 0.6250\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.2469 - acc: 0.9102 - val_loss: 0.9995 - val_acc: 0.5781\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 7s 605ms/step - loss: 0.3595 - acc: 0.8464 - val_loss: 0.6004 - val_acc: 0.6719\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 7s 595ms/step - loss: 0.3846 - acc: 0.8672 - val_loss: 0.6557 - val_acc: 0.6406\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 7s 594ms/step - loss: 0.2908 - acc: 0.9154 - val_loss: 0.9193 - val_acc: 0.6094\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 7s 600ms/step - loss: 0.1900 - acc: 0.9284 - val_loss: 1.4882 - val_acc: 0.5312\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 7s 602ms/step - loss: 0.3994 - acc: 0.8620 - val_loss: 1.1349 - val_acc: 0.5469\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.2924 - acc: 0.8633 - val_loss: 1.0075 - val_acc: 0.6094\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 0.1013 - acc: 0.9661 - val_loss: 1.5136 - val_acc: 0.6250\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 7s 598ms/step - loss: 0.4725 - acc: 0.8216 - val_loss: 0.7068 - val_acc: 0.6719\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 7s 596ms/step - loss: 0.2046 - acc: 0.9193 - val_loss: 1.1491 - val_acc: 0.6406\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.3448 - acc: 0.9010 - val_loss: 0.9082 - val_acc: 0.5625\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 581ms/step - loss: 0.0770 - acc: 0.9727 - val_loss: 0.8741 - val_acc: 0.6562\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.0425 - acc: 0.9831 - val_loss: 2.2882 - val_acc: 0.6562\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.0267 - acc: 0.9896 - val_loss: 1.8343 - val_acc: 0.6562\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 7s 594ms/step - loss: 0.0356 - acc: 0.9883 - val_loss: 1.4463 - val_acc: 0.6406\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 7s 596ms/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.9772 - val_acc: 0.7188\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 624ms/step - loss: 0.0153 - acc: 0.9948 - val_loss: 1.4919 - val_acc: 0.6562\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 0.0223 - acc: 0.9922 - val_loss: 1.0768 - val_acc: 0.6719\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 7s 596ms/step - loss: 0.0711 - acc: 0.9740 - val_loss: 1.2418 - val_acc: 0.7031\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 7s 581ms/step - loss: 0.0993 - acc: 0.9544 - val_loss: 1.2011 - val_acc: 0.6719\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 7s 598ms/step - loss: 0.0887 - acc: 0.9648 - val_loss: 1.7544 - val_acc: 0.5469\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.1012 - acc: 0.9583 - val_loss: 1.8324 - val_acc: 0.5781\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0601 - acc: 0.9792 - val_loss: 1.3026 - val_acc: 0.6875\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.0402 - acc: 0.9844 - val_loss: 1.5929 - val_acc: 0.5469\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 7s 587ms/step - loss: 0.0166 - acc: 0.9922 - val_loss: 2.1202 - val_acc: 0.6094\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 7s 591ms/step - loss: 0.0289 - acc: 0.9883 - val_loss: 2.2196 - val_acc: 0.6406\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 7s 592ms/step - loss: 0.0102 - acc: 0.9961 - val_loss: 1.3732 - val_acc: 0.6250\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 7s 590ms/step - loss: 0.0149 - acc: 0.9922 - val_loss: 2.0659 - val_acc: 0.6406\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 7s 584ms/step - loss: 0.0216 - acc: 0.9909 - val_loss: 1.5903 - val_acc: 0.6406\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0146 - acc: 0.9935 - val_loss: 1.7659 - val_acc: 0.6250\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 7s 578ms/step - loss: 0.0119 - acc: 0.9948 - val_loss: 1.6177 - val_acc: 0.6094\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.0328 - acc: 0.9909 - val_loss: 1.0651 - val_acc: 0.6250\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.0860 - acc: 0.9753 - val_loss: 1.8692 - val_acc: 0.5938\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 7s 592ms/step - loss: 0.0766 - acc: 0.9701 - val_loss: 1.5880 - val_acc: 0.6250\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.0815 - acc: 0.9727 - val_loss: 2.3478 - val_acc: 0.5469\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 7s 595ms/step - loss: 0.0698 - acc: 0.9753 - val_loss: 1.1345 - val_acc: 0.6094\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.981971\n",
      "ACC: 0.940000\n",
      "MCC : 0.879808\n",
      "TPR:0.942308\n",
      "FPR:0.062500\n",
      "Pre:0.942308\n",
      "F1:0.942308\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 1.0319 - acc: 0.5156 - val_loss: 0.6950 - val_acc: 0.4844\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 7s 594ms/step - loss: 0.7103 - acc: 0.5013 - val_loss: 0.6843 - val_acc: 0.5625\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.6729 - acc: 0.5755 - val_loss: 0.6679 - val_acc: 0.5469\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.5705 - acc: 0.7057 - val_loss: 1.0061 - val_acc: 0.5625\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 7s 578ms/step - loss: 0.5680 - acc: 0.7044 - val_loss: 0.5992 - val_acc: 0.6094\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 7s 588ms/step - loss: 0.4401 - acc: 0.7956 - val_loss: 0.7291 - val_acc: 0.7031\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.4825 - acc: 0.7760 - val_loss: 0.7847 - val_acc: 0.4531\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 7s 573ms/step - loss: 0.4410 - acc: 0.7695 - val_loss: 0.7042 - val_acc: 0.5938\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.3356 - acc: 0.8542 - val_loss: 0.7380 - val_acc: 0.6562\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 7s 563ms/step - loss: 0.2723 - acc: 0.8841 - val_loss: 0.6918 - val_acc: 0.5781\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 7s 583ms/step - loss: 0.1708 - acc: 0.9388 - val_loss: 1.2437 - val_acc: 0.5938\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 7s 586ms/step - loss: 0.0790 - acc: 0.9688 - val_loss: 1.2332 - val_acc: 0.6406\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 7s 584ms/step - loss: 0.0371 - acc: 0.9844 - val_loss: 1.6075 - val_acc: 0.5938\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 7s 578ms/step - loss: 0.3340 - acc: 0.8893 - val_loss: 1.6011 - val_acc: 0.5156\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 7s 575ms/step - loss: 0.3827 - acc: 0.8281 - val_loss: 0.7985 - val_acc: 0.5938\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 7s 553ms/step - loss: 0.1105 - acc: 0.9674 - val_loss: 2.1921 - val_acc: 0.6562\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 7s 593ms/step - loss: 0.0434 - acc: 0.9870 - val_loss: 2.4407 - val_acc: 0.6094\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 7s 573ms/step - loss: 0.0719 - acc: 0.9844 - val_loss: 1.9624 - val_acc: 0.5938\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 7s 576ms/step - loss: 0.2536 - acc: 0.9049 - val_loss: 0.8847 - val_acc: 0.5938\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 7s 565ms/step - loss: 0.2028 - acc: 0.9310 - val_loss: 1.1356 - val_acc: 0.5781\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 7s 595ms/step - loss: 0.0531 - acc: 0.9779 - val_loss: 2.0146 - val_acc: 0.6094\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 7s 575ms/step - loss: 0.1210 - acc: 0.9635 - val_loss: 1.5811 - val_acc: 0.7031\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 7s 567ms/step - loss: 0.1530 - acc: 0.9401 - val_loss: 0.8841 - val_acc: 0.6562\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 7s 602ms/step - loss: 0.0698 - acc: 0.9753 - val_loss: 1.1040 - val_acc: 0.6406\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 7s 589ms/step - loss: 0.0256 - acc: 0.9922 - val_loss: 1.6421 - val_acc: 0.7344\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 7s 594ms/step - loss: 0.0506 - acc: 0.9805 - val_loss: 1.4013 - val_acc: 0.6250\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 7s 573ms/step - loss: 0.0713 - acc: 0.9740 - val_loss: 1.8010 - val_acc: 0.6875\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 7s 565ms/step - loss: 0.1015 - acc: 0.9622 - val_loss: 1.0311 - val_acc: 0.6094\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 7s 572ms/step - loss: 0.0559 - acc: 0.9805 - val_loss: 2.2193 - val_acc: 0.6406\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 7s 576ms/step - loss: 0.0254 - acc: 0.9883 - val_loss: 1.9786 - val_acc: 0.6719\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 7s 575ms/step - loss: 0.0370 - acc: 0.9857 - val_loss: 1.2926 - val_acc: 0.6250\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 7s 577ms/step - loss: 0.0271 - acc: 0.9909 - val_loss: 1.7133 - val_acc: 0.6250\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 7s 575ms/step - loss: 0.0223 - acc: 0.9909 - val_loss: 1.3776 - val_acc: 0.6250\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 7s 584ms/step - loss: 0.0161 - acc: 0.9961 - val_loss: 2.3589 - val_acc: 0.6562\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.0135 - acc: 0.9922 - val_loss: 2.0008 - val_acc: 0.6250\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 576ms/step - loss: 0.0132 - acc: 0.9974 - val_loss: 1.9173 - val_acc: 0.6406\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.0846 - acc: 0.9779 - val_loss: 1.1702 - val_acc: 0.6406\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 7s 570ms/step - loss: 0.0369 - acc: 0.9896 - val_loss: 1.1700 - val_acc: 0.6406\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 7s 585ms/step - loss: 0.0262 - acc: 0.9909 - val_loss: 1.8167 - val_acc: 0.6094\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 7s 580ms/step - loss: 0.0958 - acc: 0.9714 - val_loss: 1.9249 - val_acc: 0.6094\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 7s 562ms/step - loss: 0.0871 - acc: 0.9740 - val_loss: 1.3023 - val_acc: 0.6094\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 7s 573ms/step - loss: 0.0229 - acc: 0.9922 - val_loss: 1.6296 - val_acc: 0.6094\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 7s 581ms/step - loss: 0.0136 - acc: 0.9974 - val_loss: 1.7036 - val_acc: 0.6719\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 7s 578ms/step - loss: 0.0176 - acc: 0.9935 - val_loss: 2.0726 - val_acc: 0.5781\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 7s 577ms/step - loss: 0.0850 - acc: 0.9779 - val_loss: 1.0278 - val_acc: 0.6406\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.975000\n",
      "ACC: 0.910000\n",
      "MCC : 0.811754\n",
      "TPR:0.950000\n",
      "FPR:0.150000\n",
      "Pre:0.904762\n",
      "F1:0.926829\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 33s 3s/step - loss: 0.9709 - acc: 0.4779 - val_loss: 0.6946 - val_acc: 0.5156\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 6s 489ms/step - loss: 0.6923 - acc: 0.5273 - val_loss: 0.7155 - val_acc: 0.5469\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 5s 414ms/step - loss: 0.6909 - acc: 0.5352 - val_loss: 0.6775 - val_acc: 0.5312\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 5s 444ms/step - loss: 0.6385 - acc: 0.6406 - val_loss: 0.6135 - val_acc: 0.6250\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 5s 439ms/step - loss: 0.6053 - acc: 0.6680 - val_loss: 0.8127 - val_acc: 0.4844\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 5s 408ms/step - loss: 0.5342 - acc: 0.7344 - val_loss: 0.5397 - val_acc: 0.6719\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 5s 433ms/step - loss: 0.4293 - acc: 0.8138 - val_loss: 0.7540 - val_acc: 0.6250\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 5s 403ms/step - loss: 0.4783 - acc: 0.7669 - val_loss: 0.8778 - val_acc: 0.5156\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 5s 406ms/step - loss: 0.3341 - acc: 0.8685 - val_loss: 0.5916 - val_acc: 0.7656\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 5s 396ms/step - loss: 0.2275 - acc: 0.9062 - val_loss: 0.5937 - val_acc: 0.7344\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 5s 404ms/step - loss: 0.1607 - acc: 0.9323 - val_loss: 1.2426 - val_acc: 0.5000\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 5s 406ms/step - loss: 0.1038 - acc: 0.9544 - val_loss: 1.1293 - val_acc: 0.6250\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 5s 411ms/step - loss: 0.1851 - acc: 0.9258 - val_loss: 0.6026 - val_acc: 0.7344\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 5s 421ms/step - loss: 0.0979 - acc: 0.9622 - val_loss: 0.7175 - val_acc: 0.7188\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 5s 407ms/step - loss: 0.0754 - acc: 0.9714 - val_loss: 0.8578 - val_acc: 0.7188\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 5s 400ms/step - loss: 0.1470 - acc: 0.9479 - val_loss: 0.6635 - val_acc: 0.7188\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 5s 399ms/step - loss: 0.1149 - acc: 0.9596 - val_loss: 0.9266 - val_acc: 0.7188\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 5s 404ms/step - loss: 0.1451 - acc: 0.9518 - val_loss: 0.5292 - val_acc: 0.8125\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 5s 407ms/step - loss: 0.0745 - acc: 0.9648 - val_loss: 0.8714 - val_acc: 0.7500\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 5s 428ms/step - loss: 0.0909 - acc: 0.9648 - val_loss: 0.7293 - val_acc: 0.7656\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 5s 395ms/step - loss: 0.0745 - acc: 0.9701 - val_loss: 1.0420 - val_acc: 0.7031\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 5s 403ms/step - loss: 0.0596 - acc: 0.9831 - val_loss: 1.1026 - val_acc: 0.7031\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 5s 393ms/step - loss: 0.0982 - acc: 0.9622 - val_loss: 1.2151 - val_acc: 0.6875\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 5s 411ms/step - loss: 0.2675 - acc: 0.9076 - val_loss: 0.6591 - val_acc: 0.7500\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 5s 406ms/step - loss: 0.1318 - acc: 0.9505 - val_loss: 0.9146 - val_acc: 0.7188\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 5s 394ms/step - loss: 0.1402 - acc: 0.9518 - val_loss: 0.5976 - val_acc: 0.7500\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 5s 397ms/step - loss: 0.0507 - acc: 0.9857 - val_loss: 0.9572 - val_acc: 0.7188\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 5s 398ms/step - loss: 0.0433 - acc: 0.9844 - val_loss: 1.0440 - val_acc: 0.7188\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 5s 401ms/step - loss: 0.1178 - acc: 0.9596 - val_loss: 0.6757 - val_acc: 0.7031\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 5s 440ms/step - loss: 0.0371 - acc: 0.9896 - val_loss: 0.9982 - val_acc: 0.7031\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 5s 417ms/step - loss: 0.0314 - acc: 0.9896 - val_loss: 1.1255 - val_acc: 0.7500\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 5s 392ms/step - loss: 0.0627 - acc: 0.9779 - val_loss: 1.0504 - val_acc: 0.7188\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 5s 416ms/step - loss: 0.0537 - acc: 0.9818 - val_loss: 0.8134 - val_acc: 0.7500\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 5s 445ms/step - loss: 0.0542 - acc: 0.9870 - val_loss: 0.9138 - val_acc: 0.7031\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 5s 437ms/step - loss: 0.0321 - acc: 0.9883 - val_loss: 0.9041 - val_acc: 0.7500\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 5s 411ms/step - loss: 0.0361 - acc: 0.9896 - val_loss: 0.9880 - val_acc: 0.7969\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 5s 408ms/step - loss: 0.0645 - acc: 0.9818 - val_loss: 1.1176 - val_acc: 0.7188\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 5s 396ms/step - loss: 0.1364 - acc: 0.9492 - val_loss: 0.6266 - val_acc: 0.7500\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.961138\n",
      "ACC: 0.900000\n",
      "MCC : 0.800002\n",
      "TPR:0.923077\n",
      "FPR:0.125000\n",
      "Pre:0.888889\n",
      "F1:0.905660\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 34s 3s/step - loss: 1.0042 - acc: 0.4948 - val_loss: 0.7130 - val_acc: 0.4844\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 6s 525ms/step - loss: 0.7009 - acc: 0.5443 - val_loss: 0.7920 - val_acc: 0.4844\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 6s 497ms/step - loss: 0.6505 - acc: 0.6107 - val_loss: 0.7284 - val_acc: 0.4844\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 6s 497ms/step - loss: 0.6595 - acc: 0.5911 - val_loss: 0.6384 - val_acc: 0.6562\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 6s 498ms/step - loss: 0.5345 - acc: 0.7357 - val_loss: 0.6131 - val_acc: 0.6719\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 6s 489ms/step - loss: 0.5605 - acc: 0.6966 - val_loss: 0.6348 - val_acc: 0.5938\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 6s 494ms/step - loss: 0.4607 - acc: 0.7682 - val_loss: 0.7578 - val_acc: 0.6250\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 6s 536ms/step - loss: 0.3348 - acc: 0.8464 - val_loss: 0.6396 - val_acc: 0.6094\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 6s 503ms/step - loss: 0.3324 - acc: 0.8659 - val_loss: 0.6830 - val_acc: 0.6719\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 6s 500ms/step - loss: 0.3745 - acc: 0.8555 - val_loss: 0.6869 - val_acc: 0.6094\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 6s 499ms/step - loss: 0.2814 - acc: 0.8906 - val_loss: 0.6388 - val_acc: 0.6719\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 6s 494ms/step - loss: 0.2016 - acc: 0.9219 - val_loss: 0.9324 - val_acc: 0.6875\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 6s 533ms/step - loss: 0.1820 - acc: 0.9258 - val_loss: 0.7274 - val_acc: 0.6406\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 6s 494ms/step - loss: 0.0755 - acc: 0.9753 - val_loss: 1.0194 - val_acc: 0.6562\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 6s 477ms/step - loss: 0.0764 - acc: 0.9740 - val_loss: 0.9983 - val_acc: 0.6719\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 6s 498ms/step - loss: 0.0713 - acc: 0.9714 - val_loss: 0.9177 - val_acc: 0.7031\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 6s 508ms/step - loss: 0.0658 - acc: 0.9727 - val_loss: 0.9467 - val_acc: 0.7031\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 6s 514ms/step - loss: 0.0441 - acc: 0.9857 - val_loss: 0.9926 - val_acc: 0.6406\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 6s 474ms/step - loss: 0.1542 - acc: 0.9271 - val_loss: 1.0361 - val_acc: 0.6719\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 6s 494ms/step - loss: 0.0441 - acc: 0.9844 - val_loss: 1.3667 - val_acc: 0.6875\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 6s 498ms/step - loss: 0.0323 - acc: 0.9909 - val_loss: 1.4332 - val_acc: 0.6719\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 6s 489ms/step - loss: 0.1553 - acc: 0.9427 - val_loss: 1.0437 - val_acc: 0.6406\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 6s 512ms/step - loss: 0.0743 - acc: 0.9753 - val_loss: 1.3593 - val_acc: 0.6406\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 6s 515ms/step - loss: 0.0274 - acc: 0.9922 - val_loss: 1.4752 - val_acc: 0.7344\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 6s 516ms/step - loss: 0.0257 - acc: 0.9883 - val_loss: 1.2601 - val_acc: 0.6719\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 6s 484ms/step - loss: 0.0153 - acc: 0.9922 - val_loss: 1.2602 - val_acc: 0.6562\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 6s 502ms/step - loss: 0.2945 - acc: 0.9076 - val_loss: 0.9426 - val_acc: 0.6719\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 6s 483ms/step - loss: 0.0815 - acc: 0.9753 - val_loss: 1.0103 - val_acc: 0.6875\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 6s 514ms/step - loss: 0.1764 - acc: 0.9388 - val_loss: 0.6326 - val_acc: 0.7188\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 6s 506ms/step - loss: 0.0513 - acc: 0.9844 - val_loss: 1.2746 - val_acc: 0.6875\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 6s 488ms/step - loss: 0.0143 - acc: 0.9935 - val_loss: 1.5869 - val_acc: 0.6875\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 6s 516ms/step - loss: 0.0208 - acc: 0.9935 - val_loss: 1.2357 - val_acc: 0.7031\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 6s 502ms/step - loss: 0.0234 - acc: 0.9948 - val_loss: 1.5370 - val_acc: 0.6719\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 6s 519ms/step - loss: 0.0189 - acc: 0.9948 - val_loss: 1.0953 - val_acc: 0.7656\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 6s 499ms/step - loss: 0.0106 - acc: 0.9948 - val_loss: 1.2469 - val_acc: 0.7188\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 6s 487ms/step - loss: 0.2115 - acc: 0.9219 - val_loss: 0.5971 - val_acc: 0.6406\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 6s 526ms/step - loss: 0.0499 - acc: 0.9935 - val_loss: 1.3248 - val_acc: 0.6875\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 6s 482ms/step - loss: 0.4940 - acc: 0.7930 - val_loss: 0.5926 - val_acc: 0.6875\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 6s 502ms/step - loss: 0.0696 - acc: 0.9844 - val_loss: 1.3817 - val_acc: 0.6719\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 6s 498ms/step - loss: 0.1066 - acc: 0.9753 - val_loss: 2.3360 - val_acc: 0.6562\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 6s 498ms/step - loss: 0.0600 - acc: 0.9831 - val_loss: 0.6287 - val_acc: 0.6406\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 6s 497ms/step - loss: 0.0478 - acc: 0.9818 - val_loss: 1.0248 - val_acc: 0.6875\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 6s 512ms/step - loss: 0.0560 - acc: 0.9818 - val_loss: 1.0796 - val_acc: 0.7188\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 6s 505ms/step - loss: 0.0134 - acc: 0.9961 - val_loss: 1.1141 - val_acc: 0.7188\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 6s 495ms/step - loss: 0.0186 - acc: 0.9961 - val_loss: 1.2451 - val_acc: 0.7188\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 6s 489ms/step - loss: 0.0154 - acc: 0.9922 - val_loss: 1.0136 - val_acc: 0.6875\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 6s 501ms/step - loss: 0.0234 - acc: 0.9909 - val_loss: 1.1913 - val_acc: 0.6875\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 6s 480ms/step - loss: 0.0227 - acc: 0.9948 - val_loss: 1.0289 - val_acc: 0.7031\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 6s 510ms/step - loss: 0.0774 - acc: 0.9701 - val_loss: 0.8761 - val_acc: 0.7031\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 6s 512ms/step - loss: 0.0381 - acc: 0.9857 - val_loss: 1.2223 - val_acc: 0.7188\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 6s 493ms/step - loss: 0.0072 - acc: 0.9987 - val_loss: 1.4327 - val_acc: 0.7188\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 6s 483ms/step - loss: 0.0139 - acc: 0.9961 - val_loss: 1.3426 - val_acc: 0.6875\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 6s 510ms/step - loss: 0.0034 - acc: 0.9987 - val_loss: 1.9508 - val_acc: 0.6406\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 6s 491ms/step - loss: 0.0247 - acc: 0.9883 - val_loss: 1.4750 - val_acc: 0.6719\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.965676\n",
      "ACC: 0.910000\n",
      "MCC : 0.819297\n",
      "TPR:0.924528\n",
      "FPR:0.106383\n",
      "Pre:0.907407\n",
      "F1:0.915888\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.950718\n",
      "mean ACC: 0.892000\n",
      "mean MCC : 0.786091\n",
      "mean TPR:0.895284\n",
      "mean FPR:0.113606\n",
      "mean Pre:0.888469\n",
      "mean F1:0.888355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'SC'\n",
    "for rep in range(2,3):\n",
    "    n_splits = 10\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'CV/valid'+str(rep)+'-'+str(split)\n",
    "        \n",
    "         \n",
    "\n",
    "        batch_size = 64\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'CV/node_go_seq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                   epochs = 100,verbose=1,validation_data = valid_generator,\n",
    "                                 callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('node2vec_go_seq'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.952451\n",
      "mean ACC: 0.891333\n",
      "mean MCC : 0.785547\n",
      "mean TPR:0.910063\n",
      "mean FPR:0.129186\n",
      "mean Pre:0.879037\n",
      "mean F1:0.891850\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'node2vec_go_seq0.npz')\n",
    "results2 =   np.load( 'node2vec_go_seq1.npz')\n",
    "results3 =   np.load( 'node2vec_go_seq2.npz')\n",
    "print ('mean AUC: %f' %  ( (np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/3     ) )\n",
    "print ('mean ACC: %f' %   ( (np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/3) )\n",
    "print ('mean MCC : %f' %  (  (np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/3))\n",
    "print('mean TPR:%f'%    ((np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/3))\n",
    "print('mean FPR:%f'%   ( (np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/3))\n",
    "print('mean Pre:%f'%    ((np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/3))\n",
    "print('mean F1:%f'%    ((np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
