{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 256\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "        self.protein2seq = load_dict('SC_protein_seqs.pkl')\n",
    "        self.read_ppi()\n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "         \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "            \n",
    "    \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        y = np.empty((self.batch_size))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "#             X_seq1[i] =  protein_one_hot(self.protein2seq[p1], self.max_seqlen) \n",
    "#             X_seq2[i] =  protein_one_hot(self.protein2seq[p2], self.max_seqlen)\n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "             \n",
    "            \n",
    "            \n",
    "        return [  X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "         \n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "             \n",
    "#             X_seq1[i] =  protein_one_hot(self.protein2seq[p1], self.max_seqlen) \n",
    "#             X_seq2[i] =  protein_one_hot(self.protein2seq[p2], self.max_seqlen) \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "           \n",
    "        return [  X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1000, 32)     1952        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 1000, 32)     672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 1000, 32)     1952        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 1000, 32)     672         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1000, 32)     5152        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1000, 32)     3104        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1000, 32)     1952        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 1000, 32)     672         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1000, 32)     5152        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1000, 32)     3104        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 1000, 32)     1952        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1000, 32)     672         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1000, 128)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1000, 128)    33024       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1000, 128)    0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1000, 128)    33024       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1000, 128)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1000, 128)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1000, 128)    0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1000, 128)    0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 128)          1128        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          1128        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 128)          1128        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          1128        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 768)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 768)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          196864      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         525312      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          524800      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            513         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,541,921\n",
      "Trainable params: 1,541,921\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.layers.merge import concatenate\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(2)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def create_share_model():\n",
    "    con_filters = 128\n",
    "    X_input = Input(shape=(max_seq_len,20))\n",
    "    # text-CNN\n",
    "#     卷积池化-全连接 ---拼接-全连接  -双向GRU-全连接\n",
    "    cnn = Conv1D(128, 3)(X_input)\n",
    "#     cnn = BatchNormalization()(cnn)\n",
    "    cnn = Activation('relu')(cnn)\n",
    "     \n",
    "    cnn = GlobalAveragePooling1D()(cnn)\n",
    "    \n",
    "    \n",
    "    model = Model(X_input, cnn)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_siamese_model():\n",
    "    left_input_seq = Input(shape=(max_seq_len,20))  \n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "    siamese_a = create_share_model()\n",
    "    siamese_b = create_share_model()\n",
    "    \n",
    "\n",
    "    encoded_l = siamese_a(left_input_seq)\n",
    "    encoded_r = siamese_b(right_input_seq)\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "#     dense = Dense(128,activation='relu')(L1_distance)\n",
    "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "\n",
    "    siamese_net = Model(inputs=[left_input_seq,right_input_seq],outputs=prediction)\n",
    "    \n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return siamese_net\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))  \n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "    x = inception_block(left_input_seq,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(64, return_sequences=True))(left_input_seq)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    left_x_seq = Concatenate()([x_a  , x_b, x_c,  x_gru_a, x_gru_b, x_gru_c])\n",
    "    left_x_seq = Dense(256, activation='relu')(left_x_seq)\n",
    "     \n",
    " \n",
    "    x = inception_block(right_input_seq,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru =Bidirectional(CuDNNGRU(64, return_sequences=True))(right_input_seq)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    right_x_seq= Concatenate()([x_a  , x_b, x_c, x_gru_a, x_gru_b, x_gru_c])\n",
    "    right_x_seq = Dense(256, activation='relu')(right_x_seq)\n",
    "    \n",
    "     \n",
    "    x =   Concatenate()([left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "  \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_seq, right_input_seq], output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "67/67 [==============================] - 28s 423ms/step - loss: 0.6932 - acc: 0.5286 - val_loss: 0.6801 - val_acc: 0.5555\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 23s 351ms/step - loss: 0.6353 - acc: 0.6374 - val_loss: 0.5992 - val_acc: 0.6773\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 23s 344ms/step - loss: 0.5267 - acc: 0.7361 - val_loss: 0.4591 - val_acc: 0.7910\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 23s 346ms/step - loss: 0.3973 - acc: 0.8218 - val_loss: 0.3821 - val_acc: 0.8340\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.3392 - acc: 0.8501 - val_loss: 0.3101 - val_acc: 0.8791\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 23s 345ms/step - loss: 0.2967 - acc: 0.8765 - val_loss: 0.2795 - val_acc: 0.8897\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 23s 348ms/step - loss: 0.2578 - acc: 0.8933 - val_loss: 0.2916 - val_acc: 0.8909\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.2645 - acc: 0.8911 - val_loss: 0.2636 - val_acc: 0.8928\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 23s 349ms/step - loss: 0.2247 - acc: 0.9046 - val_loss: 0.2447 - val_acc: 0.9078\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.2318 - acc: 0.9043 - val_loss: 0.2933 - val_acc: 0.8799\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.2206 - acc: 0.9097 - val_loss: 0.2856 - val_acc: 0.8784\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.1994 - acc: 0.9175 - val_loss: 0.2407 - val_acc: 0.9079\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.2097 - acc: 0.9141 - val_loss: 0.2271 - val_acc: 0.9076\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.1967 - acc: 0.9205 - val_loss: 0.3143 - val_acc: 0.8634\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1908 - acc: 0.9258 - val_loss: 0.2118 - val_acc: 0.9177\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1727 - acc: 0.9314 - val_loss: 0.2241 - val_acc: 0.9119\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.1652 - acc: 0.9344 - val_loss: 0.2036 - val_acc: 0.9180\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1651 - acc: 0.9366 - val_loss: 0.2078 - val_acc: 0.9173\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 23s 344ms/step - loss: 0.2108 - acc: 0.9138 - val_loss: 0.2046 - val_acc: 0.9194\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1590 - acc: 0.9383 - val_loss: 0.2047 - val_acc: 0.9181\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1676 - acc: 0.9339 - val_loss: 0.1986 - val_acc: 0.9209\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 23s 344ms/step - loss: 0.1555 - acc: 0.9376 - val_loss: 0.2145 - val_acc: 0.9131\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1606 - acc: 0.9349 - val_loss: 0.1952 - val_acc: 0.9241\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.1550 - acc: 0.9415 - val_loss: 0.2009 - val_acc: 0.9206\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.1421 - acc: 0.9434 - val_loss: 0.2058 - val_acc: 0.9174\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1583 - acc: 0.9390 - val_loss: 0.2066 - val_acc: 0.9216\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1454 - acc: 0.9427 - val_loss: 0.2116 - val_acc: 0.9157\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1491 - acc: 0.9410 - val_loss: 0.2597 - val_acc: 0.8945\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 23s 349ms/step - loss: 0.1361 - acc: 0.9468 - val_loss: 0.2005 - val_acc: 0.9214\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 23s 346ms/step - loss: 0.1410 - acc: 0.9444 - val_loss: 0.1978 - val_acc: 0.9216\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1378 - acc: 0.9465 - val_loss: 0.1949 - val_acc: 0.9231\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 23s 348ms/step - loss: 0.1325 - acc: 0.9486 - val_loss: 0.2162 - val_acc: 0.9132\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1254 - acc: 0.9513 - val_loss: 0.2260 - val_acc: 0.9131\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1419 - acc: 0.9437 - val_loss: 0.2420 - val_acc: 0.9025\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1402 - acc: 0.9469 - val_loss: 0.2080 - val_acc: 0.9173\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1193 - acc: 0.9549 - val_loss: 0.1988 - val_acc: 0.9203\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 23s 346ms/step - loss: 0.1237 - acc: 0.9532 - val_loss: 0.2039 - val_acc: 0.9199\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1172 - acc: 0.9565 - val_loss: 0.1897 - val_acc: 0.9235\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.1322 - acc: 0.9493 - val_loss: 0.1845 - val_acc: 0.9289\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 23s 349ms/step - loss: 0.1273 - acc: 0.9511 - val_loss: 0.2079 - val_acc: 0.9194\n",
      "Epoch 41/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1221 - acc: 0.9531 - val_loss: 0.2050 - val_acc: 0.9214\n",
      "Epoch 42/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.1147 - acc: 0.9554 - val_loss: 0.2112 - val_acc: 0.9181\n",
      "Epoch 43/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1326 - acc: 0.9465 - val_loss: 0.2204 - val_acc: 0.9155\n",
      "Epoch 44/100\n",
      "67/67 [==============================] - 23s 349ms/step - loss: 0.1303 - acc: 0.9505 - val_loss: 0.1872 - val_acc: 0.9266\n",
      "Epoch 45/100\n",
      "67/67 [==============================] - 23s 350ms/step - loss: 0.1176 - acc: 0.9557 - val_loss: 0.2050 - val_acc: 0.9188\n",
      "Epoch 46/100\n",
      "67/67 [==============================] - 23s 350ms/step - loss: 0.1183 - acc: 0.9563 - val_loss: 0.2347 - val_acc: 0.9079\n",
      "Epoch 47/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.1241 - acc: 0.9539 - val_loss: 0.2007 - val_acc: 0.9199\n",
      "Epoch 48/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.1100 - acc: 0.9605 - val_loss: 0.2045 - val_acc: 0.9194\n",
      "Epoch 49/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1127 - acc: 0.9567 - val_loss: 0.2140 - val_acc: 0.9176\n",
      "Epoch 50/100\n",
      "67/67 [==============================] - 22s 336ms/step - loss: 0.1239 - acc: 0.9514 - val_loss: 0.1867 - val_acc: 0.9277\n",
      "Epoch 51/100\n",
      "67/67 [==============================] - 23s 346ms/step - loss: 0.1092 - acc: 0.9594 - val_loss: 0.2803 - val_acc: 0.8823\n",
      "Epoch 52/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1110 - acc: 0.9580 - val_loss: 0.1833 - val_acc: 0.9289\n",
      "Epoch 53/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1181 - acc: 0.9540 - val_loss: 0.1853 - val_acc: 0.9261\n",
      "Epoch 54/100\n",
      "67/67 [==============================] - 23s 346ms/step - loss: 0.1042 - acc: 0.9616 - val_loss: 0.1913 - val_acc: 0.9297\n",
      "Epoch 55/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1582 - acc: 0.9360 - val_loss: 0.1954 - val_acc: 0.9221\n",
      "Epoch 56/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1063 - acc: 0.9592 - val_loss: 0.2323 - val_acc: 0.9092\n",
      "Epoch 57/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.1019 - acc: 0.9630 - val_loss: 0.2092 - val_acc: 0.9154\n",
      "Epoch 58/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1190 - acc: 0.9537 - val_loss: 0.2202 - val_acc: 0.9091\n",
      "Epoch 59/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.1037 - acc: 0.9607 - val_loss: 0.1856 - val_acc: 0.9287\n",
      "Epoch 60/100\n",
      "67/67 [==============================] - 23s 345ms/step - loss: 0.1014 - acc: 0.9604 - val_loss: 0.2265 - val_acc: 0.9081\n",
      "Epoch 61/100\n",
      "67/67 [==============================] - 23s 345ms/step - loss: 0.1068 - acc: 0.9600 - val_loss: 0.2018 - val_acc: 0.9171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "67/67 [==============================] - 24s 352ms/step - loss: 0.1288 - acc: 0.9512 - val_loss: 0.1898 - val_acc: 0.9266\n",
      "Epoch 63/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1036 - acc: 0.9606 - val_loss: 0.1991 - val_acc: 0.9252\n",
      "Epoch 64/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.0957 - acc: 0.9632 - val_loss: 0.2075 - val_acc: 0.9183\n",
      "Epoch 65/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.0981 - acc: 0.9631 - val_loss: 0.1957 - val_acc: 0.9286\n",
      "Epoch 66/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1064 - acc: 0.9595 - val_loss: 0.1869 - val_acc: 0.9271\n",
      "Epoch 67/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.0965 - acc: 0.9629 - val_loss: 0.1865 - val_acc: 0.9290\n",
      "Epoch 68/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1146 - acc: 0.9562 - val_loss: 0.2068 - val_acc: 0.9214\n",
      "Epoch 69/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1219 - acc: 0.9531 - val_loss: 0.2151 - val_acc: 0.9135\n",
      "Epoch 70/100\n",
      "67/67 [==============================] - 23s 350ms/step - loss: 0.1107 - acc: 0.9579 - val_loss: 0.1786 - val_acc: 0.9329\n",
      "Epoch 71/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.0954 - acc: 0.9644 - val_loss: 0.1841 - val_acc: 0.9296\n",
      "Epoch 72/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.0933 - acc: 0.9666 - val_loss: 0.2189 - val_acc: 0.9128\n",
      "Epoch 73/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1110 - acc: 0.9571 - val_loss: 0.1963 - val_acc: 0.9309\n",
      "Epoch 74/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.0946 - acc: 0.9654 - val_loss: 0.2042 - val_acc: 0.9189\n",
      "Epoch 75/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.0948 - acc: 0.9643 - val_loss: 0.1996 - val_acc: 0.9192\n",
      "Epoch 76/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.0940 - acc: 0.9647 - val_loss: 0.1922 - val_acc: 0.9258\n",
      "Epoch 77/100\n",
      "67/67 [==============================] - 24s 352ms/step - loss: 0.0944 - acc: 0.9640 - val_loss: 0.2138 - val_acc: 0.9170\n",
      "Epoch 78/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.0894 - acc: 0.9655 - val_loss: 0.1985 - val_acc: 0.9247\n",
      "Epoch 79/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.0833 - acc: 0.9682 - val_loss: 0.2204 - val_acc: 0.9186\n",
      "Epoch 80/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.0872 - acc: 0.9660 - val_loss: 0.2383 - val_acc: 0.9083\n",
      "Epoch 81/100\n",
      "67/67 [==============================] - 23s 343ms/step - loss: 0.1001 - acc: 0.9629 - val_loss: 0.1849 - val_acc: 0.9310\n",
      "Epoch 82/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.0954 - acc: 0.9634 - val_loss: 0.2136 - val_acc: 0.9175\n",
      "Epoch 83/100\n",
      "67/67 [==============================] - 23s 348ms/step - loss: 0.0889 - acc: 0.9666 - val_loss: 0.2286 - val_acc: 0.9116\n",
      "Epoch 84/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.0969 - acc: 0.9631 - val_loss: 0.1923 - val_acc: 0.9316\n",
      "Epoch 85/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.0922 - acc: 0.9653 - val_loss: 0.3013 - val_acc: 0.8718\n",
      "Epoch 86/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.0878 - acc: 0.9676 - val_loss: 0.1828 - val_acc: 0.9305\n",
      "Epoch 87/100\n",
      "67/67 [==============================] - 23s 345ms/step - loss: 0.0857 - acc: 0.9675 - val_loss: 0.2127 - val_acc: 0.9196\n",
      "Epoch 88/100\n",
      "67/67 [==============================] - 24s 352ms/step - loss: 0.0875 - acc: 0.9661 - val_loss: 0.2320 - val_acc: 0.9154\n",
      "Epoch 89/100\n",
      "67/67 [==============================] - 23s 347ms/step - loss: 0.0875 - acc: 0.9661 - val_loss: 0.1898 - val_acc: 0.9302\n",
      "Epoch 90/100\n",
      "67/67 [==============================] - 23s 348ms/step - loss: 0.0821 - acc: 0.9693 - val_loss: 0.2023 - val_acc: 0.9209\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.978074\n",
      "ACC: 0.927677\n",
      "MCC : 0.855469\n",
      "TPR:0.918781\n",
      "FPR:0.063496\n",
      "Pre:0.934880\n",
      "F1:0.926761\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.978074\n",
      "mean ACC: 0.927677\n",
      "mean MCC : 0.855469\n",
      "mean TPR:0.918781\n",
      "mean FPR:0.063496\n",
      "mean Pre:0.934880\n",
      "mean F1:0.926761\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 28s 418ms/step - loss: 0.6919 - acc: 0.5347 - val_loss: 0.6697 - val_acc: 0.6025\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.6242 - acc: 0.6452 - val_loss: 0.5896 - val_acc: 0.6777\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.5042 - acc: 0.7522 - val_loss: 0.5827 - val_acc: 0.6776\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.4044 - acc: 0.8223 - val_loss: 0.3676 - val_acc: 0.8600\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.3405 - acc: 0.8541 - val_loss: 0.3137 - val_acc: 0.8735\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.2977 - acc: 0.8742 - val_loss: 0.2870 - val_acc: 0.8845\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.2763 - acc: 0.8854 - val_loss: 0.2793 - val_acc: 0.8894\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.2643 - acc: 0.8921 - val_loss: 0.3449 - val_acc: 0.8550\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 23s 345ms/step - loss: 0.2620 - acc: 0.8886 - val_loss: 0.2707 - val_acc: 0.8997\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.2307 - acc: 0.9060 - val_loss: 0.2467 - val_acc: 0.9020\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.2264 - acc: 0.9098 - val_loss: 0.2371 - val_acc: 0.9061\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.2090 - acc: 0.9162 - val_loss: 0.2406 - val_acc: 0.9034\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.1966 - acc: 0.9227 - val_loss: 0.2255 - val_acc: 0.9096\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1880 - acc: 0.9271 - val_loss: 0.2435 - val_acc: 0.9022\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.2073 - acc: 0.9149 - val_loss: 0.2163 - val_acc: 0.9123\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.2048 - acc: 0.9187 - val_loss: 0.3084 - val_acc: 0.8635\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.2024 - acc: 0.9162 - val_loss: 0.2196 - val_acc: 0.9162\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1641 - acc: 0.9357 - val_loss: 0.2138 - val_acc: 0.9156\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1843 - acc: 0.9257 - val_loss: 0.2403 - val_acc: 0.9058\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1714 - acc: 0.9315 - val_loss: 0.2213 - val_acc: 0.9113\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1766 - acc: 0.9305 - val_loss: 0.2420 - val_acc: 0.9001\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1797 - acc: 0.9277 - val_loss: 0.2094 - val_acc: 0.9183\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.1651 - acc: 0.9353 - val_loss: 0.2369 - val_acc: 0.9031\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1512 - acc: 0.9429 - val_loss: 0.2201 - val_acc: 0.9122\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1695 - acc: 0.9325 - val_loss: 0.1990 - val_acc: 0.9225\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1502 - acc: 0.9422 - val_loss: 0.2006 - val_acc: 0.9214\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1509 - acc: 0.9429 - val_loss: 0.2260 - val_acc: 0.9090\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 23s 342ms/step - loss: 0.1465 - acc: 0.9435 - val_loss: 0.2391 - val_acc: 0.9029\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1504 - acc: 0.9420 - val_loss: 0.2007 - val_acc: 0.9205\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1407 - acc: 0.9458 - val_loss: 0.2033 - val_acc: 0.9184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "67/67 [==============================] - 23s 339ms/step - loss: 0.1448 - acc: 0.9436 - val_loss: 0.2035 - val_acc: 0.9188\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1358 - acc: 0.9482 - val_loss: 0.2201 - val_acc: 0.9135\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1425 - acc: 0.9443 - val_loss: 0.1981 - val_acc: 0.9209\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1778 - acc: 0.9286 - val_loss: 0.2122 - val_acc: 0.9147\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 22s 329ms/step - loss: 0.1399 - acc: 0.9455 - val_loss: 0.1968 - val_acc: 0.9216\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 22s 323ms/step - loss: 0.1531 - acc: 0.9388 - val_loss: 0.1973 - val_acc: 0.9205\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 21s 315ms/step - loss: 0.1252 - acc: 0.9525 - val_loss: 0.1916 - val_acc: 0.9241\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1291 - acc: 0.9489 - val_loss: 0.2079 - val_acc: 0.9160\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1340 - acc: 0.9488 - val_loss: 0.2165 - val_acc: 0.9111\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.1303 - acc: 0.9475 - val_loss: 0.2163 - val_acc: 0.9141\n",
      "Epoch 41/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.1254 - acc: 0.9493 - val_loss: 0.2594 - val_acc: 0.9042\n",
      "Epoch 42/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1307 - acc: 0.9515 - val_loss: 0.2000 - val_acc: 0.9179\n",
      "Epoch 43/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.1256 - acc: 0.9499 - val_loss: 0.1839 - val_acc: 0.9268\n",
      "Epoch 44/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1314 - acc: 0.9485 - val_loss: 0.2000 - val_acc: 0.9189\n",
      "Epoch 45/100\n",
      "67/67 [==============================] - 22s 325ms/step - loss: 0.1185 - acc: 0.9553 - val_loss: 0.1888 - val_acc: 0.9245\n",
      "Epoch 46/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.1206 - acc: 0.9548 - val_loss: 0.1919 - val_acc: 0.9214\n",
      "Epoch 47/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1171 - acc: 0.9541 - val_loss: 0.1839 - val_acc: 0.9287\n",
      "Epoch 48/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1452 - acc: 0.9437 - val_loss: 0.2202 - val_acc: 0.9106\n",
      "Epoch 49/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1165 - acc: 0.9552 - val_loss: 0.1874 - val_acc: 0.9257\n",
      "Epoch 50/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.1159 - acc: 0.9564 - val_loss: 0.1831 - val_acc: 0.9277\n",
      "Epoch 51/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1141 - acc: 0.9556 - val_loss: 0.1856 - val_acc: 0.9289\n",
      "Epoch 52/100\n",
      "67/67 [==============================] - 22s 323ms/step - loss: 0.1139 - acc: 0.9562 - val_loss: 0.1821 - val_acc: 0.9278\n",
      "Epoch 53/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.1170 - acc: 0.9555 - val_loss: 0.1963 - val_acc: 0.9214\n",
      "Epoch 54/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1067 - acc: 0.9588 - val_loss: 0.1792 - val_acc: 0.9278\n",
      "Epoch 55/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1125 - acc: 0.9574 - val_loss: 0.1848 - val_acc: 0.9291\n",
      "Epoch 56/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1132 - acc: 0.9567 - val_loss: 0.1984 - val_acc: 0.9251\n",
      "Epoch 57/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1011 - acc: 0.9621 - val_loss: 0.1959 - val_acc: 0.9258\n",
      "Epoch 58/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1229 - acc: 0.9514 - val_loss: 0.1810 - val_acc: 0.9303\n",
      "Epoch 59/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1347 - acc: 0.9468 - val_loss: 0.1877 - val_acc: 0.9273\n",
      "Epoch 60/100\n",
      "67/67 [==============================] - 21s 315ms/step - loss: 0.1027 - acc: 0.9597 - val_loss: 0.1881 - val_acc: 0.9293\n",
      "Epoch 61/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1277 - acc: 0.9482 - val_loss: 0.2304 - val_acc: 0.9035\n",
      "Epoch 62/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1126 - acc: 0.9573 - val_loss: 0.1838 - val_acc: 0.9293\n",
      "Epoch 63/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1118 - acc: 0.9562 - val_loss: 0.1892 - val_acc: 0.9291\n",
      "Epoch 64/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.1297 - acc: 0.9498 - val_loss: 0.1929 - val_acc: 0.9248\n",
      "Epoch 65/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.0996 - acc: 0.9627 - val_loss: 0.1816 - val_acc: 0.9293\n",
      "Epoch 66/100\n",
      "67/67 [==============================] - 22s 325ms/step - loss: 0.1063 - acc: 0.9579 - val_loss: 0.1894 - val_acc: 0.9283\n",
      "Epoch 67/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1203 - acc: 0.9538 - val_loss: 0.1798 - val_acc: 0.9315\n",
      "Epoch 68/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.0999 - acc: 0.9608 - val_loss: 0.1785 - val_acc: 0.9325\n",
      "Epoch 69/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.1014 - acc: 0.9616 - val_loss: 0.1796 - val_acc: 0.9325\n",
      "Epoch 70/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.0968 - acc: 0.9622 - val_loss: 0.1912 - val_acc: 0.9280\n",
      "Epoch 71/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1336 - acc: 0.9464 - val_loss: 0.1820 - val_acc: 0.9290\n",
      "Epoch 72/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.0961 - acc: 0.9626 - val_loss: 0.1926 - val_acc: 0.9263\n",
      "Epoch 73/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1011 - acc: 0.9606 - val_loss: 0.2449 - val_acc: 0.8988\n",
      "Epoch 74/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.0977 - acc: 0.9635 - val_loss: 0.1997 - val_acc: 0.9246\n",
      "Epoch 75/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1013 - acc: 0.9622 - val_loss: 0.1931 - val_acc: 0.9229\n",
      "Epoch 76/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.0971 - acc: 0.9627 - val_loss: 0.2053 - val_acc: 0.9183\n",
      "Epoch 77/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1108 - acc: 0.9558 - val_loss: 0.2001 - val_acc: 0.9199\n",
      "Epoch 78/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.0961 - acc: 0.9642 - val_loss: 0.1805 - val_acc: 0.9350\n",
      "Epoch 79/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1110 - acc: 0.9567 - val_loss: 0.2002 - val_acc: 0.9208\n",
      "Epoch 80/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.0995 - acc: 0.9630 - val_loss: 0.1952 - val_acc: 0.9286\n",
      "Epoch 81/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1047 - acc: 0.9593 - val_loss: 0.1796 - val_acc: 0.9329\n",
      "Epoch 82/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.0876 - acc: 0.9669 - val_loss: 0.1878 - val_acc: 0.9280\n",
      "Epoch 83/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.0908 - acc: 0.9649 - val_loss: 0.1949 - val_acc: 0.9279\n",
      "Epoch 84/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.0894 - acc: 0.9650 - val_loss: 0.2055 - val_acc: 0.9220\n",
      "Epoch 85/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.0843 - acc: 0.9675 - val_loss: 0.2222 - val_acc: 0.9231\n",
      "Epoch 86/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.0832 - acc: 0.9664 - val_loss: 0.1874 - val_acc: 0.9278\n",
      "Epoch 87/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.0957 - acc: 0.9613 - val_loss: 0.1926 - val_acc: 0.9329\n",
      "Epoch 88/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.0912 - acc: 0.9658 - val_loss: 0.2166 - val_acc: 0.9222\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.977699\n",
      "ACC: 0.927446\n",
      "MCC : 0.855769\n",
      "TPR:0.907247\n",
      "FPR:0.051651\n",
      "Pre:0.947857\n",
      "F1:0.927108\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.977699\n",
      "mean ACC: 0.927446\n",
      "mean MCC : 0.855769\n",
      "mean TPR:0.907247\n",
      "mean FPR:0.051651\n",
      "mean Pre:0.947857\n",
      "mean F1:0.927108\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 27s 398ms/step - loss: 0.6982 - acc: 0.5093 - val_loss: 0.6847 - val_acc: 0.5772\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.6552 - acc: 0.6119 - val_loss: 0.6086 - val_acc: 0.6696\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 22s 321ms/step - loss: 0.5603 - acc: 0.7158 - val_loss: 0.4764 - val_acc: 0.8118\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.4220 - acc: 0.8113 - val_loss: 0.3662 - val_acc: 0.8567\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 22s 322ms/step - loss: 0.3463 - acc: 0.8531 - val_loss: 0.3393 - val_acc: 0.8621\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 21s 315ms/step - loss: 0.2925 - acc: 0.8795 - val_loss: 0.2862 - val_acc: 0.8862\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.2709 - acc: 0.8899 - val_loss: 0.2851 - val_acc: 0.8842\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 21s 321ms/step - loss: 0.2757 - acc: 0.8857 - val_loss: 0.2959 - val_acc: 0.8816\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 22s 324ms/step - loss: 0.2414 - acc: 0.9039 - val_loss: 0.2573 - val_acc: 0.8980\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.2220 - acc: 0.9113 - val_loss: 0.2381 - val_acc: 0.9034\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 22s 321ms/step - loss: 0.2379 - acc: 0.9037 - val_loss: 0.2704 - val_acc: 0.8946\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.2071 - acc: 0.9185 - val_loss: 0.2632 - val_acc: 0.8930\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.2258 - acc: 0.9069 - val_loss: 0.2335 - val_acc: 0.9052\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1951 - acc: 0.9207 - val_loss: 0.2126 - val_acc: 0.9144\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 21s 321ms/step - loss: 0.1892 - acc: 0.9249 - val_loss: 0.2131 - val_acc: 0.9154\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 22s 323ms/step - loss: 0.1739 - acc: 0.9320 - val_loss: 0.2904 - val_acc: 0.8816\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1729 - acc: 0.9329 - val_loss: 0.2137 - val_acc: 0.9154\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1569 - acc: 0.9399 - val_loss: 0.2257 - val_acc: 0.9067\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1724 - acc: 0.9327 - val_loss: 0.2448 - val_acc: 0.8999\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 21s 316ms/step - loss: 0.1587 - acc: 0.9380 - val_loss: 0.2087 - val_acc: 0.9150\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 22s 322ms/step - loss: 0.1822 - acc: 0.9269 - val_loss: 0.1973 - val_acc: 0.9201\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1584 - acc: 0.9389 - val_loss: 0.2201 - val_acc: 0.9099\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 21s 317ms/step - loss: 0.1697 - acc: 0.9312 - val_loss: 0.2028 - val_acc: 0.9186\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 22s 321ms/step - loss: 0.1567 - acc: 0.9378 - val_loss: 0.2214 - val_acc: 0.9113\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 22s 322ms/step - loss: 0.1554 - acc: 0.9396 - val_loss: 0.2037 - val_acc: 0.9190\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 21s 320ms/step - loss: 0.1581 - acc: 0.9385 - val_loss: 0.2591 - val_acc: 0.8948\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 21s 318ms/step - loss: 0.1652 - acc: 0.9337 - val_loss: 0.2156 - val_acc: 0.9130\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 21s 319ms/step - loss: 0.1429 - acc: 0.9453 - val_loss: 0.2968 - val_acc: 0.8731\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1500 - acc: 0.9405 - val_loss: 0.2032 - val_acc: 0.9194\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 23s 341ms/step - loss: 0.1400 - acc: 0.9473 - val_loss: 0.1864 - val_acc: 0.9232\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1332 - acc: 0.9495 - val_loss: 0.2194 - val_acc: 0.9124\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1475 - acc: 0.9408 - val_loss: 0.2725 - val_acc: 0.8897\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1359 - acc: 0.9486 - val_loss: 0.2046 - val_acc: 0.9199\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1381 - acc: 0.9472 - val_loss: 0.1941 - val_acc: 0.9233\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1413 - acc: 0.9435 - val_loss: 0.1826 - val_acc: 0.9285\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1285 - acc: 0.9516 - val_loss: 0.1872 - val_acc: 0.9254\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1343 - acc: 0.9487 - val_loss: 0.1867 - val_acc: 0.9266\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1225 - acc: 0.9534 - val_loss: 0.2039 - val_acc: 0.9210\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1359 - acc: 0.9483 - val_loss: 0.2172 - val_acc: 0.9109\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1238 - acc: 0.9552 - val_loss: 0.1916 - val_acc: 0.9248\n",
      "Epoch 41/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1336 - acc: 0.9495 - val_loss: 0.1915 - val_acc: 0.9247\n",
      "Epoch 42/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1215 - acc: 0.9546 - val_loss: 0.1965 - val_acc: 0.9239\n",
      "Epoch 43/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1124 - acc: 0.9588 - val_loss: 0.2044 - val_acc: 0.9205\n",
      "Epoch 44/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1444 - acc: 0.9449 - val_loss: 0.2259 - val_acc: 0.9136\n",
      "Epoch 45/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1357 - acc: 0.9476 - val_loss: 0.2413 - val_acc: 0.9087\n",
      "Epoch 46/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1258 - acc: 0.9514 - val_loss: 0.1973 - val_acc: 0.9210\n",
      "Epoch 47/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1283 - acc: 0.9507 - val_loss: 0.1981 - val_acc: 0.9186\n",
      "Epoch 48/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1222 - acc: 0.9526 - val_loss: 0.1979 - val_acc: 0.9223\n",
      "Epoch 49/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1183 - acc: 0.9547 - val_loss: 0.2120 - val_acc: 0.9135\n",
      "Epoch 50/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.1182 - acc: 0.9542 - val_loss: 0.2294 - val_acc: 0.9084\n",
      "Epoch 51/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1213 - acc: 0.9530 - val_loss: 0.1879 - val_acc: 0.9254\n",
      "Epoch 52/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1166 - acc: 0.9555 - val_loss: 0.1901 - val_acc: 0.9228\n",
      "Epoch 53/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1498 - acc: 0.9418 - val_loss: 0.3407 - val_acc: 0.8571\n",
      "Epoch 54/100\n",
      "67/67 [==============================] - 22s 336ms/step - loss: 0.1196 - acc: 0.9534 - val_loss: 0.2142 - val_acc: 0.9139\n",
      "Epoch 55/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1168 - acc: 0.9553 - val_loss: 0.2270 - val_acc: 0.9061\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.977361\n",
      "ACC: 0.922925\n",
      "MCC : 0.846835\n",
      "TPR:0.897645\n",
      "FPR:0.052086\n",
      "Pre:0.944553\n",
      "F1:0.920502\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.977361\n",
      "mean ACC: 0.922925\n",
      "mean MCC : 0.846835\n",
      "mean TPR:0.897645\n",
      "mean FPR:0.052086\n",
      "mean Pre:0.944553\n",
      "mean F1:0.920502\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 29s 426ms/step - loss: 0.6967 - acc: 0.5188 - val_loss: 0.6827 - val_acc: 0.5788\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.6341 - acc: 0.6375 - val_loss: 0.5937 - val_acc: 0.7036\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.5646 - acc: 0.7149 - val_loss: 0.5277 - val_acc: 0.7848\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.4456 - acc: 0.7982 - val_loss: 0.4063 - val_acc: 0.8346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.3412 - acc: 0.8534 - val_loss: 0.3463 - val_acc: 0.8686\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.2828 - acc: 0.8784 - val_loss: 0.3056 - val_acc: 0.8673\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.2740 - acc: 0.8856 - val_loss: 0.2697 - val_acc: 0.8905\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 23s 340ms/step - loss: 0.2508 - acc: 0.8946 - val_loss: 0.2774 - val_acc: 0.8887\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.2357 - acc: 0.9038 - val_loss: 0.2548 - val_acc: 0.8990\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.2198 - acc: 0.9125 - val_loss: 0.2516 - val_acc: 0.8969\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.2241 - acc: 0.9060 - val_loss: 0.2358 - val_acc: 0.9047\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.2131 - acc: 0.9117 - val_loss: 0.2469 - val_acc: 0.9009\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1995 - acc: 0.9209 - val_loss: 0.2465 - val_acc: 0.8996\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.1807 - acc: 0.9274 - val_loss: 0.2220 - val_acc: 0.9081\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1842 - acc: 0.9262 - val_loss: 0.2286 - val_acc: 0.9067\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1825 - acc: 0.9265 - val_loss: 0.2333 - val_acc: 0.9051\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1762 - acc: 0.9303 - val_loss: 0.2185 - val_acc: 0.9093\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1877 - acc: 0.9247 - val_loss: 0.2335 - val_acc: 0.9040\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.1527 - acc: 0.9416 - val_loss: 0.2210 - val_acc: 0.9091\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1528 - acc: 0.9412 - val_loss: 0.2518 - val_acc: 0.8950\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1558 - acc: 0.9402 - val_loss: 0.2289 - val_acc: 0.9065\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1848 - acc: 0.9255 - val_loss: 0.2724 - val_acc: 0.8911\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1502 - acc: 0.9408 - val_loss: 0.2146 - val_acc: 0.9117\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1513 - acc: 0.9420 - val_loss: 0.2527 - val_acc: 0.8976\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1605 - acc: 0.9358 - val_loss: 0.2668 - val_acc: 0.8866\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1703 - acc: 0.9317 - val_loss: 0.2172 - val_acc: 0.9115\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1375 - acc: 0.9464 - val_loss: 0.2091 - val_acc: 0.9124\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1519 - acc: 0.9391 - val_loss: 0.2077 - val_acc: 0.9143\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1430 - acc: 0.9462 - val_loss: 0.2087 - val_acc: 0.9143\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1395 - acc: 0.9474 - val_loss: 0.2087 - val_acc: 0.9154\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1359 - acc: 0.9465 - val_loss: 0.2118 - val_acc: 0.9125\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1319 - acc: 0.9488 - val_loss: 0.1989 - val_acc: 0.9220\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1287 - acc: 0.9497 - val_loss: 0.1997 - val_acc: 0.9222\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1300 - acc: 0.9511 - val_loss: 0.2001 - val_acc: 0.9196\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.1417 - acc: 0.9450 - val_loss: 0.2093 - val_acc: 0.9135\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1440 - acc: 0.9440 - val_loss: 0.2468 - val_acc: 0.8954\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1263 - acc: 0.9524 - val_loss: 0.1966 - val_acc: 0.9209\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.1226 - acc: 0.9529 - val_loss: 0.2334 - val_acc: 0.9039\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1339 - acc: 0.9489 - val_loss: 0.2294 - val_acc: 0.9053\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1160 - acc: 0.9562 - val_loss: 0.2088 - val_acc: 0.9179\n",
      "Epoch 41/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1095 - acc: 0.9595 - val_loss: 0.2091 - val_acc: 0.9160\n",
      "Epoch 42/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1187 - acc: 0.9566 - val_loss: 0.1972 - val_acc: 0.9197\n",
      "Epoch 43/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1213 - acc: 0.9557 - val_loss: 0.2371 - val_acc: 0.9091\n",
      "Epoch 44/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1236 - acc: 0.9539 - val_loss: 0.1993 - val_acc: 0.9175\n",
      "Epoch 45/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1320 - acc: 0.9486 - val_loss: 0.3136 - val_acc: 0.8722\n",
      "Epoch 46/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1161 - acc: 0.9557 - val_loss: 0.2233 - val_acc: 0.9112\n",
      "Epoch 47/100\n",
      "67/67 [==============================] - 22s 336ms/step - loss: 0.1231 - acc: 0.9524 - val_loss: 0.2200 - val_acc: 0.9142\n",
      "Epoch 48/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.1109 - acc: 0.9595 - val_loss: 0.2311 - val_acc: 0.9080\n",
      "Epoch 49/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1087 - acc: 0.9587 - val_loss: 0.1999 - val_acc: 0.9192\n",
      "Epoch 50/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.1160 - acc: 0.9548 - val_loss: 0.2271 - val_acc: 0.9087\n",
      "Epoch 51/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1274 - acc: 0.9495 - val_loss: 0.2594 - val_acc: 0.8945\n",
      "Epoch 52/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1052 - acc: 0.9599 - val_loss: 0.2267 - val_acc: 0.9139\n",
      "Epoch 53/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1047 - acc: 0.9608 - val_loss: 0.2205 - val_acc: 0.9139\n",
      "Epoch 54/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1144 - acc: 0.9558 - val_loss: 0.2113 - val_acc: 0.9181\n",
      "Epoch 55/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1064 - acc: 0.9601 - val_loss: 0.1955 - val_acc: 0.9214\n",
      "Epoch 56/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1136 - acc: 0.9553 - val_loss: 0.2045 - val_acc: 0.9183\n",
      "Epoch 57/100\n",
      "67/67 [==============================] - 22s 329ms/step - loss: 0.1065 - acc: 0.9579 - val_loss: 0.1975 - val_acc: 0.9208\n",
      "Epoch 58/100\n",
      "67/67 [==============================] - 22s 329ms/step - loss: 0.1162 - acc: 0.9545 - val_loss: 0.2598 - val_acc: 0.8957\n",
      "Epoch 59/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1116 - acc: 0.9581 - val_loss: 0.2004 - val_acc: 0.9182\n",
      "Epoch 60/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1231 - acc: 0.9514 - val_loss: 0.2034 - val_acc: 0.9205\n",
      "Epoch 61/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1013 - acc: 0.9618 - val_loss: 0.2235 - val_acc: 0.9132\n",
      "Epoch 62/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.0974 - acc: 0.9639 - val_loss: 0.1998 - val_acc: 0.9184\n",
      "Epoch 63/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.1217 - acc: 0.9542 - val_loss: 0.1986 - val_acc: 0.9193\n",
      "Epoch 64/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1084 - acc: 0.9581 - val_loss: 0.2263 - val_acc: 0.9128\n",
      "Epoch 65/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1032 - acc: 0.9608 - val_loss: 0.3239 - val_acc: 0.8692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      "67/67 [==============================] - 22s 336ms/step - loss: 0.1115 - acc: 0.9577 - val_loss: 0.1971 - val_acc: 0.9232\n",
      "Epoch 67/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.0998 - acc: 0.9625 - val_loss: 0.2439 - val_acc: 0.9103\n",
      "Epoch 68/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1135 - acc: 0.9568 - val_loss: 0.2044 - val_acc: 0.9212\n",
      "Epoch 69/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.0882 - acc: 0.9671 - val_loss: 0.2129 - val_acc: 0.9165\n",
      "Epoch 70/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.0920 - acc: 0.9655 - val_loss: 0.2115 - val_acc: 0.9160\n",
      "Epoch 71/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.0909 - acc: 0.9661 - val_loss: 0.2222 - val_acc: 0.9113\n",
      "Epoch 72/100\n",
      "67/67 [==============================] - 23s 338ms/step - loss: 0.0994 - acc: 0.9633 - val_loss: 0.2365 - val_acc: 0.9100\n",
      "Epoch 73/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1075 - acc: 0.9583 - val_loss: 0.2331 - val_acc: 0.9098\n",
      "Epoch 74/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1035 - acc: 0.9608 - val_loss: 0.2122 - val_acc: 0.9194\n",
      "Epoch 75/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.0878 - acc: 0.9664 - val_loss: 0.1979 - val_acc: 0.9223\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.977544\n",
      "ACC: 0.924780\n",
      "MCC : 0.849855\n",
      "TPR:0.913352\n",
      "FPR:0.063470\n",
      "Pre:0.936694\n",
      "F1:0.924876\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.977544\n",
      "mean ACC: 0.924780\n",
      "mean MCC : 0.849855\n",
      "mean TPR:0.913352\n",
      "mean FPR:0.063470\n",
      "mean Pre:0.936694\n",
      "mean F1:0.924876\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 29s 432ms/step - loss: 0.6900 - acc: 0.5406 - val_loss: 0.6612 - val_acc: 0.6255\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.6168 - acc: 0.6594 - val_loss: 0.5403 - val_acc: 0.7369\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.4958 - acc: 0.7649 - val_loss: 0.4266 - val_acc: 0.8317\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.3903 - acc: 0.8292 - val_loss: 0.3517 - val_acc: 0.8632\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.3762 - acc: 0.8366 - val_loss: 0.4081 - val_acc: 0.8138\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.2876 - acc: 0.8812 - val_loss: 0.3118 - val_acc: 0.8733\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.2796 - acc: 0.8856 - val_loss: 0.3114 - val_acc: 0.8732\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.2738 - acc: 0.8862 - val_loss: 0.2670 - val_acc: 0.8969\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.2424 - acc: 0.9023 - val_loss: 0.2372 - val_acc: 0.9083\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.2370 - acc: 0.9031 - val_loss: 0.2265 - val_acc: 0.9121\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.2152 - acc: 0.9123 - val_loss: 0.2837 - val_acc: 0.8810\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.2065 - acc: 0.9161 - val_loss: 0.2235 - val_acc: 0.9110\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1888 - acc: 0.9246 - val_loss: 0.2338 - val_acc: 0.9059\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1850 - acc: 0.9275 - val_loss: 0.2149 - val_acc: 0.9131\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.2163 - acc: 0.9145 - val_loss: 0.2850 - val_acc: 0.8800\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1990 - acc: 0.9209 - val_loss: 0.2534 - val_acc: 0.8916\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1744 - acc: 0.9317 - val_loss: 0.2052 - val_acc: 0.9169\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 22s 330ms/step - loss: 0.1577 - acc: 0.9389 - val_loss: 0.2648 - val_acc: 0.8939\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1604 - acc: 0.9383 - val_loss: 0.1975 - val_acc: 0.9195\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1677 - acc: 0.9346 - val_loss: 0.2052 - val_acc: 0.9180\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1541 - acc: 0.9395 - val_loss: 0.2076 - val_acc: 0.9197\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1551 - acc: 0.9391 - val_loss: 0.2008 - val_acc: 0.9209\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1484 - acc: 0.9429 - val_loss: 0.1930 - val_acc: 0.9228\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1476 - acc: 0.9433 - val_loss: 0.1891 - val_acc: 0.9238\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1503 - acc: 0.9430 - val_loss: 0.2023 - val_acc: 0.9202\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1320 - acc: 0.9495 - val_loss: 0.2028 - val_acc: 0.9197\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1562 - acc: 0.9392 - val_loss: 0.2422 - val_acc: 0.9061\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1480 - acc: 0.9448 - val_loss: 0.1920 - val_acc: 0.9215\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 23s 337ms/step - loss: 0.1550 - acc: 0.9401 - val_loss: 0.2157 - val_acc: 0.9131\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1424 - acc: 0.9451 - val_loss: 0.2244 - val_acc: 0.9111\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1347 - acc: 0.9484 - val_loss: 0.1933 - val_acc: 0.9225\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1270 - acc: 0.9525 - val_loss: 0.1968 - val_acc: 0.9228\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1176 - acc: 0.9554 - val_loss: 0.2001 - val_acc: 0.9225\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1286 - acc: 0.9507 - val_loss: 0.2244 - val_acc: 0.9142\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1366 - acc: 0.9468 - val_loss: 0.2047 - val_acc: 0.9175\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1185 - acc: 0.9560 - val_loss: 0.2096 - val_acc: 0.9197\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1223 - acc: 0.9539 - val_loss: 0.2109 - val_acc: 0.9157\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1533 - acc: 0.9401 - val_loss: 0.2844 - val_acc: 0.8873\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1312 - acc: 0.9504 - val_loss: 0.1886 - val_acc: 0.9231\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1169 - acc: 0.9556 - val_loss: 0.1974 - val_acc: 0.9199\n",
      "Epoch 41/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1150 - acc: 0.9582 - val_loss: 0.2055 - val_acc: 0.9197\n",
      "Epoch 42/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1169 - acc: 0.9558 - val_loss: 0.1986 - val_acc: 0.9234\n",
      "Epoch 43/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1214 - acc: 0.9545 - val_loss: 0.1994 - val_acc: 0.9202\n",
      "Epoch 44/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1124 - acc: 0.9573 - val_loss: 0.2254 - val_acc: 0.9148\n",
      "Epoch 45/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1236 - acc: 0.9521 - val_loss: 0.1952 - val_acc: 0.9258\n",
      "Epoch 46/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1218 - acc: 0.9528 - val_loss: 0.2542 - val_acc: 0.9035\n",
      "Epoch 47/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1157 - acc: 0.9565 - val_loss: 0.1998 - val_acc: 0.9223\n",
      "Epoch 48/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1171 - acc: 0.9565 - val_loss: 0.2287 - val_acc: 0.9100\n",
      "Epoch 49/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1116 - acc: 0.9573 - val_loss: 0.2389 - val_acc: 0.9112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1455 - acc: 0.9434 - val_loss: 0.1918 - val_acc: 0.9234\n",
      "Epoch 51/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1335 - acc: 0.9486 - val_loss: 0.2161 - val_acc: 0.9187\n",
      "Epoch 52/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1174 - acc: 0.9553 - val_loss: 0.1868 - val_acc: 0.9257\n",
      "Epoch 53/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1040 - acc: 0.9610 - val_loss: 0.2001 - val_acc: 0.9216\n",
      "Epoch 54/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1081 - acc: 0.9597 - val_loss: 0.1830 - val_acc: 0.9276\n",
      "Epoch 55/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1051 - acc: 0.9609 - val_loss: 0.2058 - val_acc: 0.9186\n",
      "Epoch 56/100\n",
      "67/67 [==============================] - 22s 336ms/step - loss: 0.1144 - acc: 0.9566 - val_loss: 0.2052 - val_acc: 0.9187\n",
      "Epoch 57/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1036 - acc: 0.9600 - val_loss: 0.1866 - val_acc: 0.9273\n",
      "Epoch 58/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1172 - acc: 0.9545 - val_loss: 0.2006 - val_acc: 0.9253\n",
      "Epoch 59/100\n",
      "67/67 [==============================] - 23s 336ms/step - loss: 0.1018 - acc: 0.9609 - val_loss: 0.2291 - val_acc: 0.9162\n",
      "Epoch 60/100\n",
      "67/67 [==============================] - 22s 333ms/step - loss: 0.1082 - acc: 0.9585 - val_loss: 0.1918 - val_acc: 0.9229\n",
      "Epoch 61/100\n",
      "67/67 [==============================] - 22s 329ms/step - loss: 0.1187 - acc: 0.9537 - val_loss: 0.1854 - val_acc: 0.9259\n",
      "Epoch 62/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.1069 - acc: 0.9586 - val_loss: 0.2746 - val_acc: 0.8944\n",
      "Epoch 63/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.1385 - acc: 0.9471 - val_loss: 0.1911 - val_acc: 0.9244\n",
      "Epoch 64/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.1030 - acc: 0.9608 - val_loss: 0.2158 - val_acc: 0.9192\n",
      "Epoch 65/100\n",
      "67/67 [==============================] - 22s 329ms/step - loss: 0.1137 - acc: 0.9565 - val_loss: 0.2077 - val_acc: 0.9197\n",
      "Epoch 66/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1112 - acc: 0.9585 - val_loss: 0.2023 - val_acc: 0.9234\n",
      "Epoch 67/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.0948 - acc: 0.9635 - val_loss: 0.1933 - val_acc: 0.9246\n",
      "Epoch 68/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.1083 - acc: 0.9580 - val_loss: 0.2092 - val_acc: 0.9232\n",
      "Epoch 69/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.0961 - acc: 0.9648 - val_loss: 0.2129 - val_acc: 0.9213\n",
      "Epoch 70/100\n",
      "67/67 [==============================] - 22s 332ms/step - loss: 0.0866 - acc: 0.9695 - val_loss: 0.2174 - val_acc: 0.9218\n",
      "Epoch 71/100\n",
      "67/67 [==============================] - 22s 335ms/step - loss: 0.0921 - acc: 0.9644 - val_loss: 0.2125 - val_acc: 0.9182\n",
      "Epoch 72/100\n",
      "67/67 [==============================] - 22s 334ms/step - loss: 0.0933 - acc: 0.9634 - val_loss: 0.1964 - val_acc: 0.9213\n",
      "Epoch 73/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.0955 - acc: 0.9628 - val_loss: 0.1994 - val_acc: 0.9228\n",
      "Epoch 74/100\n",
      "67/67 [==============================] - 22s 331ms/step - loss: 0.0962 - acc: 0.9637 - val_loss: 0.2398 - val_acc: 0.9150\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.977407\n",
      "ACC: 0.924548\n",
      "MCC : 0.849096\n",
      "TPR:0.925116\n",
      "FPR:0.076017\n",
      "Pre:0.923613\n",
      "F1:0.924364\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.977407\n",
      "mean ACC: 0.924548\n",
      "mean MCC : 0.849096\n",
      "mean TPR:0.925116\n",
      "mean FPR:0.076017\n",
      "mean Pre:0.923613\n",
      "mean F1:0.924364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'SC'\n",
    "for rep in range(5):\n",
    "    n_splits = 1\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'SC_CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'SC_CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'SC_CV/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 256\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'SC_CV/sc_seq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='min')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=valid_generator,\n",
    "                    epochs = 100,verbose=1,callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('SC_seq_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.977617\n",
      "mean ACC: 0.925475\n",
      "mean MCC : 0.851405\n",
      "mean TPR:0.912428\n",
      "mean FPR:0.061344\n",
      "mean Pre:0.937520\n",
      "mean F1:0.924722\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'SC_seq_1.npz')\n",
    "results2 =   np.load( 'SC_seq_0.npz')\n",
    "results3 =   np.load( 'SC_seq_2.npz')\n",
    "results4 =   np.load( 'SC_seq_3.npz')\n",
    "results5 =   np.load( 'SC_seq_4.npz')\n",
    "print ('mean AUC: %f' %  (  (np.mean( results4[ 'AUCs' ] )  +  np.mean( results5[ 'AUCs' ] )  + np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/5     ) )\n",
    "print ('mean ACC: %f' %   (  ( np.mean( results4[ 'ACCs' ] )  + np.mean(  results5[ 'ACCs' ] )  +   np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/5) )\n",
    "print ('mean MCC : %f' %  ( ( np.mean( results4[ 'MCCs' ] )  + np.mean(  results5[ 'MCCs' ] )  + np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/5))\n",
    "print('mean TPR:%f'%    (( np.mean( results4[ 'TPRs' ] )  + np.mean(  results5[ 'TPRs' ] )  + np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/5))\n",
    "print('mean FPR:%f'%   (  (np.mean( results4[ 'FPRs' ] )  + np.mean(  results5[ 'FPRs' ] )  + np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/5))\n",
    "print('mean Pre:%f'%    ( (np.mean( results4[ 'Precs' ] )  + np.mean(  results5[ 'Precs' ] )  + np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/5))\n",
    "print('mean F1:%f'%    (  (np.mean( results4[ 'F1s' ] )  + np.mean(  results5[ 'F1s' ] )  +np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
