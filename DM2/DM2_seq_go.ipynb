{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 512\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "protein2go =  load_dict('DM2prot2go.pkl')\n",
    "protein2seq = load_dict('DM2prot2seq.pkl')\n",
    "protein2onehot = {}\n",
    "prot2emb = {}\n",
    "\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,768))\n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "        else:\n",
    "            feature = np.zeros((1,768))\n",
    "        if count + feature.shape[0] > max_go_len:\n",
    "            break\n",
    "        X_go1 = np.concatenate((X_go1,feature ))    \n",
    "        count += feature.shape[0]   \n",
    "    prot2emb[key] =  X_go1[1:]  \n",
    "\n",
    "for key, value in protein2seq.items():\n",
    "    protein2onehot[key] =  protein_one_hot(value, max_seq_len) \n",
    "    \n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.protein2go =  load_dict('DM2prot2go.pkl')\n",
    "        self.protein2seq = load_dict('DM2prot2seq.pkl')\n",
    "        self.read_ppi()\n",
    "        self.protein2onehot = protein2onehot\n",
    "#         self.onehot_seqs()\n",
    "        self.prot2emb = prot2emb\n",
    "#         self.prot2embedding() \n",
    "         \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "     \n",
    "    \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        y = np.empty((self.batch_size))\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "             \n",
    "            \n",
    "            \n",
    "        return [X_go1, X_go2,  X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "         \n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "           \n",
    "        return [X_go1, X_go2,  X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 512, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 512, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 512, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 512, 64)      20544       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 512, 64)      12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 512, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 512, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 512, 64)      20544       conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 512, 64)      12352       conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 16)     1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 16)     784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 16)     1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 16)     784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512, 256)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512, 128)     320256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 256)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 512, 128)     320256      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 64)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 64)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 256)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512, 256)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000, 64)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 64)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          768         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          640         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          768         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          640         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           1064        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           1064        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1152)         0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1152)         0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 576)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 576)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          295168      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          295168      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          147712      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          147712      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            513         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,086,177\n",
      "Trainable params: 5,086,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(4)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([x_a, x_b, x_c, x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "     \n",
    " \n",
    "     \n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters//4, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters//4, gru_units)\n",
    "     \n",
    "    \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go, left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    optimizer = Lookahead(RAdam())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n",
      "AUC: 0.960857\n",
      "ACC: 0.893939\n",
      "MCC : 0.785709\n",
      "TPR:0.945946\n",
      "FPR:0.172414\n",
      "Pre:0.875000\n",
      "F1:0.909091\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.968719\n",
      "ACC: 0.843750\n",
      "MCC : 0.725932\n",
      "TPR:1.000000\n",
      "FPR:0.303030\n",
      "Pre:0.756098\n",
      "F1:0.861111\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.818719\n",
      "ACC: 0.734375\n",
      "MCC : 0.462682\n",
      "TPR:0.689655\n",
      "FPR:0.228571\n",
      "Pre:0.714286\n",
      "F1:0.701754\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.948242\n",
      "ACC: 0.750000\n",
      "MCC : 0.556038\n",
      "TPR:0.968750\n",
      "FPR:0.468750\n",
      "Pre:0.673913\n",
      "F1:0.794872\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.986275\n",
      "ACC: 0.906250\n",
      "MCC : 0.824621\n",
      "TPR:1.000000\n",
      "FPR:0.200000\n",
      "Pre:0.850000\n",
      "F1:0.918919\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.956012\n",
      "ACC: 0.843750\n",
      "MCC : 0.691443\n",
      "TPR:0.774194\n",
      "FPR:0.090909\n",
      "Pre:0.888889\n",
      "F1:0.827586\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.908824\n",
      "ACC: 0.843750\n",
      "MCC : 0.686403\n",
      "TPR:0.800000\n",
      "FPR:0.117647\n",
      "Pre:0.857143\n",
      "F1:0.827586\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.961310\n",
      "ACC: 0.828125\n",
      "MCC : 0.667849\n",
      "TPR:0.972222\n",
      "FPR:0.357143\n",
      "Pre:0.777778\n",
      "F1:0.864198\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.985294\n",
      "ACC: 0.906250\n",
      "MCC : 0.819608\n",
      "TPR:0.852941\n",
      "FPR:0.033333\n",
      "Pre:0.966667\n",
      "F1:0.906250\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.955956\n",
      "ACC: 0.906250\n",
      "MCC : 0.807557\n",
      "TPR:0.851852\n",
      "FPR:0.054054\n",
      "Pre:0.920000\n",
      "F1:0.884615\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.945021\n",
      "mean ACC: 0.845644\n",
      "mean MCC : 0.702784\n",
      "mean TPR:0.885556\n",
      "mean FPR:0.202585\n",
      "mean Pre:0.827977\n",
      "mean F1:0.849598\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.995340\n",
      "ACC: 0.939394\n",
      "MCC : 0.882023\n",
      "TPR:1.000000\n",
      "FPR:0.137931\n",
      "Pre:0.902439\n",
      "F1:0.948718\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.979492\n",
      "ACC: 0.906250\n",
      "MCC : 0.827170\n",
      "TPR:1.000000\n",
      "FPR:0.187500\n",
      "Pre:0.842105\n",
      "F1:0.914286\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.960899\n",
      "ACC: 0.906250\n",
      "MCC : 0.812317\n",
      "TPR:0.903226\n",
      "FPR:0.090909\n",
      "Pre:0.903226\n",
      "F1:0.903226\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.917969\n",
      "ACC: 0.765625\n",
      "MCC : 0.601417\n",
      "TPR:1.000000\n",
      "FPR:0.468750\n",
      "Pre:0.680851\n",
      "F1:0.810127\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.945098\n",
      "ACC: 0.953125\n",
      "MCC : 0.906145\n",
      "TPR:0.970588\n",
      "FPR:0.066667\n",
      "Pre:0.942857\n",
      "F1:0.956522\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.969608\n",
      "ACC: 0.906250\n",
      "MCC : 0.811765\n",
      "TPR:0.911765\n",
      "FPR:0.100000\n",
      "Pre:0.911765\n",
      "F1:0.911765\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.963725\n",
      "ACC: 0.921875\n",
      "MCC : 0.843858\n",
      "TPR:0.911765\n",
      "FPR:0.066667\n",
      "Pre:0.939394\n",
      "F1:0.925373\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.968627\n",
      "ACC: 0.953125\n",
      "MCC : 0.906511\n",
      "TPR:0.941176\n",
      "FPR:0.033333\n",
      "Pre:0.969697\n",
      "F1:0.955224\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.731373\n",
      "ACC: 0.687500\n",
      "MCC : 0.372549\n",
      "TPR:0.666667\n",
      "FPR:0.294118\n",
      "Pre:0.666667\n",
      "F1:0.666667\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.937434\n",
      "ACC: 0.718750\n",
      "MCC : 0.532921\n",
      "TPR:0.956522\n",
      "FPR:0.414634\n",
      "Pre:0.564103\n",
      "F1:0.709677\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.936957\n",
      "mean ACC: 0.865814\n",
      "mean MCC : 0.749668\n",
      "mean TPR:0.926171\n",
      "mean FPR:0.186051\n",
      "mean Pre:0.832310\n",
      "mean F1:0.870158\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.918274\n",
      "ACC: 0.833333\n",
      "MCC : 0.669439\n",
      "TPR:0.787879\n",
      "FPR:0.121212\n",
      "Pre:0.866667\n",
      "F1:0.825397\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.966967\n",
      "ACC: 0.937500\n",
      "MCC : 0.874903\n",
      "TPR:0.962963\n",
      "FPR:0.081081\n",
      "Pre:0.896552\n",
      "F1:0.928571\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.994141\n",
      "ACC: 0.984375\n",
      "MCC : 0.969223\n",
      "TPR:1.000000\n",
      "FPR:0.031250\n",
      "Pre:0.969697\n",
      "F1:0.984615\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.987292\n",
      "ACC: 0.953125\n",
      "MCC : 0.909921\n",
      "TPR:1.000000\n",
      "FPR:0.096774\n",
      "Pre:0.916667\n",
      "F1:0.956522\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.962551\n",
      "ACC: 0.937500\n",
      "MCC : 0.870727\n",
      "TPR:0.973684\n",
      "FPR:0.115385\n",
      "Pre:0.925000\n",
      "F1:0.948718\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.965787\n",
      "ACC: 0.906250\n",
      "MCC : 0.818060\n",
      "TPR:0.969697\n",
      "FPR:0.161290\n",
      "Pre:0.864865\n",
      "F1:0.914286\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.976042\n",
      "ACC: 0.875000\n",
      "MCC : 0.735789\n",
      "TPR:0.975000\n",
      "FPR:0.291667\n",
      "Pre:0.847826\n",
      "F1:0.906977\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.870968\n",
      "ACC: 0.734375\n",
      "MCC : 0.503990\n",
      "TPR:0.903226\n",
      "FPR:0.424242\n",
      "Pre:0.666667\n",
      "F1:0.767123\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.993355\n",
      "ACC: 0.968750\n",
      "MCC : 0.933047\n",
      "TPR:1.000000\n",
      "FPR:0.046512\n",
      "Pre:0.913043\n",
      "F1:0.954545\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.773216\n",
      "ACC: 0.703125\n",
      "MCC : 0.467214\n",
      "TPR:0.484848\n",
      "FPR:0.064516\n",
      "Pre:0.888889\n",
      "F1:0.627451\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.940859\n",
      "mean ACC: 0.883333\n",
      "mean MCC : 0.775231\n",
      "mean TPR:0.905730\n",
      "mean FPR:0.143393\n",
      "mean Pre:0.875587\n",
      "mean F1:0.881421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "dataset_name = 'DM2'\n",
    "for rep in range(3):\n",
    "    n_splits = 10\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "     \n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "#         train_pairs_file = 'CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'CV/test'+str(rep)+'-'+str(split)\n",
    "#         valid_pairs_file = 'CV/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 48\n",
    "#         train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "#         valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'CV/GoplusSeq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        #  max_queue_size=16, workers=8, use_multiprocessing=True,\n",
    "#         hist = model.fit_generator(generator=train_generator,\n",
    "#                     validation_data=valid_generator,\n",
    "#                     epochs = 100,verbose=1,callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('seq_and_go__incep_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.949705\n",
      "mean ACC: 0.887232\n",
      "mean MCC : 0.783003\n",
      "mean TPR:0.905616\n",
      "mean FPR:0.130484\n",
      "mean Pre:0.880701\n",
      "mean F1:0.886034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'seq_and_go__incep_0.npz')\n",
    "results2 =   np.load( 'seq_and_go__incep_1.npz')\n",
    "results3 =   np.load( 'seq_and_go__incep_2.npz')\n",
    "print ('mean AUC: %f' %  ( (np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/3     ) )\n",
    "print ('mean ACC: %f' %   ( (np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/3) )\n",
    "print ('mean MCC : %f' %  (  (np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/3))\n",
    "print('mean TPR:%f'%    ((np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/3))\n",
    "print('mean FPR:%f'%   ( (np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/3))\n",
    "print('mean Pre:%f'%    ((np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/3))\n",
    "print('mean F1:%f'%    ((np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 512, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 512, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 512, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 512, 64)      20544       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 512, 64)      12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 512, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 512, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 512, 64)      20544       conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 512, 64)      12352       conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 16)     1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 16)     784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 16)     1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 16)     784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512, 256)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512, 128)     320256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 256)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 512, 128)     320256      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 64)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 64)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 256)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512, 256)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000, 64)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 64)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          768         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          640         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          768         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          640         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           1064        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           1064        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1152)         0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1152)         0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 576)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 576)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          295168      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          295168      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          147712      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          147712      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,085,664\n",
      "Trainable params: 5,085,664\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('CV/GoplusSeq0-0.hdf5')\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('dense_7').output)\n",
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 0\n",
    "split = 0\n",
    "test_pairs_file = 'CV/test'+str(rep)+'-'+str(split)\n",
    "batch_size = 32\n",
    "         \n",
    "test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_pairs_file, 'r') as f:\n",
    "    test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "test_len = len(test_ppi_pairs) \n",
    "list_IDs_temp = np.arange(test_len)\n",
    "test_x, y_test = test_generator.all_data(list_IDs_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = intermediate_layer_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66, 512)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 65 nearest neighbors...\n",
      "[t-SNE] Indexed 66 samples in 0.000s...\n",
      "[t-SNE] Computed neighbors for 66 samples in 0.002s...\n",
      "[t-SNE] Computed conditional probabilities for sample 66 / 66\n",
      "[t-SNE] Mean sigma: 4.856797\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 48.365337\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.318576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0, verbose=1, perplexity=40, n_iter=1000)\n",
    "X_2d = tsne.fit_transform(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f25fb0c3a20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "colors = 'r',   'b'\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "for i, c, label in zip([0,1], colors, ['Positive', 'Negative']):\n",
    "    plt.scatter(X_2d[y_test == i, 0], X_2d [ y_test == i, 1], c=c, label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.929564957244453 0.8681186868686869 0.7386845622047421 0.8584363714317109 0.12549765624418144 0.8832711757306612 0.8663409281274809\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "aucs = 0\n",
    "accs = 0\n",
    "mccs = 0\n",
    "tprs = 0\n",
    "fprs = 0\n",
    "pres = 0\n",
    "f1s  = 0\n",
    "for i in range(3):\n",
    "    for j in range(10):\n",
    "        results_file = 'CV/sent_fusion_'+str(i)+'-'+str(j)+'.npz'\n",
    "        results = np.load(results_file)\n",
    "        aucs += results['AUCs']\n",
    "        accs += results['ACCs']\n",
    "        mccs += results['MCCs']\n",
    "        tprs += results['TPRs']\n",
    "        fprs += results['FPRs']\n",
    "        pres += results['Precs']\n",
    "        f1s += results['F1s']\n",
    "        \n",
    "print(aucs/30, accs/30, mccs/30, tprs/30, fprs/30, pres/30, f1s/30)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
