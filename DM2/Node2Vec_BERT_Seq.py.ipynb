{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermVectors:\n",
    "    def __init__(self):\n",
    "        self.termVectorDict=dict()\n",
    "        self.terms=None\n",
    "        self.vectors=None\n",
    "\n",
    "    def parse_term_embedding_file(self,file_path):\n",
    "        with open(file_path) as f:\n",
    "            lines=f.readlines()\n",
    "            self.vectors=lines[1:] # 第一个是term的数量，不是termID\n",
    "            terms = []\n",
    "            for line in lines[1:]:\n",
    "                term=line.split()[0]\n",
    "                terms.append(term)\n",
    "            self.terms = terms\n",
    "\n",
    "    def str_to_vector(self,Str):\n",
    "        Str = Str.strip('\\n')\n",
    "        nums = Str.split()\n",
    "        vec = []\n",
    "        for num in nums:\n",
    "            vec.append(float(num))\n",
    "        return np.array(vec)\n",
    "\n",
    "    def construct_term_vector_dict(self):\n",
    "        for term in self.terms:\n",
    "            termindex = self.terms.index(term)\n",
    "            line=self.vectors[termindex]\n",
    "            s =line[len(term):].lstrip(' ')\n",
    "            s_vec =self.str_to_vector(s)\n",
    "            self.termVectorDict[term]=s_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29699\n",
      "4202\n",
      "11148\n"
     ]
    }
   ],
   "source": [
    "# construct Node2Vec termVector\n",
    "BP_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/BP.emb'\n",
    "CC_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/CC.emb'\n",
    "MF_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/MF.emb'\n",
    "Node2Vec_dim = 300\n",
    "\n",
    "\n",
    "BPTermVectors = TermVectors()\n",
    "BPTermVectors.parse_term_embedding_file(BP_TERM_EMB_FILE_PATH)\n",
    "BPTermVectors.construct_term_vector_dict()\n",
    "print(len(BPTermVectors.termVectorDict))\n",
    "\n",
    "CCTermVectors = TermVectors()\n",
    "CCTermVectors.parse_term_embedding_file(CC_TERM_EMB_FILE_PATH)\n",
    "CCTermVectors.construct_term_vector_dict()\n",
    "print(len(CCTermVectors.termVectorDict))\n",
    "\n",
    "MFTermVectors = TermVectors()\n",
    "MFTermVectors.parse_term_embedding_file(MF_TERM_EMB_FILE_PATH)\n",
    "MFTermVectors.construct_term_vector_dict()\n",
    "print(len(MFTermVectors.termVectorDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read go.obo obtain ontology type\n",
    "id_type_dicts = {}\n",
    "obo_file = '../cross-species/go.obo'\n",
    "fp=open(obo_file,'r')\n",
    "obo_txt=fp.read()\n",
    "fp.close()\n",
    "obo_txt=obo_txt[obo_txt.find(\"[Term]\")-1:]\n",
    "obo_txt=obo_txt[:obo_txt.find(\"[Typedef]\")]\n",
    "# obo_dict=parse_obo_txt(obo_txt)\n",
    "id_type_dicts = {}\n",
    "for Term_txt in obo_txt.split(\"[Term]\\n\"):\n",
    "    if not Term_txt.strip():\n",
    "        continue\n",
    "    name = ''\n",
    "    ids = []\n",
    "    for line in Term_txt.splitlines():\n",
    "        if   line.startswith(\"id: \"):\n",
    "            ids.append(line[len(\"id: \"):])     \n",
    "        elif line.startswith(\"namespace: \"):\n",
    "             name=line[len(\"namespace: \"):]\n",
    "        elif line.startswith(\"alt_id: \"):\n",
    "            ids.append(line[len(\"alt_id: \"):])\n",
    "    \n",
    "    for t_id in ids:\n",
    "        id_type_dicts[t_id] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 512\n",
    "max_seq_len = 1000\n",
    "max_node_len = 256\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "protein2go = load_dict('DM2prot2go.pkl')\n",
    "prot2nodevec = {}\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,Node2Vec_dim))\n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        if go.startswith('GO'):\n",
    "            if id_type_dicts[go] == 'biological_process':\n",
    "                termVectors = BPTermVectors\n",
    "                term_ID=go[3:].lstrip('0')\n",
    "                if term_ID in termVectors.termVectorDict.keys():\n",
    "                    feature = termVectors.termVectorDict[term_ID].reshape(1, Node2Vec_dim)\n",
    "                else:\n",
    "                    feature = np.zeros((1,Node2Vec_dim))\n",
    "                \n",
    "            elif id_type_dicts[go] == 'cellular_component':\n",
    "                termVectors = CCTermVectors\n",
    "                term_ID=go[3:].lstrip('0')\n",
    "                if term_ID in termVectors.termVectorDict.keys():\n",
    "                    feature = termVectors.termVectorDict[term_ID].reshape(1, Node2Vec_dim)\n",
    "                else:\n",
    "                    feature = np.zeros((1,Node2Vec_dim))\n",
    "            elif id_type_dicts[go] == 'molecular_function':\n",
    "                termVectors = MFTermVectors\n",
    "                term_ID=go[3:].lstrip('0')\n",
    "                if term_ID in termVectors.termVectorDict.keys():\n",
    "                    feature = termVectors.termVectorDict[term_ID].reshape(1, Node2Vec_dim)\n",
    "                else:\n",
    "                    feature = np.zeros((1,Node2Vec_dim))\n",
    "            else:\n",
    "                 feature = np.zeros((1,Node2Vec_dim))\n",
    "\n",
    "\n",
    "            if count + feature.shape[0] > max_node_len:\n",
    "                break\n",
    "            X_go1 = np.concatenate((X_go1,feature ))    \n",
    "            count += feature.shape[0]\n",
    "    prot2nodevec[key] =  X_go1[1:]  \n",
    "prot2emb = {}\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,768))\n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        if go.startswith('GO'):\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            if count + feature.shape[0] > max_go_len:\n",
    "                break\n",
    "            X_go1 = np.concatenate((X_go1,feature ))    \n",
    "            count += feature.shape[0]\n",
    "    prot2emb[key] =  X_go1[1:]   \n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.max_node_len = max_node_len\n",
    "        self.protein2go =  load_dict('DM2prot2go.pkl')\n",
    "        self.protein2seq = load_dict('DM2prot2seq.pkl')\n",
    "        self.read_ppi()\n",
    "        self.prot2emb = prot2emb\n",
    "#         self.prot2embedding()\n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.prot2nodevec = prot2nodevec\n",
    "#         self.prot2nodevec_fun()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def prot2nodevec_fun(self):\n",
    "        for key, value in self.protein2go.items():\n",
    "            X_go1 =  np.zeros((1,Node2Vec_dim))\n",
    "            allgos = value.split(',') \n",
    "            allgos = list(set(allgos))\n",
    "            count = 0\n",
    "            for  go in  allgos:\n",
    "                if go.startswith('GO'):\n",
    "                    if id_type_dicts[go] == '':\n",
    "                        termVectors = BPTermVectors\n",
    "                        term_ID=go[3:].lstrip('0')\n",
    "                        feature = termVectors.termVectorDict[term_ID]\n",
    "                    elif id_type_dicts[go] == '':\n",
    "                        termVectors = CCTermVectors\n",
    "                        term_ID=go[3:].lstrip('0')\n",
    "                        feature = termVectors.termVectorDict[term_ID]\n",
    "                    elif id_type_dicts[go] == '':\n",
    "                        termVectors = MFTermVectors\n",
    "                        term_ID=go[3:].lstrip('0')\n",
    "                        feature = termVectors.termVectorDict[term_ID]\n",
    "                    else:\n",
    "                        feature = np.zeros((1,Node2Vec_dim))\n",
    "                    \n",
    "                     \n",
    "                    if count + feature.shape[0] > max_go_len:\n",
    "                        break\n",
    "                    X_go1 = np.concatenate((X_go1,feature ))    \n",
    "                    count += feature.shape[0]\n",
    "            self.prot2nodevec[key] =  X_go1[1:]   \n",
    "        \n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "            \n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def prot2embedding(self):\n",
    "        for key, value in self.protein2go.items():\n",
    "            X_go1 =  np.zeros((1,768))\n",
    "            allgos = value.split(',') \n",
    "            allgos = list(set(allgos))\n",
    "            count = 0\n",
    "            for  go in  allgos:\n",
    "                if go.startswith('GO'):\n",
    "                    feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "                    if count + feature.shape[0] > max_go_len:\n",
    "                        break\n",
    "                    X_go1 = np.concatenate((X_go1,feature ))    \n",
    "                    count += feature.shape[0]\n",
    "            self.prot2emb[key] =  X_go1[1:]   \n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        y = np.empty((self.batch_size))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "        X_node1 = np.empty((self.batch_size, self.max_node_len,Node2Vec_dim))\n",
    "        X_node2 = np.empty((self.batch_size, self.max_node_len,Node2Vec_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_node = self.prot2nodevec[p1]\n",
    "            X_node1[i,:prot1emb_node.shape[0]] = prot1emb_node\n",
    "            \n",
    "            prot2emb_node = self.prot2nodevec[p2]\n",
    "            X_node2[i,:prot2emb_node.shape[0]] = prot2emb_node\n",
    "            \n",
    "            \n",
    "            \n",
    "     \n",
    "        return [X_go1,X_go2,  X_node1, X_node2, X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "         \n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "        X_node1 = np.empty((len(list_IDs_temp), self.max_node_len,Node2Vec_dim))\n",
    "        X_node2 = np.empty((len(list_IDs_temp), self.max_node_len,Node2Vec_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_node = self.prot2nodevec[p1]\n",
    "            X_node1[i,:prot1emb_node.shape[0]] = prot1emb_node\n",
    "            \n",
    "            prot2emb_node = self.prot2nodevec[p2]\n",
    "            X_node2[i,:prot2emb_node.shape[0]] = prot2emb_node\n",
    "            \n",
    "  \n",
    "        return [X_go1,X_go2, X_node1, X_node2, X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 32)      73760       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 512, 32)      24608       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 512, 32)      73760       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 512, 32)      24608       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 256, 32)      28832       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 256, 32)      9632        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 256, 32)      28832       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 256, 32)      9632        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 32)     1952        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 32)     672         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 32)     1952        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 32)     672         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 512, 32)      5152        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 512, 32)      3104        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 512, 32)      73760       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 512, 32)      24608       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 512, 32)      5152        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 512, 32)      3104        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 32)      73760       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 32)      24608       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 256, 32)      5152        conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 256, 32)      3104        conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 256, 32)      28832       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 256, 32)      9632        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 256, 32)      5152        conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 256, 32)      3104        conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 256, 32)      28832       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, 256, 32)      9632        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 32)     5152        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 32)     3104        conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 32)     1952        input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 32)     672         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 32)     5152        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 32)     3104        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 32)     1952        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 32)     672         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512, 128)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 128)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 256, 128)     0           conv1d_26[0][0]                  \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "                                                                 conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256, 128)     0           conv1d_32[0][0]                  \n",
      "                                                                 conv1d_34[0][0]                  \n",
      "                                                                 conv1d_35[0][0]                  \n",
      "                                                                 conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 128)    0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 128)    0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 128)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512, 128)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 256, 128)     0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 256, 128)     0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000, 128)    0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 128)    0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512, 128)     74496       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 512, 128)     74496       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 256, 128)     74496       dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 256, 128)     74496       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    74496       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    74496       dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 256, 128)     0           bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 256, 128)     0           bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 128)          640         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          640         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 128)          640         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          640         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_9 (Glo (None, 128)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_9 (GlobalM (None, 128)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 128)          384         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_10 (Gl (None, 128)          0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 128)          384         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_11 (Gl (None, 128)          0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 128)          384         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_12 (Gl (None, 128)          0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 128)          384         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 128)          1128        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 128)          0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 128)          1128        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 768)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 768)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 768)          0           global_average_pooling1d_9[0][0] \n",
      "                                                                 global_max_pooling1d_9[0][0]     \n",
      "                                                                 attention_9[0][0]                \n",
      "                                                                 global_average_pooling1d_10[0][0]\n",
      "                                                                 global_max_pooling1d_10[0][0]    \n",
      "                                                                 attention_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 768)          0           global_average_pooling1d_11[0][0]\n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 attention_11[0][0]               \n",
      "                                                                 global_average_pooling1d_12[0][0]\n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 768)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 768)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          196864      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          196864      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          196864      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          196864      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          196864      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 1536)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         1573888     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         1049600     dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          524800      dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1)            513         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 5,392,929\n",
      "Trainable params: 5,392,929\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(2)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    " \n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([ x_a, x_b, x_c,  x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_cnn_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    \n",
    "    x = Concatenate()([ x_a, x_b, x_c])\n",
    "    x = Dense(256)(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "    left_input_node = Input(shape=(max_node_len,Node2Vec_dim))\n",
    "    right_input_node = Input(shape=(max_node_len,Node2Vec_dim))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "\n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters, gru_units)\n",
    "    \n",
    "    left_x_node = build_cnn_gru_model(left_input_node, con_filters, gru_units)\n",
    "    right_x_node = build_cnn_gru_model(right_input_node, con_filters,gru_units)\n",
    "    \n",
    "   \n",
    "    \n",
    "   \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go, left_x_node,  right_x_node, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go, left_input_node, right_input_node,     left_input_seq, right_input_seq], output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "# siamese_a = create_share_model()\n",
    "# siamese_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 27s 3s/step - loss: 0.9679 - acc: 0.5125 - val_loss: 0.7089 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.45833, saving model to CV/node_go_seq2-0.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 8s 767ms/step - loss: 0.7165 - acc: 0.5021 - val_loss: 0.6641 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.45833 to 0.68750, saving model to CV/node_go_seq2-0.hdf5\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 10s 953ms/step - loss: 0.6435 - acc: 0.6167 - val_loss: 0.5037 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.68750 to 0.79167, saving model to CV/node_go_seq2-0.hdf5\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 9s 865ms/step - loss: 0.5285 - acc: 0.7083 - val_loss: 0.4470 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79167\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 8s 823ms/step - loss: 0.3763 - acc: 0.8354 - val_loss: 0.6138 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79167\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 8s 821ms/step - loss: 0.2639 - acc: 0.8771 - val_loss: 1.4576 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79167\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 8s 817ms/step - loss: 0.2242 - acc: 0.9083 - val_loss: 0.5726 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79167\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 9s 885ms/step - loss: 0.1197 - acc: 0.9542 - val_loss: 0.9736 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79167\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 9s 899ms/step - loss: 0.3129 - acc: 0.8938 - val_loss: 0.6083 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79167\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 8s 811ms/step - loss: 0.1403 - acc: 0.9458 - val_loss: 0.7777 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79167\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 9s 876ms/step - loss: 0.0841 - acc: 0.9667 - val_loss: 1.3449 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79167\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 8s 824ms/step - loss: 0.0606 - acc: 0.9729 - val_loss: 1.1182 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79167\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 8s 824ms/step - loss: 0.0484 - acc: 0.9750 - val_loss: 1.4721 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79167\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 8s 796ms/step - loss: 0.1184 - acc: 0.9479 - val_loss: 0.9203 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79167\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 8s 824ms/step - loss: 0.1109 - acc: 0.9563 - val_loss: 0.6898 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79167\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 8s 823ms/step - loss: 0.1688 - acc: 0.9375 - val_loss: 0.6277 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79167\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 8s 798ms/step - loss: 0.1499 - acc: 0.9396 - val_loss: 0.6813 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79167\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 8s 825ms/step - loss: 0.0347 - acc: 0.9917 - val_loss: 1.3018 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79167\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 8s 772ms/step - loss: 0.1035 - acc: 0.9667 - val_loss: 1.2148 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79167\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 8s 797ms/step - loss: 0.0512 - acc: 0.9792 - val_loss: 1.0067 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79167\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 8s 765ms/step - loss: 0.1483 - acc: 0.9417 - val_loss: 1.0973 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79167\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 9s 890ms/step - loss: 0.1356 - acc: 0.9458 - val_loss: 1.6151 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79167\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 9s 875ms/step - loss: 0.3133 - acc: 0.8833 - val_loss: 0.7441 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79167\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.865932\n",
      "ACC: 0.848485\n",
      "MCC : 0.702147\n",
      "TPR:0.787879\n",
      "FPR:0.090909\n",
      "Pre:0.896552\n",
      "F1:0.838710\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 25s 3s/step - loss: 1.0058 - acc: 0.5083 - val_loss: 0.6838 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56250, saving model to CV/node_go_seq2-1.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 7s 717ms/step - loss: 0.6814 - acc: 0.5646 - val_loss: 0.7405 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.56250\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 9s 872ms/step - loss: 0.5799 - acc: 0.6917 - val_loss: 0.6110 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.56250 to 0.79167, saving model to CV/node_go_seq2-1.hdf5\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 8s 792ms/step - loss: 0.5407 - acc: 0.7292 - val_loss: 0.6542 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79167\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 8s 778ms/step - loss: 0.5008 - acc: 0.7333 - val_loss: 0.6084 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79167\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 8s 785ms/step - loss: 0.3703 - acc: 0.8542 - val_loss: 0.9061 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79167\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 8s 780ms/step - loss: 0.2574 - acc: 0.9062 - val_loss: 0.7605 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79167\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 8s 771ms/step - loss: 0.1339 - acc: 0.9521 - val_loss: 1.4252 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79167\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 8s 778ms/step - loss: 0.2641 - acc: 0.9000 - val_loss: 0.8946 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79167\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 8s 788ms/step - loss: 0.2911 - acc: 0.8937 - val_loss: 0.7823 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79167\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 8s 838ms/step - loss: 0.1076 - acc: 0.9646 - val_loss: 1.0071 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79167\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 8s 812ms/step - loss: 0.6988 - acc: 0.8063 - val_loss: 0.6410 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79167\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 8s 794ms/step - loss: 0.3619 - acc: 0.8167 - val_loss: 0.7307 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79167\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 8s 772ms/step - loss: 0.3996 - acc: 0.8396 - val_loss: 0.8004 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79167\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 7s 732ms/step - loss: 0.2163 - acc: 0.9292 - val_loss: 0.9047 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79167\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 7s 694ms/step - loss: 0.2253 - acc: 0.9021 - val_loss: 0.8037 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79167\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 7s 744ms/step - loss: 0.0944 - acc: 0.9688 - val_loss: 0.9221 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79167\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 8s 763ms/step - loss: 0.0426 - acc: 0.9896 - val_loss: 1.2749 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79167\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 9s 950ms/step - loss: 0.0514 - acc: 0.9771 - val_loss: 1.2613 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79167\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 817ms/step - loss: 0.0585 - acc: 0.9896 - val_loss: 1.1619 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79167\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 9s 874ms/step - loss: 0.0508 - acc: 0.9833 - val_loss: 1.0606 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79167\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 8s 768ms/step - loss: 0.0341 - acc: 0.9917 - val_loss: 1.2403 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79167\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 8s 814ms/step - loss: 0.0292 - acc: 0.9875 - val_loss: 1.3805 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79167\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.899900\n",
      "ACC: 0.765625\n",
      "MCC : 0.542905\n",
      "TPR:0.481481\n",
      "FPR:0.027027\n",
      "Pre:0.928571\n",
      "F1:0.634146\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 28s 3s/step - loss: 1.2503 - acc: 0.4875 - val_loss: 0.8055 - val_acc: 0.3750\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.37500, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 8s 795ms/step - loss: 0.6720 - acc: 0.5875 - val_loss: 0.6336 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.37500 to 0.62500, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 9s 880ms/step - loss: 0.6877 - acc: 0.5729 - val_loss: 0.6132 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.62500 to 0.72917, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 9s 860ms/step - loss: 0.5554 - acc: 0.6979 - val_loss: 0.5508 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.72917\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 9s 853ms/step - loss: 0.4414 - acc: 0.7937 - val_loss: 0.6089 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.72917\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 8s 834ms/step - loss: 0.4743 - acc: 0.7792 - val_loss: 0.5156 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72917\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 8s 770ms/step - loss: 0.5238 - acc: 0.7312 - val_loss: 0.4631 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72917 to 0.75000, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 9s 876ms/step - loss: 0.3230 - acc: 0.8688 - val_loss: 0.3944 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.75000 to 0.79167, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 9s 884ms/step - loss: 0.2526 - acc: 0.8937 - val_loss: 0.3824 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79167\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 8s 811ms/step - loss: 0.1957 - acc: 0.9187 - val_loss: 0.5170 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.79167 to 0.81250, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 9s 877ms/step - loss: 0.1867 - acc: 0.9188 - val_loss: 0.3871 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.81250 to 0.83333, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 9s 863ms/step - loss: 0.1109 - acc: 0.9646 - val_loss: 0.5695 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83333\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 9s 872ms/step - loss: 0.1095 - acc: 0.9646 - val_loss: 0.5822 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83333\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 8s 819ms/step - loss: 0.1215 - acc: 0.9563 - val_loss: 0.4793 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.83333 to 0.85417, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 8s 805ms/step - loss: 0.0975 - acc: 0.9563 - val_loss: 0.6090 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85417\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 7s 726ms/step - loss: 0.1129 - acc: 0.9563 - val_loss: 0.6264 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85417\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 8s 849ms/step - loss: 0.0949 - acc: 0.9708 - val_loss: 0.6003 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85417\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 7s 717ms/step - loss: 0.0939 - acc: 0.9604 - val_loss: 0.5483 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85417\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 8s 771ms/step - loss: 0.1105 - acc: 0.9521 - val_loss: 0.7150 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85417\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 9s 870ms/step - loss: 0.0485 - acc: 0.9875 - val_loss: 1.0160 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85417\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 8s 775ms/step - loss: 0.2122 - acc: 0.9271 - val_loss: 0.7333 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85417\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 8s 794ms/step - loss: 0.1308 - acc: 0.9583 - val_loss: 0.5523 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.85417 to 0.87500, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 9s 860ms/step - loss: 0.0501 - acc: 0.9792 - val_loss: 1.1086 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87500\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 8s 764ms/step - loss: 0.0472 - acc: 0.9813 - val_loss: 1.0464 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87500\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 8s 750ms/step - loss: 0.0983 - acc: 0.9646 - val_loss: 0.7970 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87500\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 8s 832ms/step - loss: 0.0500 - acc: 0.9875 - val_loss: 0.5389 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.87500 to 0.89583, saving model to CV/node_go_seq2-2.hdf5\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 7s 689ms/step - loss: 0.0388 - acc: 0.9833 - val_loss: 0.9596 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.89583\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.0688 - acc: 0.9771 - val_loss: 0.9481 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.89583\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 9s 901ms/step - loss: 0.1847 - acc: 0.9354 - val_loss: 0.4984 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.89583\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 7s 747ms/step - loss: 0.0608 - acc: 0.9812 - val_loss: 0.5477 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.89583\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 8s 773ms/step - loss: 0.1144 - acc: 0.9688 - val_loss: 0.7662 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.89583\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 9s 887ms/step - loss: 0.1988 - acc: 0.9375 - val_loss: 0.3650 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.89583\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 9s 862ms/step - loss: 0.1177 - acc: 0.9542 - val_loss: 0.4574 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.89583\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 8s 763ms/step - loss: 0.0389 - acc: 0.9792 - val_loss: 0.8916 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.89583\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 9s 861ms/step - loss: 0.0488 - acc: 0.9875 - val_loss: 0.8170 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.89583\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 7s 733ms/step - loss: 0.0344 - acc: 0.9875 - val_loss: 0.7347 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.89583\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 9s 909ms/step - loss: 0.0286 - acc: 0.9896 - val_loss: 0.6593 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.89583\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 808ms/step - loss: 0.0288 - acc: 0.9896 - val_loss: 0.6623 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.89583\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 8s 772ms/step - loss: 0.0078 - acc: 0.9979 - val_loss: 0.8076 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.89583\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 8s 799ms/step - loss: 0.0086 - acc: 0.9979 - val_loss: 0.7694 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.89583\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 9s 875ms/step - loss: 0.0414 - acc: 0.9917 - val_loss: 0.7137 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.89583\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 8s 818ms/step - loss: 0.0409 - acc: 0.9896 - val_loss: 0.8744 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.89583\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 8s 801ms/step - loss: 0.0514 - acc: 0.9833 - val_loss: 0.5077 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.89583\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 8s 836ms/step - loss: 0.0239 - acc: 0.9917 - val_loss: 0.6184 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.89583\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 9s 902ms/step - loss: 0.0108 - acc: 0.9979 - val_loss: 0.8398 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.89583\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 8s 839ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 1.3119 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.89583\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.970703\n",
      "ACC: 0.921875\n",
      "MCC : 0.847483\n",
      "TPR:0.968750\n",
      "FPR:0.125000\n",
      "Pre:0.885714\n",
      "F1:0.925373\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 32s 3s/step - loss: 1.1780 - acc: 0.4875 - val_loss: 0.7254 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50000, saving model to CV/node_go_seq2-3.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 8s 762ms/step - loss: 0.7248 - acc: 0.5500 - val_loss: 0.6782 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.50000\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 7s 684ms/step - loss: 0.6449 - acc: 0.6021 - val_loss: 0.6615 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.50000 to 0.68750, saving model to CV/node_go_seq2-3.hdf5\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 6s 641ms/step - loss: 0.5121 - acc: 0.7500 - val_loss: 0.7116 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.68750\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 6s 638ms/step - loss: 0.4310 - acc: 0.8229 - val_loss: 0.6722 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.68750\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 6s 622ms/step - loss: 0.3169 - acc: 0.8708 - val_loss: 0.8053 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.68750\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 0.2145 - acc: 0.9104 - val_loss: 0.9380 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.68750\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 7s 686ms/step - loss: 0.1745 - acc: 0.9396 - val_loss: 1.2035 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.68750\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 7s 652ms/step - loss: 0.3905 - acc: 0.8271 - val_loss: 1.2666 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.68750\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 7s 673ms/step - loss: 0.3306 - acc: 0.8479 - val_loss: 1.2660 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.68750\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 0.3421 - acc: 0.8292 - val_loss: 0.9315 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.68750\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 6s 639ms/step - loss: 0.2274 - acc: 0.9021 - val_loss: 1.0557 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.68750\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 7s 658ms/step - loss: 0.2803 - acc: 0.9042 - val_loss: 0.8068 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.68750\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 7s 659ms/step - loss: 0.2007 - acc: 0.9375 - val_loss: 0.7414 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.68750 to 0.70833, saving model to CV/node_go_seq2-3.hdf5\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 7s 655ms/step - loss: 0.0922 - acc: 0.9688 - val_loss: 1.3292 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.70833\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 8s 751ms/step - loss: 0.0976 - acc: 0.9688 - val_loss: 1.3557 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.70833\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 8s 805ms/step - loss: 0.1683 - acc: 0.9354 - val_loss: 0.7833 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.70833\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 9s 892ms/step - loss: 0.1894 - acc: 0.9271 - val_loss: 1.2458 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.70833\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 8s 808ms/step - loss: 0.3370 - acc: 0.8708 - val_loss: 0.6619 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.70833\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 8s 812ms/step - loss: 0.1105 - acc: 0.9521 - val_loss: 1.1358 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.70833 to 0.75000, saving model to CV/node_go_seq2-3.hdf5\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 8s 798ms/step - loss: 0.0425 - acc: 0.9833 - val_loss: 1.8458 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.75000\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 8s 827ms/step - loss: 0.0423 - acc: 0.9875 - val_loss: 1.4372 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.75000\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 8s 819ms/step - loss: 0.0595 - acc: 0.9771 - val_loss: 1.6089 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.75000\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 8s 813ms/step - loss: 0.0393 - acc: 0.9875 - val_loss: 1.3866 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.75000\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 7s 687ms/step - loss: 0.0331 - acc: 0.9917 - val_loss: 1.2539 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.75000\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 8s 834ms/step - loss: 0.0206 - acc: 0.9938 - val_loss: 1.8404 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.75000\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 8s 754ms/step - loss: 0.0336 - acc: 0.9917 - val_loss: 1.9384 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.75000\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 8s 758ms/step - loss: 0.0200 - acc: 0.9958 - val_loss: 1.6359 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.75000\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 8s 750ms/step - loss: 0.0229 - acc: 0.9896 - val_loss: 1.5276 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.75000\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 9s 911ms/step - loss: 0.0482 - acc: 0.9813 - val_loss: 1.4944 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.75000\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 8s 812ms/step - loss: 0.0269 - acc: 0.9896 - val_loss: 1.5918 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.75000\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 8s 802ms/step - loss: 0.0152 - acc: 0.9917 - val_loss: 1.9186 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.75000\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 8s 803ms/step - loss: 0.0680 - acc: 0.9813 - val_loss: 1.5038 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.75000\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 8s 844ms/step - loss: 0.0580 - acc: 0.9771 - val_loss: 1.0913 - val_acc: 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00034: val_acc did not improve from 0.75000\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 8s 816ms/step - loss: 0.0227 - acc: 0.9917 - val_loss: 1.6141 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.75000\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 8s 848ms/step - loss: 0.0118 - acc: 0.9979 - val_loss: 2.1259 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.75000\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 8s 824ms/step - loss: 0.0952 - acc: 0.9792 - val_loss: 1.1625 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.75000\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 8s 793ms/step - loss: 0.0729 - acc: 0.9729 - val_loss: 1.3190 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.75000 to 0.77083, saving model to CV/node_go_seq2-3.hdf5\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 8s 847ms/step - loss: 0.0698 - acc: 0.9771 - val_loss: 1.6402 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.77083\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 8s 842ms/step - loss: 0.0344 - acc: 0.9896 - val_loss: 1.7831 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.77083\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 7s 747ms/step - loss: 0.0247 - acc: 0.9875 - val_loss: 1.7455 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.77083\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 9s 858ms/step - loss: 0.0350 - acc: 0.9875 - val_loss: 1.7957 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.77083\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 8s 763ms/step - loss: 0.0163 - acc: 0.9896 - val_loss: 1.9076 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.77083\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 8s 815ms/step - loss: 0.0479 - acc: 0.9938 - val_loss: 1.4800 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.77083\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 8s 804ms/step - loss: 0.0280 - acc: 0.9875 - val_loss: 1.4032 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.77083\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 9s 850ms/step - loss: 0.0950 - acc: 0.9604 - val_loss: 0.9842 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.77083\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 8s 786ms/step - loss: 0.0556 - acc: 0.9750 - val_loss: 1.3151 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.77083\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 8s 770ms/step - loss: 0.0510 - acc: 0.9792 - val_loss: 1.8852 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.77083\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 8s 793ms/step - loss: 0.0114 - acc: 1.0000 - val_loss: 1.5969 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.77083\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 8s 788ms/step - loss: 0.0065 - acc: 0.9979 - val_loss: 2.0128 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.77083\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 7s 746ms/step - loss: 0.0310 - acc: 0.9938 - val_loss: 1.8864 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.77083\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 7s 741ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 2.0360 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.77083\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 7s 740ms/step - loss: 0.0050 - acc: 0.9979 - val_loss: 2.1282 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.77083\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 8s 787ms/step - loss: 0.0241 - acc: 0.9917 - val_loss: 2.5504 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.77083\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 7s 742ms/step - loss: 0.0566 - acc: 0.9792 - val_loss: 1.9373 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.77083\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 7s 712ms/step - loss: 0.0449 - acc: 0.9792 - val_loss: 1.6819 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.77083\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 7s 737ms/step - loss: 0.0514 - acc: 0.9854 - val_loss: 1.7121 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.77083\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 7s 717ms/step - loss: 0.0184 - acc: 0.9896 - val_loss: 1.9766 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.77083\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.984360\n",
      "ACC: 0.953125\n",
      "MCC : 0.906511\n",
      "TPR:0.969697\n",
      "FPR:0.064516\n",
      "Pre:0.941176\n",
      "F1:0.955224\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 34s 3s/step - loss: 1.2174 - acc: 0.5229 - val_loss: 0.7502 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.47917, saving model to CV/node_go_seq2-4.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 8s 849ms/step - loss: 0.7465 - acc: 0.5187 - val_loss: 0.6584 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.47917 to 0.50000, saving model to CV/node_go_seq2-4.hdf5\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 9s 916ms/step - loss: 0.5887 - acc: 0.6542 - val_loss: 0.5236 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.50000 to 0.72917, saving model to CV/node_go_seq2-4.hdf5\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 8s 751ms/step - loss: 0.5117 - acc: 0.7625 - val_loss: 0.5501 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.72917\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 9s 865ms/step - loss: 0.3777 - acc: 0.8187 - val_loss: 0.8963 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.72917\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 7s 719ms/step - loss: 0.4127 - acc: 0.8042 - val_loss: 0.6362 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72917\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.3113 - acc: 0.8542 - val_loss: 0.5585 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.72917\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 8s 820ms/step - loss: 0.2086 - acc: 0.9146 - val_loss: 0.5288 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.72917 to 0.79167, saving model to CV/node_go_seq2-4.hdf5\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 8s 829ms/step - loss: 0.2390 - acc: 0.9125 - val_loss: 0.4379 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79167\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 9s 905ms/step - loss: 0.1401 - acc: 0.9583 - val_loss: 0.4875 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79167\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 8s 787ms/step - loss: 0.1246 - acc: 0.9542 - val_loss: 0.4983 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79167\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 9s 893ms/step - loss: 0.1295 - acc: 0.9438 - val_loss: 0.6245 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79167\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 8s 812ms/step - loss: 0.1161 - acc: 0.9542 - val_loss: 0.5590 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79167\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 8s 784ms/step - loss: 0.0621 - acc: 0.9833 - val_loss: 0.6311 - val_acc: 0.8542\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.79167 to 0.85417, saving model to CV/node_go_seq2-4.hdf5\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 9s 930ms/step - loss: 0.0833 - acc: 0.9708 - val_loss: 0.7021 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.85417\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 8s 784ms/step - loss: 0.1154 - acc: 0.9646 - val_loss: 0.4947 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85417\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 8s 755ms/step - loss: 0.0928 - acc: 0.9688 - val_loss: 0.5226 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85417\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 7s 697ms/step - loss: 0.0684 - acc: 0.9750 - val_loss: 0.6076 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85417\n",
      "Epoch 19/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 760ms/step - loss: 0.0476 - acc: 0.9833 - val_loss: 0.7718 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85417\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 8s 790ms/step - loss: 0.0623 - acc: 0.9792 - val_loss: 1.1049 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85417\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.1340 - acc: 0.9458 - val_loss: 0.5797 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.85417\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 7s 726ms/step - loss: 0.0545 - acc: 0.9917 - val_loss: 0.6934 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.85417\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 9s 905ms/step - loss: 0.0593 - acc: 0.9833 - val_loss: 0.6911 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.85417\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 7s 709ms/step - loss: 0.0896 - acc: 0.9667 - val_loss: 0.5715 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.85417\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 9s 870ms/step - loss: 0.0983 - acc: 0.9625 - val_loss: 0.8931 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.85417\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 9s 895ms/step - loss: 0.0844 - acc: 0.9708 - val_loss: 0.5705 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.85417\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 8s 797ms/step - loss: 0.0342 - acc: 0.9896 - val_loss: 0.6837 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.85417\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 8s 802ms/step - loss: 0.0211 - acc: 0.9917 - val_loss: 0.8330 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.85417\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 8s 810ms/step - loss: 0.0372 - acc: 0.9875 - val_loss: 0.7090 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.85417\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.0702 - acc: 0.9708 - val_loss: 0.5718 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.85417\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 8s 831ms/step - loss: 0.0400 - acc: 0.9896 - val_loss: 0.5302 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.85417\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 7s 724ms/step - loss: 0.0522 - acc: 0.9813 - val_loss: 0.4829 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.85417\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 8s 826ms/step - loss: 0.0224 - acc: 0.9938 - val_loss: 0.6869 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.85417\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 9s 852ms/step - loss: 0.0211 - acc: 0.9917 - val_loss: 0.6367 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.85417\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.913968\n",
      "ACC: 0.828125\n",
      "MCC : 0.674394\n",
      "TPR:0.763158\n",
      "FPR:0.076923\n",
      "Pre:0.935484\n",
      "F1:0.840580\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 34s 3s/step - loss: 1.0739 - acc: 0.5083 - val_loss: 0.7127 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.52083, saving model to CV/node_go_seq2-5.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 8s 790ms/step - loss: 0.6737 - acc: 0.5583 - val_loss: 0.5930 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.52083 to 0.72917, saving model to CV/node_go_seq2-5.hdf5\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 7s 702ms/step - loss: 0.5288 - acc: 0.7250 - val_loss: 1.0519 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.72917\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 6s 645ms/step - loss: 0.5095 - acc: 0.7500 - val_loss: 0.7684 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.72917\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 6s 619ms/step - loss: 0.3629 - acc: 0.8333 - val_loss: 0.5369 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.72917 to 0.77083, saving model to CV/node_go_seq2-5.hdf5\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 6s 642ms/step - loss: 0.3371 - acc: 0.8479 - val_loss: 0.4847 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.77083\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 6s 622ms/step - loss: 0.2158 - acc: 0.9104 - val_loss: 0.5887 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.77083\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 6s 650ms/step - loss: 0.3790 - acc: 0.8458 - val_loss: 0.6358 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.77083\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 7s 669ms/step - loss: 0.3297 - acc: 0.8875 - val_loss: 0.5235 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.77083 to 0.79167, saving model to CV/node_go_seq2-5.hdf5\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 6s 630ms/step - loss: 0.2694 - acc: 0.8958 - val_loss: 0.6101 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79167\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 6s 647ms/step - loss: 0.1861 - acc: 0.9292 - val_loss: 1.6502 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79167\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 7s 708ms/step - loss: 0.3425 - acc: 0.8417 - val_loss: 0.9208 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79167\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 8s 827ms/step - loss: 0.1892 - acc: 0.9208 - val_loss: 1.1190 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79167\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 7s 702ms/step - loss: 0.1092 - acc: 0.9625 - val_loss: 0.8058 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79167\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 0.1107 - acc: 0.9542 - val_loss: 0.8153 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79167\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 6s 627ms/step - loss: 0.0831 - acc: 0.9646 - val_loss: 0.5539 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79167\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 6s 627ms/step - loss: 0.0912 - acc: 0.9667 - val_loss: 0.5051 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.79167 to 0.83333, saving model to CV/node_go_seq2-5.hdf5\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 7s 650ms/step - loss: 0.0555 - acc: 0.9833 - val_loss: 0.5674 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83333\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 6s 615ms/step - loss: 0.0521 - acc: 0.9813 - val_loss: 0.5962 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83333\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 6s 639ms/step - loss: 0.0334 - acc: 0.9875 - val_loss: 0.7140 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83333\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 6s 638ms/step - loss: 0.0729 - acc: 0.9750 - val_loss: 0.6584 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83333\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 6s 648ms/step - loss: 0.0627 - acc: 0.9729 - val_loss: 0.8098 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83333\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 6s 612ms/step - loss: 0.0656 - acc: 0.9729 - val_loss: 0.7724 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83333\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 6s 642ms/step - loss: 0.0799 - acc: 0.9688 - val_loss: 0.8268 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83333\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 6s 644ms/step - loss: 0.0589 - acc: 0.9833 - val_loss: 1.1870 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83333\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 6s 641ms/step - loss: 0.2576 - acc: 0.8896 - val_loss: 0.5865 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83333\n",
      "Epoch 27/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 656ms/step - loss: 0.1592 - acc: 0.9250 - val_loss: 1.4810 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83333\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 6s 645ms/step - loss: 0.1875 - acc: 0.9313 - val_loss: 1.0636 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83333\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 6s 632ms/step - loss: 0.0634 - acc: 0.9729 - val_loss: 0.9970 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83333\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 8s 786ms/step - loss: 0.0185 - acc: 0.9917 - val_loss: 1.6440 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83333\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 8s 779ms/step - loss: 0.0098 - acc: 0.9958 - val_loss: 2.0597 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83333\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 7s 735ms/step - loss: 0.0274 - acc: 0.9896 - val_loss: 3.0169 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83333\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 8s 803ms/step - loss: 0.2099 - acc: 0.9438 - val_loss: 0.7970 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83333\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 8s 784ms/step - loss: 0.1138 - acc: 0.9542 - val_loss: 0.6272 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83333\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 8s 771ms/step - loss: 0.0461 - acc: 0.9812 - val_loss: 1.0076 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83333\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 9s 896ms/step - loss: 0.0444 - acc: 0.9792 - val_loss: 1.4433 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.83333\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 8s 788ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 1.2499 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83333\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.960899\n",
      "ACC: 0.890625\n",
      "MCC : 0.789777\n",
      "TPR:0.969697\n",
      "FPR:0.193548\n",
      "Pre:0.842105\n",
      "F1:0.901408\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 37s 4s/step - loss: 1.1138 - acc: 0.5021 - val_loss: 0.6828 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56250, saving model to CV/node_go_seq2-6.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 7s 735ms/step - loss: 0.6983 - acc: 0.5333 - val_loss: 0.6967 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.56250\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 9s 850ms/step - loss: 0.6415 - acc: 0.6313 - val_loss: 0.7051 - val_acc: 0.4375\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.56250\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 8s 789ms/step - loss: 0.5487 - acc: 0.7187 - val_loss: 0.4841 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.56250 to 0.79167, saving model to CV/node_go_seq2-6.hdf5\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 8s 800ms/step - loss: 0.5534 - acc: 0.7062 - val_loss: 0.4840 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79167\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 7s 735ms/step - loss: 0.4346 - acc: 0.7854 - val_loss: 0.5337 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.79167\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 8s 758ms/step - loss: 0.4927 - acc: 0.7583 - val_loss: 0.4998 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.79167\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 9s 856ms/step - loss: 0.2785 - acc: 0.8771 - val_loss: 0.7036 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.79167\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 8s 822ms/step - loss: 0.3164 - acc: 0.8688 - val_loss: 0.6159 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.79167\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 8s 778ms/step - loss: 0.2252 - acc: 0.9146 - val_loss: 0.6308 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.79167\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 9s 855ms/step - loss: 0.1582 - acc: 0.9396 - val_loss: 0.8862 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.79167\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 7s 723ms/step - loss: 0.3513 - acc: 0.8708 - val_loss: 0.6535 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.79167\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 8s 776ms/step - loss: 0.2170 - acc: 0.8958 - val_loss: 0.7069 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.79167\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 8s 759ms/step - loss: 0.0891 - acc: 0.9688 - val_loss: 0.8878 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.79167\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 8s 780ms/step - loss: 0.0305 - acc: 0.9896 - val_loss: 1.6912 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.79167\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 8s 789ms/step - loss: 0.0945 - acc: 0.9688 - val_loss: 1.1993 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.79167\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 9s 895ms/step - loss: 0.0773 - acc: 0.9708 - val_loss: 0.8700 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.79167\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 8s 762ms/step - loss: 0.0550 - acc: 0.9833 - val_loss: 0.8515 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.79167\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 8s 783ms/step - loss: 0.0328 - acc: 0.9958 - val_loss: 1.2576 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.79167\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 8s 834ms/step - loss: 0.0631 - acc: 0.9792 - val_loss: 1.1060 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.79167\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 8s 802ms/step - loss: 0.0611 - acc: 0.9792 - val_loss: 0.8959 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.79167\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 8s 840ms/step - loss: 0.0393 - acc: 0.9917 - val_loss: 1.4930 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.79167\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 8s 777ms/step - loss: 0.0950 - acc: 0.9625 - val_loss: 1.3382 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.79167\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 9s 894ms/step - loss: 0.0380 - acc: 0.9896 - val_loss: 1.3505 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.79167\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.887500\n",
      "ACC: 0.796875\n",
      "MCC : 0.556304\n",
      "TPR:0.900000\n",
      "FPR:0.375000\n",
      "Pre:0.800000\n",
      "F1:0.847059\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 38s 4s/step - loss: 1.0420 - acc: 0.5208 - val_loss: 0.6941 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.47917, saving model to CV/node_go_seq2-7.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 7s 660ms/step - loss: 0.7249 - acc: 0.5188 - val_loss: 0.7069 - val_acc: 0.4792\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.47917\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 8s 813ms/step - loss: 0.6953 - acc: 0.5250 - val_loss: 0.6848 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.47917 to 0.52083, saving model to CV/node_go_seq2-7.hdf5\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 7s 721ms/step - loss: 0.6691 - acc: 0.5563 - val_loss: 0.6679 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.52083\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 6s 647ms/step - loss: 0.5784 - acc: 0.6958 - val_loss: 0.5322 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.52083 to 0.66667, saving model to CV/node_go_seq2-7.hdf5\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 6s 642ms/step - loss: 0.5789 - acc: 0.7167 - val_loss: 0.5390 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.66667 to 0.83333, saving model to CV/node_go_seq2-7.hdf5\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 7s 695ms/step - loss: 0.4315 - acc: 0.8063 - val_loss: 0.5204 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.83333\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 7s 662ms/step - loss: 0.4106 - acc: 0.8167 - val_loss: 0.5501 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.83333\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 6s 629ms/step - loss: 0.3605 - acc: 0.8312 - val_loss: 0.5030 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83333\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 7s 674ms/step - loss: 0.2849 - acc: 0.8917 - val_loss: 0.5791 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.83333\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 7s 667ms/step - loss: 0.1916 - acc: 0.9292 - val_loss: 0.9855 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.83333\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 6s 617ms/step - loss: 0.2103 - acc: 0.9229 - val_loss: 0.7282 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.83333\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 7s 667ms/step - loss: 0.1821 - acc: 0.9333 - val_loss: 0.7489 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.83333\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 6s 645ms/step - loss: 0.1014 - acc: 0.9667 - val_loss: 0.5945 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.83333\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 6s 647ms/step - loss: 0.0685 - acc: 0.9667 - val_loss: 1.1531 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.83333\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 6s 614ms/step - loss: 0.3097 - acc: 0.8729 - val_loss: 0.5862 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.83333\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 7s 654ms/step - loss: 0.2679 - acc: 0.8979 - val_loss: 0.5049 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.83333\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 6s 640ms/step - loss: 0.1225 - acc: 0.9625 - val_loss: 1.4116 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.83333\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 7s 657ms/step - loss: 0.2370 - acc: 0.9125 - val_loss: 0.6939 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.83333\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 6s 642ms/step - loss: 0.1399 - acc: 0.9500 - val_loss: 0.8201 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.83333\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 7s 669ms/step - loss: 0.0890 - acc: 0.9583 - val_loss: 0.9309 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.83333\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 7s 660ms/step - loss: 0.0471 - acc: 0.9875 - val_loss: 1.0545 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83333\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 7s 749ms/step - loss: 0.0228 - acc: 0.9896 - val_loss: 1.4665 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.83333\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 8s 842ms/step - loss: 0.0597 - acc: 0.9771 - val_loss: 1.0847 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83333\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 9s 852ms/step - loss: 0.1043 - acc: 0.9604 - val_loss: 0.9270 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83333\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 7s 743ms/step - loss: 0.0501 - acc: 0.9833 - val_loss: 1.1584 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83333\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.889541\n",
      "ACC: 0.796875\n",
      "MCC : 0.610734\n",
      "TPR:0.903226\n",
      "FPR:0.303030\n",
      "Pre:0.736842\n",
      "F1:0.811594\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 42s 4s/step - loss: 1.2251 - acc: 0.4917 - val_loss: 0.7291 - val_acc: 0.4583\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.45833, saving model to CV/node_go_seq2-8.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 8s 782ms/step - loss: 0.6871 - acc: 0.5458 - val_loss: 0.7104 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.45833 to 0.54167, saving model to CV/node_go_seq2-8.hdf5\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 8s 812ms/step - loss: 0.5999 - acc: 0.6604 - val_loss: 0.9366 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.54167\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 7s 742ms/step - loss: 0.4875 - acc: 0.7646 - val_loss: 0.6920 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.54167 to 0.58333, saving model to CV/node_go_seq2-8.hdf5\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 8s 847ms/step - loss: 0.5461 - acc: 0.7396 - val_loss: 0.6305 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.58333\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 9s 909ms/step - loss: 0.4180 - acc: 0.7917 - val_loss: 0.6223 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.58333 to 0.66667, saving model to CV/node_go_seq2-8.hdf5\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 9s 855ms/step - loss: 0.3455 - acc: 0.8313 - val_loss: 0.7438 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.66667 to 0.72917, saving model to CV/node_go_seq2-8.hdf5\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 9s 875ms/step - loss: 0.3594 - acc: 0.8271 - val_loss: 0.5805 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.72917\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 8s 847ms/step - loss: 0.1902 - acc: 0.9354 - val_loss: 1.4261 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.72917\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 8s 837ms/step - loss: 0.1217 - acc: 0.9479 - val_loss: 0.7641 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.72917\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 8s 777ms/step - loss: 0.0810 - acc: 0.9729 - val_loss: 1.0149 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.72917\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 8s 839ms/step - loss: 0.2957 - acc: 0.8938 - val_loss: 0.9906 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.72917\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 8s 791ms/step - loss: 0.3056 - acc: 0.8667 - val_loss: 0.7361 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.72917\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 8s 771ms/step - loss: 0.2715 - acc: 0.8875 - val_loss: 1.0337 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.72917\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 9s 894ms/step - loss: 0.3202 - acc: 0.8562 - val_loss: 0.9215 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.72917\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 8s 801ms/step - loss: 0.2045 - acc: 0.9229 - val_loss: 0.7755 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.72917\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 8s 776ms/step - loss: 0.0530 - acc: 0.9792 - val_loss: 1.0971 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.72917 to 0.77083, saving model to CV/node_go_seq2-8.hdf5\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 9s 854ms/step - loss: 0.0469 - acc: 0.9854 - val_loss: 1.1221 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.77083\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 9s 881ms/step - loss: 0.1311 - acc: 0.9479 - val_loss: 0.8047 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.77083\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 9s 852ms/step - loss: 0.0725 - acc: 0.9729 - val_loss: 0.8721 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.77083\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 9s 868ms/step - loss: 0.0590 - acc: 0.9792 - val_loss: 2.8493 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.77083\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 8s 822ms/step - loss: 0.5637 - acc: 0.8333 - val_loss: 0.8404 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.77083\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 789ms/step - loss: 0.2592 - acc: 0.8917 - val_loss: 1.1555 - val_acc: 0.6875\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.77083\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 8s 777ms/step - loss: 0.1471 - acc: 0.9563 - val_loss: 1.0070 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.77083\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 9s 870ms/step - loss: 0.0968 - acc: 0.9646 - val_loss: 1.4718 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.77083\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 8s 786ms/step - loss: 0.1462 - acc: 0.9458 - val_loss: 0.8241 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.77083\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 9s 855ms/step - loss: 0.0354 - acc: 0.9938 - val_loss: 1.1877 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.77083\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 8s 796ms/step - loss: 0.0097 - acc: 0.9979 - val_loss: 1.8294 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.77083\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 8s 794ms/step - loss: 0.0620 - acc: 0.9854 - val_loss: 1.0555 - val_acc: 0.8125\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.77083 to 0.81250, saving model to CV/node_go_seq2-8.hdf5\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 9s 851ms/step - loss: 0.0418 - acc: 0.9896 - val_loss: 0.9795 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.81250\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 9s 879ms/step - loss: 0.0270 - acc: 0.9896 - val_loss: 1.0817 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.81250\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 8s 822ms/step - loss: 0.0299 - acc: 0.9854 - val_loss: 1.1363 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.81250\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 7s 730ms/step - loss: 0.0105 - acc: 0.9958 - val_loss: 1.3446 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.81250\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 7s 744ms/step - loss: 0.0150 - acc: 0.9938 - val_loss: 1.6377 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.81250\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 9s 892ms/step - loss: 0.0137 - acc: 0.9938 - val_loss: 1.8439 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.81250\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 8s 769ms/step - loss: 0.0081 - acc: 0.9979 - val_loss: 1.8320 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.81250\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 8s 797ms/step - loss: 0.0405 - acc: 0.9917 - val_loss: 1.8485 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.81250\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 8s 835ms/step - loss: 0.0141 - acc: 0.9917 - val_loss: 1.7935 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.81250\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 8s 768ms/step - loss: 0.0969 - acc: 0.9646 - val_loss: 1.1048 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.81250\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 8s 761ms/step - loss: 0.0659 - acc: 0.9792 - val_loss: 1.1701 - val_acc: 0.7917\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.81250\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 9s 925ms/step - loss: 0.0590 - acc: 0.9854 - val_loss: 1.3211 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.81250\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 7s 734ms/step - loss: 0.0252 - acc: 0.9958 - val_loss: 1.3437 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.81250\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 8s 814ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 1.6945 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.81250\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 8s 829ms/step - loss: 0.0219 - acc: 0.9896 - val_loss: 2.0348 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.81250\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 9s 869ms/step - loss: 0.0632 - acc: 0.9729 - val_loss: 1.6460 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.81250\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 8s 760ms/step - loss: 0.0296 - acc: 0.9938 - val_loss: 1.1904 - val_acc: 0.7500\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.81250\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 9s 913ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 1.4596 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.81250\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 8s 787ms/step - loss: 0.0102 - acc: 0.9958 - val_loss: 1.6052 - val_acc: 0.7292\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.81250\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 8s 818ms/step - loss: 0.0127 - acc: 0.9938 - val_loss: 1.6067 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.81250\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.984496\n",
      "ACC: 0.937500\n",
      "MCC : 0.872846\n",
      "TPR:1.000000\n",
      "FPR:0.093023\n",
      "Pre:0.840000\n",
      "F1:0.913043\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "10/10 [==============================] - 41s 4s/step - loss: 0.9688 - acc: 0.5083 - val_loss: 0.6943 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.52083, saving model to CV/node_go_seq2-9.hdf5\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 7s 652ms/step - loss: 0.6612 - acc: 0.6062 - val_loss: 0.6932 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.52083\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 6s 614ms/step - loss: 0.5655 - acc: 0.6875 - val_loss: 0.6900 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.52083 to 0.56250, saving model to CV/node_go_seq2-9.hdf5\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 6s 645ms/step - loss: 0.4479 - acc: 0.7875 - val_loss: 0.9024 - val_acc: 0.5417\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.56250\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 6s 617ms/step - loss: 0.3966 - acc: 0.8167 - val_loss: 0.8025 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.56250 to 0.58333, saving model to CV/node_go_seq2-9.hdf5\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 8s 763ms/step - loss: 0.3160 - acc: 0.8625 - val_loss: 0.8369 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.58333 to 0.64583, saving model to CV/node_go_seq2-9.hdf5\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 7s 744ms/step - loss: 0.2830 - acc: 0.8750 - val_loss: 0.9916 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.64583\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 8s 761ms/step - loss: 0.2926 - acc: 0.8667 - val_loss: 0.8100 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.64583 to 0.66667, saving model to CV/node_go_seq2-9.hdf5\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 8s 801ms/step - loss: 0.1416 - acc: 0.9333 - val_loss: 1.3976 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.66667\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 9s 857ms/step - loss: 0.2809 - acc: 0.9146 - val_loss: 1.1152 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.66667\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 9s 856ms/step - loss: 0.3317 - acc: 0.8438 - val_loss: 1.0359 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.66667\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 8s 799ms/step - loss: 0.1459 - acc: 0.9333 - val_loss: 1.6366 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.66667\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 8s 802ms/step - loss: 0.0925 - acc: 0.9708 - val_loss: 1.5990 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.66667\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 8s 801ms/step - loss: 0.1105 - acc: 0.9458 - val_loss: 1.1672 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.66667\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 8s 775ms/step - loss: 0.0727 - acc: 0.9750 - val_loss: 1.7228 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.66667\n",
      "Epoch 16/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 8s 848ms/step - loss: 0.0441 - acc: 0.9896 - val_loss: 2.5122 - val_acc: 0.5625\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.66667\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 8s 775ms/step - loss: 0.0524 - acc: 0.9875 - val_loss: 2.2717 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.66667\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 8s 815ms/step - loss: 0.0427 - acc: 0.9833 - val_loss: 1.6265 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.66667\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 8s 834ms/step - loss: 0.0964 - acc: 0.9542 - val_loss: 1.9161 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.66667\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 9s 866ms/step - loss: 0.0875 - acc: 0.9729 - val_loss: 2.0663 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.66667\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 8s 755ms/step - loss: 0.0968 - acc: 0.9625 - val_loss: 1.4870 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.66667\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 10s 981ms/step - loss: 0.0264 - acc: 0.9938 - val_loss: 1.9555 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.66667\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 8s 788ms/step - loss: 0.0188 - acc: 0.9917 - val_loss: 3.0322 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.66667\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 8s 768ms/step - loss: 0.2676 - acc: 0.9104 - val_loss: 1.1051 - val_acc: 0.5833\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.66667\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 8s 800ms/step - loss: 0.1312 - acc: 0.9500 - val_loss: 1.5026 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.66667\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 8s 844ms/step - loss: 0.0404 - acc: 0.9854 - val_loss: 1.9005 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.66667\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 8s 804ms/step - loss: 0.0280 - acc: 0.9938 - val_loss: 2.2468 - val_acc: 0.6458\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.66667\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 8s 776ms/step - loss: 0.0252 - acc: 0.9896 - val_loss: 2.7135 - val_acc: 0.6042\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.66667\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.990225\n",
      "ACC: 0.937500\n",
      "MCC : 0.876356\n",
      "TPR:0.969697\n",
      "FPR:0.096774\n",
      "Pre:0.914286\n",
      "F1:0.941176\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.934752\n",
      "mean ACC: 0.867661\n",
      "mean MCC : 0.737946\n",
      "mean TPR:0.871358\n",
      "mean FPR:0.144575\n",
      "mean Pre:0.872073\n",
      "mean F1:0.860831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'DM2'\n",
    "for rep in range(2,3):\n",
    "    n_splits = 10\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'CV/valid'+str(rep)+'-'+str(split)\n",
    "        \n",
    "         \n",
    "\n",
    "        batch_size = 48\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'CV/node_go_seq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name,verbose=1, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                   epochs = 100,verbose=1,validation_data = valid_generator,\n",
    "                                 callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('node2vec_go_seq'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez('yeast_go_seq_node2vec'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.951818\n",
      "mean ACC: 0.890956\n",
      "mean MCC : 0.786942\n",
      "mean TPR:0.912878\n",
      "mean FPR:0.132153\n",
      "mean Pre:0.877957\n",
      "mean F1:0.888986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'node2vec_go_seq0.npz')\n",
    "results2 =   np.load( 'node2vec_go_seq1.npz')\n",
    "results3 =   np.load( 'node2vec_go_seq2.npz')\n",
    "print ('mean AUC: %f' %  ( (np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/3     ) )\n",
    "print ('mean ACC: %f' %   ( (np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/3) )\n",
    "print ('mean MCC : %f' %  (  (np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/3))\n",
    "print('mean TPR:%f'%    ((np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/3))\n",
    "print('mean FPR:%f'%   ( (np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/3))\n",
    "print('mean Pre:%f'%    ((np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/3))\n",
    "print('mean F1:%f'%    ((np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
