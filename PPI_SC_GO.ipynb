{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  #设置GPU 0,1\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 128\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "        self.protein2seq = load_dict('SC_protein_seqs.pkl')\n",
    "        self.read_ppi()\n",
    "        self.prot2emb = {}\n",
    "        self.prot2embedding()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "            \n",
    "    \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def prot2embedding(self):\n",
    "        for key, value in self.protein2go.items():\n",
    "            X_go1 =  np.zeros((1,768))\n",
    "            allgos = value.split(';') \n",
    "            allgos = list(set(allgos))\n",
    "            count = 0\n",
    "            for  go in  allgos:\n",
    "                feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "                if count + feature.shape[0] > max_go_len:\n",
    "                    break\n",
    "                X_go1 = np.concatenate((X_go1,feature ))    \n",
    "                count += feature.shape[0]\n",
    "            self.prot2emb[key] =  X_go1[1:]   \n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        y = np.empty((self.batch_size))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "\n",
    "        return [X_go1,X_go2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "\n",
    "        return [X_go1,X_go2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 128, 64)      147520      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 128, 64)      49216       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 128, 64)      147520      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 128, 64)      49216       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 128, 64)      20544       conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 128, 64)      12352       conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 128, 64)      147520      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 128, 64)      49216       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 128, 64)      20544       conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 128, 64)      12352       conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 128, 64)      147520      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 128, 64)      49216       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 256)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 128, 128)     320256      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 128, 256)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 128, 128)     320256      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 128, 256)     0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 128, 128)     0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 128, 256)     0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 128, 128)     0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 256)          0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 256)          0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 256)          384         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          256         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 256)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 256)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 256)          384         dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          256         dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1152)         0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1152)         0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          295168      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          295168      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         525312      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1024)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1024)         1049600     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1024)         0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 512)          524800      dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            513         dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1)            0           dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,185,089\n",
      "Trainable params: 4,185,089\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([x_a, x_b, x_c, x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "     \n",
    "     \n",
    " \n",
    "     \n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    \n",
    "     \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go])\n",
    "     \n",
    "     \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    optimizer = Lookahead(RAdam())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "67/67 [==============================] - 47s 700ms/step - loss: 0.5797 - acc: 0.6874 - val_loss: 0.4262 - val_acc: 0.8318\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 39s 575ms/step - loss: 0.3773 - acc: 0.8362 - val_loss: 0.3058 - val_acc: 0.8794\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 40s 599ms/step - loss: 0.2761 - acc: 0.8851 - val_loss: 0.2267 - val_acc: 0.9123\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 39s 581ms/step - loss: 0.2015 - acc: 0.9211 - val_loss: 0.1901 - val_acc: 0.9296\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.1686 - acc: 0.9333 - val_loss: 0.1736 - val_acc: 0.9381\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 39s 580ms/step - loss: 0.1570 - acc: 0.9425 - val_loss: 0.1789 - val_acc: 0.9299\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.1396 - acc: 0.9468 - val_loss: 0.1556 - val_acc: 0.9450\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 39s 582ms/step - loss: 0.1436 - acc: 0.9467 - val_loss: 0.1455 - val_acc: 0.9463\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.1279 - acc: 0.9525 - val_loss: 0.1428 - val_acc: 0.9478\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.1114 - acc: 0.9591 - val_loss: 0.1562 - val_acc: 0.9429\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 39s 580ms/step - loss: 0.1027 - acc: 0.9640 - val_loss: 0.1427 - val_acc: 0.9477\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0981 - acc: 0.9635 - val_loss: 0.1631 - val_acc: 0.9392\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 40s 601ms/step - loss: 0.0910 - acc: 0.9674 - val_loss: 0.1343 - val_acc: 0.9522\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0947 - acc: 0.9651 - val_loss: 0.1587 - val_acc: 0.9412\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.0838 - acc: 0.9694 - val_loss: 0.1311 - val_acc: 0.9536\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0795 - acc: 0.9697 - val_loss: 0.1528 - val_acc: 0.9437\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 39s 583ms/step - loss: 0.0752 - acc: 0.9731 - val_loss: 0.1472 - val_acc: 0.9458\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0962 - acc: 0.9620 - val_loss: 0.1370 - val_acc: 0.9511\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0735 - acc: 0.9725 - val_loss: 0.1265 - val_acc: 0.9524\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0589 - acc: 0.9785 - val_loss: 0.1411 - val_acc: 0.9517\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.0526 - acc: 0.9805 - val_loss: 0.1741 - val_acc: 0.9452\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 39s 578ms/step - loss: 0.0648 - acc: 0.9770 - val_loss: 0.1357 - val_acc: 0.9564\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.0540 - acc: 0.9801 - val_loss: 0.1296 - val_acc: 0.9536\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.0574 - acc: 0.9785 - val_loss: 0.1535 - val_acc: 0.9446\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.0758 - acc: 0.9699 - val_loss: 0.1444 - val_acc: 0.9497\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.0533 - acc: 0.9804 - val_loss: 0.1355 - val_acc: 0.9577\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0564 - acc: 0.9791 - val_loss: 0.1317 - val_acc: 0.9545\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.0443 - acc: 0.9848 - val_loss: 0.1395 - val_acc: 0.9561\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 39s 581ms/step - loss: 0.0416 - acc: 0.9847 - val_loss: 0.1493 - val_acc: 0.9489\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 39s 582ms/step - loss: 0.0478 - acc: 0.9809 - val_loss: 0.1567 - val_acc: 0.9495\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.0439 - acc: 0.9830 - val_loss: 0.1391 - val_acc: 0.9568\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.1505 - val_acc: 0.9536\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 40s 598ms/step - loss: 0.0436 - acc: 0.9832 - val_loss: 0.1491 - val_acc: 0.9515\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0345 - acc: 0.9871 - val_loss: 0.1548 - val_acc: 0.9518\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0549 - acc: 0.9795 - val_loss: 0.1405 - val_acc: 0.9531\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0494 - acc: 0.9805 - val_loss: 0.1636 - val_acc: 0.9490\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0375 - acc: 0.9867 - val_loss: 0.1520 - val_acc: 0.9537\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0337 - acc: 0.9869 - val_loss: 0.1615 - val_acc: 0.9506\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 39s 583ms/step - loss: 0.0323 - acc: 0.9873 - val_loss: 0.1678 - val_acc: 0.9482\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.989084\n",
      "ACC: 0.955957\n",
      "MCC : 0.913554\n",
      "TPR:0.925529\n",
      "FPR:0.013854\n",
      "Pre:0.985137\n",
      "F1:0.954404\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.989084\n",
      "mean ACC: 0.955957\n",
      "mean MCC : 0.913554\n",
      "mean TPR:0.925529\n",
      "mean FPR:0.013854\n",
      "mean Pre:0.985137\n",
      "mean F1:0.954404\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 45s 674ms/step - loss: 0.5667 - acc: 0.7009 - val_loss: 0.4280 - val_acc: 0.8121\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 38s 568ms/step - loss: 0.3610 - acc: 0.8449 - val_loss: 0.2849 - val_acc: 0.8852\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 36s 536ms/step - loss: 0.2857 - acc: 0.8822 - val_loss: 0.2249 - val_acc: 0.9115\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 35s 526ms/step - loss: 0.2077 - acc: 0.9184 - val_loss: 0.1929 - val_acc: 0.9254\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 35s 525ms/step - loss: 0.1754 - acc: 0.9328 - val_loss: 0.1942 - val_acc: 0.9253\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 35s 525ms/step - loss: 0.1592 - acc: 0.9399 - val_loss: 0.1813 - val_acc: 0.9298\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 36s 536ms/step - loss: 0.1310 - acc: 0.9516 - val_loss: 0.1488 - val_acc: 0.9464\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 35s 528ms/step - loss: 0.1254 - acc: 0.9526 - val_loss: 0.1922 - val_acc: 0.9237\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 35s 529ms/step - loss: 0.1399 - acc: 0.9468 - val_loss: 0.1499 - val_acc: 0.9450\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 35s 526ms/step - loss: 0.1086 - acc: 0.9598 - val_loss: 0.1673 - val_acc: 0.9342\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 35s 526ms/step - loss: 0.1029 - acc: 0.9623 - val_loss: 0.1490 - val_acc: 0.9450\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 35s 528ms/step - loss: 0.1156 - acc: 0.9572 - val_loss: 0.1513 - val_acc: 0.9444\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 35s 527ms/step - loss: 0.0948 - acc: 0.9639 - val_loss: 0.1377 - val_acc: 0.9530\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 35s 529ms/step - loss: 0.0932 - acc: 0.9654 - val_loss: 0.1380 - val_acc: 0.9515\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 35s 526ms/step - loss: 0.0971 - acc: 0.9636 - val_loss: 0.1670 - val_acc: 0.9364\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 35s 529ms/step - loss: 0.0858 - acc: 0.9687 - val_loss: 0.1253 - val_acc: 0.9557\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 35s 524ms/step - loss: 0.0714 - acc: 0.9740 - val_loss: 0.1358 - val_acc: 0.9506\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 36s 533ms/step - loss: 0.0914 - acc: 0.9667 - val_loss: 0.1525 - val_acc: 0.9397\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 35s 530ms/step - loss: 0.0824 - acc: 0.9695 - val_loss: 0.1283 - val_acc: 0.9536\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 36s 531ms/step - loss: 0.0725 - acc: 0.9723 - val_loss: 0.1367 - val_acc: 0.9472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100\n",
      "67/67 [==============================] - 36s 536ms/step - loss: 0.0667 - acc: 0.9753 - val_loss: 0.1455 - val_acc: 0.9441\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 35s 525ms/step - loss: 0.0595 - acc: 0.9770 - val_loss: 0.1371 - val_acc: 0.9524\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 36s 531ms/step - loss: 0.0623 - acc: 0.9776 - val_loss: 0.1566 - val_acc: 0.9497\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 36s 531ms/step - loss: 0.0612 - acc: 0.9774 - val_loss: 0.1412 - val_acc: 0.9525\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 36s 538ms/step - loss: 0.0530 - acc: 0.9802 - val_loss: 0.1471 - val_acc: 0.9530\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 36s 535ms/step - loss: 0.0566 - acc: 0.9784 - val_loss: 0.1612 - val_acc: 0.9437\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 35s 527ms/step - loss: 0.0611 - acc: 0.9770 - val_loss: 0.1630 - val_acc: 0.9498\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 36s 533ms/step - loss: 0.0614 - acc: 0.9766 - val_loss: 0.2006 - val_acc: 0.9261\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 36s 536ms/step - loss: 0.0661 - acc: 0.9749 - val_loss: 0.1682 - val_acc: 0.9516\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 36s 532ms/step - loss: 0.0541 - acc: 0.9795 - val_loss: 0.1785 - val_acc: 0.9348\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 36s 537ms/step - loss: 0.0527 - acc: 0.9792 - val_loss: 0.1801 - val_acc: 0.9489\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 36s 537ms/step - loss: 0.0397 - acc: 0.9861 - val_loss: 0.1461 - val_acc: 0.9537\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 36s 532ms/step - loss: 0.0368 - acc: 0.9851 - val_loss: 0.1732 - val_acc: 0.9491\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 36s 533ms/step - loss: 0.0419 - acc: 0.9835 - val_loss: 0.2792 - val_acc: 0.9280\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 36s 534ms/step - loss: 0.0381 - acc: 0.9851 - val_loss: 0.1675 - val_acc: 0.9493\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 36s 534ms/step - loss: 0.0332 - acc: 0.9870 - val_loss: 0.1566 - val_acc: 0.9516\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.988980\n",
      "ACC: 0.959203\n",
      "MCC : 0.918693\n",
      "TPR:0.947812\n",
      "FPR:0.029009\n",
      "Pre:0.971275\n",
      "F1:0.959400\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.988980\n",
      "mean ACC: 0.959203\n",
      "mean MCC : 0.918693\n",
      "mean TPR:0.947812\n",
      "mean FPR:0.029009\n",
      "mean Pre:0.971275\n",
      "mean F1:0.959400\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 47s 708ms/step - loss: 0.5757 - acc: 0.6892 - val_loss: 0.4645 - val_acc: 0.7687\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.3723 - acc: 0.8386 - val_loss: 0.3077 - val_acc: 0.8784\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 39s 579ms/step - loss: 0.2786 - acc: 0.8849 - val_loss: 0.2228 - val_acc: 0.9156\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.2046 - acc: 0.9204 - val_loss: 0.1734 - val_acc: 0.9360\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.1817 - acc: 0.9288 - val_loss: 0.2829 - val_acc: 0.8770\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.1509 - acc: 0.9444 - val_loss: 0.1907 - val_acc: 0.9223\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.1558 - acc: 0.9408 - val_loss: 0.1526 - val_acc: 0.9464\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 40s 599ms/step - loss: 0.1480 - acc: 0.9477 - val_loss: 0.1757 - val_acc: 0.9342\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.1156 - acc: 0.9586 - val_loss: 0.2316 - val_acc: 0.9053\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.1229 - acc: 0.9538 - val_loss: 0.1353 - val_acc: 0.9505\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.1053 - acc: 0.9607 - val_loss: 0.1329 - val_acc: 0.9540\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.0911 - acc: 0.9674 - val_loss: 0.1398 - val_acc: 0.9493\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 39s 578ms/step - loss: 0.1001 - acc: 0.9632 - val_loss: 0.1232 - val_acc: 0.9561\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.1058 - acc: 0.9604 - val_loss: 0.1299 - val_acc: 0.9513\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0806 - acc: 0.9715 - val_loss: 0.1703 - val_acc: 0.9360\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 40s 598ms/step - loss: 0.0886 - acc: 0.9675 - val_loss: 0.1816 - val_acc: 0.9306\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0790 - acc: 0.9713 - val_loss: 0.1467 - val_acc: 0.9447\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.0779 - acc: 0.9715 - val_loss: 0.1242 - val_acc: 0.9550\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 39s 588ms/step - loss: 0.0714 - acc: 0.9738 - val_loss: 0.1245 - val_acc: 0.9547\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.0659 - acc: 0.9764 - val_loss: 0.1272 - val_acc: 0.9532\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.0670 - acc: 0.9743 - val_loss: 0.1240 - val_acc: 0.9562\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0614 - acc: 0.9782 - val_loss: 0.1455 - val_acc: 0.9477\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0577 - acc: 0.9786 - val_loss: 0.1263 - val_acc: 0.9542\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.1187 - acc: 0.9566 - val_loss: 0.1321 - val_acc: 0.9508\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0650 - acc: 0.9773 - val_loss: 0.1588 - val_acc: 0.9392\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0535 - acc: 0.9808 - val_loss: 0.1436 - val_acc: 0.9471\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0475 - acc: 0.9832 - val_loss: 0.1274 - val_acc: 0.9567\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0552 - acc: 0.9806 - val_loss: 0.1262 - val_acc: 0.9542\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0471 - acc: 0.9829 - val_loss: 0.1360 - val_acc: 0.9537\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 40s 596ms/step - loss: 0.0463 - acc: 0.9827 - val_loss: 0.1464 - val_acc: 0.9535\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.0515 - acc: 0.9798 - val_loss: 0.1327 - val_acc: 0.9567\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.0404 - acc: 0.9856 - val_loss: 0.1314 - val_acc: 0.9547\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0417 - acc: 0.9855 - val_loss: 0.1552 - val_acc: 0.9536\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.987911\n",
      "ACC: 0.955262\n",
      "MCC : 0.910700\n",
      "TPR:0.944742\n",
      "FPR:0.034340\n",
      "Pre:0.964532\n",
      "F1:0.954535\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.987911\n",
      "mean ACC: 0.955262\n",
      "mean MCC : 0.910700\n",
      "mean TPR:0.944742\n",
      "mean FPR:0.034340\n",
      "mean Pre:0.964532\n",
      "mean F1:0.954535\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 48s 722ms/step - loss: 0.5752 - acc: 0.6932 - val_loss: 0.4454 - val_acc: 0.8042\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 39s 582ms/step - loss: 0.3671 - acc: 0.8436 - val_loss: 0.3179 - val_acc: 0.8761\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.2710 - acc: 0.8895 - val_loss: 0.2429 - val_acc: 0.9053\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.2052 - acc: 0.9225 - val_loss: 0.1870 - val_acc: 0.9303\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.1928 - acc: 0.9237 - val_loss: 0.1806 - val_acc: 0.9317\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.1444 - acc: 0.9462 - val_loss: 0.1615 - val_acc: 0.9408\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 39s 590ms/step - loss: 0.1483 - acc: 0.9454 - val_loss: 0.1591 - val_acc: 0.9409\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 40s 591ms/step - loss: 0.1396 - acc: 0.9483 - val_loss: 0.1619 - val_acc: 0.9458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "67/67 [==============================] - 40s 591ms/step - loss: 0.1123 - acc: 0.9584 - val_loss: 0.1676 - val_acc: 0.9383\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.1064 - acc: 0.9607 - val_loss: 0.1609 - val_acc: 0.9409\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 40s 597ms/step - loss: 0.1072 - acc: 0.9614 - val_loss: 0.1560 - val_acc: 0.9408\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0934 - acc: 0.9669 - val_loss: 0.1433 - val_acc: 0.9500\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.1073 - acc: 0.9622 - val_loss: 0.1345 - val_acc: 0.9496\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.0795 - acc: 0.9715 - val_loss: 0.1594 - val_acc: 0.9425\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.0854 - acc: 0.9696 - val_loss: 0.2161 - val_acc: 0.9150\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.0853 - acc: 0.9699 - val_loss: 0.1598 - val_acc: 0.9412\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0732 - acc: 0.9738 - val_loss: 0.1346 - val_acc: 0.9512\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0633 - acc: 0.9777 - val_loss: 0.1546 - val_acc: 0.9454\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 39s 588ms/step - loss: 0.1272 - acc: 0.9491 - val_loss: 0.1653 - val_acc: 0.9390\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0853 - acc: 0.9676 - val_loss: 0.1326 - val_acc: 0.9513\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.0643 - acc: 0.9769 - val_loss: 0.1304 - val_acc: 0.9537\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0604 - acc: 0.9776 - val_loss: 0.1353 - val_acc: 0.9529\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0571 - acc: 0.9783 - val_loss: 0.1442 - val_acc: 0.9505\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0552 - acc: 0.9792 - val_loss: 0.1602 - val_acc: 0.9411\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0688 - acc: 0.9747 - val_loss: 0.1671 - val_acc: 0.9459\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 40s 591ms/step - loss: 0.0538 - acc: 0.9795 - val_loss: 0.1321 - val_acc: 0.9527\n",
      "Epoch 27/100\n",
      "67/67 [==============================] - 40s 591ms/step - loss: 0.0458 - acc: 0.9838 - val_loss: 0.1404 - val_acc: 0.9506\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0592 - acc: 0.9793 - val_loss: 0.1557 - val_acc: 0.9500\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0485 - acc: 0.9817 - val_loss: 0.1366 - val_acc: 0.9490\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 39s 588ms/step - loss: 0.0574 - acc: 0.9789 - val_loss: 0.1420 - val_acc: 0.9511\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0455 - acc: 0.9834 - val_loss: 0.1760 - val_acc: 0.9433\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 40s 591ms/step - loss: 0.0436 - acc: 0.9832 - val_loss: 0.1312 - val_acc: 0.9545\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.0376 - acc: 0.9859 - val_loss: 0.1515 - val_acc: 0.9524\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0452 - acc: 0.9836 - val_loss: 0.2366 - val_acc: 0.9267\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.0553 - acc: 0.9785 - val_loss: 0.1422 - val_acc: 0.9512\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 39s 588ms/step - loss: 0.0334 - acc: 0.9875 - val_loss: 0.1593 - val_acc: 0.9510\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 39s 588ms/step - loss: 0.0502 - acc: 0.9825 - val_loss: 0.1376 - val_acc: 0.9504\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 39s 582ms/step - loss: 0.0367 - acc: 0.9859 - val_loss: 0.1586 - val_acc: 0.9508\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 40s 597ms/step - loss: 0.0287 - acc: 0.9896 - val_loss: 0.1745 - val_acc: 0.9492\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0268 - acc: 0.9890 - val_loss: 0.1928 - val_acc: 0.9495\n",
      "Epoch 41/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.0321 - acc: 0.9878 - val_loss: 0.2780 - val_acc: 0.9150\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.987911\n",
      "ACC: 0.955146\n",
      "MCC : 0.910608\n",
      "TPR:0.943073\n",
      "FPR:0.032440\n",
      "Pre:0.967628\n",
      "F1:0.955193\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.987911\n",
      "mean ACC: 0.955146\n",
      "mean MCC : 0.910608\n",
      "mean TPR:0.943073\n",
      "mean FPR:0.032440\n",
      "mean Pre:0.967628\n",
      "mean F1:0.955193\n",
      "Epoch 1/100\n",
      "67/67 [==============================] - 50s 746ms/step - loss: 0.5833 - acc: 0.6920 - val_loss: 0.4447 - val_acc: 0.8027\n",
      "Epoch 2/100\n",
      "67/67 [==============================] - 39s 584ms/step - loss: 0.3825 - acc: 0.8325 - val_loss: 0.3058 - val_acc: 0.8799\n",
      "Epoch 3/100\n",
      "67/67 [==============================] - 39s 588ms/step - loss: 0.2675 - acc: 0.8917 - val_loss: 0.2246 - val_acc: 0.9197\n",
      "Epoch 4/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.2010 - acc: 0.9201 - val_loss: 0.2057 - val_acc: 0.9225\n",
      "Epoch 5/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.1712 - acc: 0.9331 - val_loss: 0.2094 - val_acc: 0.9219\n",
      "Epoch 6/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.1551 - acc: 0.9425 - val_loss: 0.2096 - val_acc: 0.9222\n",
      "Epoch 7/100\n",
      "67/67 [==============================] - 40s 601ms/step - loss: 0.1425 - acc: 0.9475 - val_loss: 0.1651 - val_acc: 0.9371\n",
      "Epoch 8/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.1234 - acc: 0.9545 - val_loss: 0.1429 - val_acc: 0.9487\n",
      "Epoch 9/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.1086 - acc: 0.9606 - val_loss: 0.1434 - val_acc: 0.9447\n",
      "Epoch 10/100\n",
      "67/67 [==============================] - 40s 595ms/step - loss: 0.1243 - acc: 0.9545 - val_loss: 0.1390 - val_acc: 0.9504\n",
      "Epoch 11/100\n",
      "67/67 [==============================] - 40s 596ms/step - loss: 0.1070 - acc: 0.9619 - val_loss: 0.2017 - val_acc: 0.9210\n",
      "Epoch 12/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.1087 - acc: 0.9594 - val_loss: 0.1385 - val_acc: 0.9484\n",
      "Epoch 13/100\n",
      "67/67 [==============================] - 40s 598ms/step - loss: 0.0831 - acc: 0.9699 - val_loss: 0.1329 - val_acc: 0.9512\n",
      "Epoch 14/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0785 - acc: 0.9725 - val_loss: 0.1459 - val_acc: 0.9459\n",
      "Epoch 15/100\n",
      "67/67 [==============================] - 40s 598ms/step - loss: 0.0764 - acc: 0.9735 - val_loss: 0.1325 - val_acc: 0.9505\n",
      "Epoch 16/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0922 - acc: 0.9655 - val_loss: 0.1735 - val_acc: 0.9364\n",
      "Epoch 17/100\n",
      "67/67 [==============================] - 40s 596ms/step - loss: 0.0923 - acc: 0.9664 - val_loss: 0.1281 - val_acc: 0.9522\n",
      "Epoch 18/100\n",
      "67/67 [==============================] - 39s 585ms/step - loss: 0.0659 - acc: 0.9763 - val_loss: 0.1400 - val_acc: 0.9478\n",
      "Epoch 19/100\n",
      "67/67 [==============================] - 39s 588ms/step - loss: 0.0773 - acc: 0.9710 - val_loss: 0.1799 - val_acc: 0.9368\n",
      "Epoch 20/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0630 - acc: 0.9762 - val_loss: 0.1187 - val_acc: 0.9553\n",
      "Epoch 21/100\n",
      "67/67 [==============================] - 40s 594ms/step - loss: 0.0701 - acc: 0.9729 - val_loss: 0.1394 - val_acc: 0.9478\n",
      "Epoch 22/100\n",
      "67/67 [==============================] - 40s 590ms/step - loss: 0.0598 - acc: 0.9777 - val_loss: 0.1481 - val_acc: 0.9498\n",
      "Epoch 23/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0542 - acc: 0.9802 - val_loss: 0.1359 - val_acc: 0.9517\n",
      "Epoch 24/100\n",
      "67/67 [==============================] - 39s 587ms/step - loss: 0.0534 - acc: 0.9807 - val_loss: 0.1404 - val_acc: 0.9508\n",
      "Epoch 25/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0579 - acc: 0.9796 - val_loss: 0.1328 - val_acc: 0.9538\n",
      "Epoch 26/100\n",
      "67/67 [==============================] - 40s 602ms/step - loss: 0.0527 - acc: 0.9802 - val_loss: 0.1448 - val_acc: 0.9450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0466 - acc: 0.9827 - val_loss: 0.1304 - val_acc: 0.9573\n",
      "Epoch 28/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0518 - acc: 0.9812 - val_loss: 0.1964 - val_acc: 0.9355\n",
      "Epoch 29/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0553 - acc: 0.9787 - val_loss: 0.1315 - val_acc: 0.9531\n",
      "Epoch 30/100\n",
      "67/67 [==============================] - 40s 591ms/step - loss: 0.0566 - acc: 0.9791 - val_loss: 0.1233 - val_acc: 0.9580\n",
      "Epoch 31/100\n",
      "67/67 [==============================] - 40s 598ms/step - loss: 0.0388 - acc: 0.9850 - val_loss: 0.1381 - val_acc: 0.9521\n",
      "Epoch 32/100\n",
      "67/67 [==============================] - 39s 586ms/step - loss: 0.0376 - acc: 0.9858 - val_loss: 0.1498 - val_acc: 0.9517\n",
      "Epoch 33/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.1306 - val_acc: 0.9545\n",
      "Epoch 34/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.1017 - acc: 0.9594 - val_loss: 0.1549 - val_acc: 0.9416\n",
      "Epoch 35/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0526 - acc: 0.9821 - val_loss: 0.1499 - val_acc: 0.9509\n",
      "Epoch 36/100\n",
      "67/67 [==============================] - 40s 592ms/step - loss: 0.0355 - acc: 0.9870 - val_loss: 0.1429 - val_acc: 0.9541\n",
      "Epoch 37/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0390 - acc: 0.9852 - val_loss: 0.1224 - val_acc: 0.9563\n",
      "Epoch 38/100\n",
      "67/67 [==============================] - 39s 589ms/step - loss: 0.0293 - acc: 0.9893 - val_loss: 0.1442 - val_acc: 0.9528\n",
      "Epoch 39/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0305 - acc: 0.9881 - val_loss: 0.1717 - val_acc: 0.9528\n",
      "Epoch 40/100\n",
      "67/67 [==============================] - 40s 593ms/step - loss: 0.0359 - acc: 0.9868 - val_loss: 0.1968 - val_acc: 0.9502\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.988217\n",
      "ACC: 0.955494\n",
      "MCC : 0.911444\n",
      "TPR:0.939302\n",
      "FPR:0.028420\n",
      "Pre:0.970447\n",
      "F1:0.954621\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.988217\n",
      "mean ACC: 0.955494\n",
      "mean MCC : 0.911444\n",
      "mean TPR:0.939302\n",
      "mean FPR:0.028420\n",
      "mean Pre:0.970447\n",
      "mean F1:0.954621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'SC'\n",
    "for rep in range(5):\n",
    "    n_splits = 1\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "\n",
    "    count = 0\n",
    "    for split in range(1):\n",
    "        train_pairs_file = 'SC_CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'SC_CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'SC_CV/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 256\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'SC_CV/sc_go'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='min')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=valid_generator, \n",
    "                    epochs = 100,verbose=1,callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('SC_go_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.988421\n",
      "mean ACC: 0.956212\n",
      "mean MCC : 0.913000\n",
      "mean TPR:0.940092\n",
      "mean FPR:0.027612\n",
      "mean Pre:0.971804\n",
      "mean F1:0.955630\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'SC_go_0.npz')\n",
    "results2 =   np.load( 'SC_go_1.npz')\n",
    "results3 =   np.load( 'SC_go_2.npz')\n",
    "results4 =   np.load( 'SC_go_3.npz')\n",
    "results5 =   np.load( 'SC_go_4.npz')\n",
    "print ('mean AUC: %f' %  (  (np.mean( results4[ 'AUCs' ] )  +  np.mean( results5[ 'AUCs' ] )  + np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/5     ) )\n",
    "print ('mean ACC: %f' %   (  ( np.mean( results4[ 'ACCs' ] )  + np.mean(  results5[ 'ACCs' ] )  +   np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/5) )\n",
    "print ('mean MCC : %f' %  ( ( np.mean( results4[ 'MCCs' ] )  + np.mean(  results5[ 'MCCs' ] )  + np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/5))\n",
    "print('mean TPR:%f'%    (( np.mean( results4[ 'TPRs' ] )  + np.mean(  results5[ 'TPRs' ] )  + np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/5))\n",
    "print('mean FPR:%f'%   (  (np.mean( results4[ 'FPRs' ] )  + np.mean(  results5[ 'FPRs' ] )  + np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/5))\n",
    "print('mean Pre:%f'%    ( (np.mean( results4[ 'Precs' ] )  + np.mean(  results5[ 'Precs' ] )  + np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/5))\n",
    "print('mean F1:%f'%    (  (np.mean( results4[ 'F1s' ] )  + np.mean(  results5[ 'F1s' ] )  +np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numba import cuda\n",
    "# cuda.select_device(0)\n",
    "# cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.638305\n",
      "mean ACC: 0.608909\n",
      "mean MCC : 0.227555\n",
      "mean TPR:0.657010\n",
      "mean FPR:0.439137\n",
      "mean Pre:0.417911\n",
      "mean F1:0.505120\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'new_seq_and_go__incep_1.npz')\n",
    "results2 =   np.load( 'new_seq_and_go__incep_0.npz')\n",
    "results3 =   np.load( 'new_seq_and_go__incep_2.npz')\n",
    "results4 =   np.load( 'new_seq_and_go__incep_4.npz')\n",
    "results5 =   np.load( 'new_seq_and_go__incep_3.npz')\n",
    "print ('mean AUC: %f' %  (  (np.mean( results4[ 'AUCs' ] )  +  np.mean( results5[ 'AUCs' ] )  + np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/5     ) )\n",
    "print ('mean ACC: %f' %   (  ( np.mean( results4[ 'ACCs' ] )  + np.mean(  results5[ 'ACCs' ] )  +   np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/5) )\n",
    "print ('mean MCC : %f' %  ( ( np.mean( results4[ 'MCCs' ] )  + np.mean(  results5[ 'MCCs' ] )  + np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/5))\n",
    "print('mean TPR:%f'%    (( np.mean( results4[ 'TPRs' ] )  + np.mean(  results5[ 'TPRs' ] )  + np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/5))\n",
    "print('mean FPR:%f'%   (  (np.mean( results4[ 'FPRs' ] )  + np.mean(  results5[ 'FPRs' ] )  + np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/5))\n",
    "print('mean Pre:%f'%    ( (np.mean( results4[ 'Precs' ] )  + np.mean(  results5[ 'Precs' ] )  + np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/5))\n",
    "print('mean F1:%f'%    (  (np.mean( results4[ 'F1s' ] )  + np.mean(  results5[ 'F1s' ] )  +np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
