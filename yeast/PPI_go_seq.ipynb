{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 512\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "protein2go =  load_dict('yeast_data/protein2go_dicts.pkl')\n",
    "prot2emb = {}\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,768))\n",
    "    allgos = value.split(';') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        if go.startswith('GO'):\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            if count + feature.shape[0] > max_go_len:\n",
    "                break\n",
    "            X_go1 = np.concatenate((X_go1,feature ))    \n",
    "            count += feature.shape[0]\n",
    "    prot2emb[key] =  X_go1[1:]   \n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.protein2go =  load_dict('yeast_data/protein2go_dicts.pkl')\n",
    "        self.protein2seq = load_dict('yeast_data/protein2seq_dicts.pkl')\n",
    "        self.read_ppi()\n",
    "#         self.prot2emb = {}\n",
    "#         self.prot2embedding()\n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "            \n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def prot2embedding(self):\n",
    "        for key, value in self.protein2go.items():\n",
    "            X_go1 =  np.zeros((1,768))\n",
    "            allgos = value.split(';') \n",
    "            allgos = list(set(allgos))\n",
    "            count = 0\n",
    "            for  go in  allgos:\n",
    "                if go.startswith('GO'):\n",
    "                    feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "                    if count + feature.shape[0] > max_go_len:\n",
    "                        break\n",
    "                    X_go1 = np.concatenate((X_go1,feature ))    \n",
    "                    count += feature.shape[0]\n",
    "            self.prot2emb[key] =  X_go1[1:]   \n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        y = np.empty((self.batch_size))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "            prot1emb_tmp = prot2emb[p1]\n",
    "            X_go1[i,:prot1emb_tmp.shape[0]] = prot1emb_tmp\n",
    "            \n",
    "            prot2emb_tmp = prot2emb[p2]\n",
    "            X_go2[i,:prot2emb_tmp.shape[0]] = prot2emb_tmp\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "     \n",
    "        return [X_go1,X_go2, X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "         \n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            prot1emb_tmp = prot2emb[p1]\n",
    "            X_go1[i,:prot1emb_tmp.shape[0]] = prot1emb_tmp\n",
    "            \n",
    "            prot2emb_tmp = prot2emb[p2]\n",
    "            X_go2[i,:prot2emb_tmp.shape[0]] = prot2emb_tmp\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "  \n",
    "        return [X_go1,X_go2, X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 512, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 512, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 512, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 512, 64)      20544       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 512, 64)      12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 512, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 512, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 512, 64)      20544       conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 512, 64)      12352       conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 16)     1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 16)     784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 16)     1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 16)     784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512, 256)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512, 128)     320256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 256)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 512, 128)     320256      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 64)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 64)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 256)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512, 256)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000, 64)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 64)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          768         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          640         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          768         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          640         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           1064        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           1064        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1152)         0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1152)         0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 576)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 576)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          295168      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          295168      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          147712      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          147712      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            513         dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,086,177\n",
      "Trainable params: 5,086,177\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.optimizers import Adam,  RMSprop\n",
    "from keras import regularizers\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(2)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([x_a, x_b, x_c, x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_cnn_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    \n",
    "    x = Concatenate()([ x_a, x_b, x_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "\n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters//4, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters//4, gru_units)\n",
    "     \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go,   left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go,    left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model)\n",
    "#     rms = RMSprop(lr=0.0001)\n",
    "#     adam = Adam(lr=0.001, amsgrad=True, epsilon=1e-5)\n",
    "    \n",
    "    optimizer = Lookahead(RAdam())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "69/69 [==============================] - 81s 1s/step - loss: 0.6553 - acc: 0.6269\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 65s 943ms/step - loss: 0.5876 - acc: 0.6799\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 65s 937ms/step - loss: 0.5188 - acc: 0.7346\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 64s 927ms/step - loss: 0.4501 - acc: 0.7832\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 64s 923ms/step - loss: 0.4008 - acc: 0.8128\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 64s 923ms/step - loss: 0.3655 - acc: 0.8320\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 63s 920ms/step - loss: 0.2937 - acc: 0.8706\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.2772 - acc: 0.8805\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 64s 923ms/step - loss: 0.2405 - acc: 0.8956\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.2255 - acc: 0.9039\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.2092 - acc: 0.9106\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 63s 917ms/step - loss: 0.1804 - acc: 0.9241\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 64s 928ms/step - loss: 0.1770 - acc: 0.9250\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 63s 920ms/step - loss: 0.1530 - acc: 0.9365\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 62s 894ms/step - loss: 0.1379 - acc: 0.9450\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.1645 - acc: 0.9337\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.1256 - acc: 0.9490\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 62s 902ms/step - loss: 0.1291 - acc: 0.9493\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.1125 - acc: 0.9557\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.1100 - acc: 0.9547\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 63s 906ms/step - loss: 0.1168 - acc: 0.9554\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.0908 - acc: 0.9664\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.1471 - acc: 0.9477\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.1398 - acc: 0.9460\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 62s 899ms/step - loss: 0.0703 - acc: 0.9746\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 62s 904ms/step - loss: 0.0730 - acc: 0.9710\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 62s 899ms/step - loss: 0.0682 - acc: 0.9744\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 62s 899ms/step - loss: 0.0959 - acc: 0.9639\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.0785 - acc: 0.9702\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.0537 - acc: 0.9808\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 62s 904ms/step - loss: 0.0496 - acc: 0.9821\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.0605 - acc: 0.9781\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.0528 - acc: 0.9829\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.0502 - acc: 0.9825\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 62s 904ms/step - loss: 0.0453 - acc: 0.9841\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 62s 902ms/step - loss: 0.0450 - acc: 0.9823\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 62s 902ms/step - loss: 0.0425 - acc: 0.9856\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.0569 - acc: 0.9793\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 62s 899ms/step - loss: 0.0504 - acc: 0.9811\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 62s 904ms/step - loss: 0.0402 - acc: 0.9873\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.0568 - acc: 0.9815\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.0434 - acc: 0.9841\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.0343 - acc: 0.9891\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.0333 - acc: 0.9906\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 62s 900ms/step - loss: 0.0264 - acc: 0.9913\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 63s 910ms/step - loss: 0.0524 - acc: 0.9818\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.0275 - acc: 0.9905\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.0295 - acc: 0.9901\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 63s 907ms/step - loss: 0.0319 - acc: 0.9896\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 62s 900ms/step - loss: 0.0421 - acc: 0.9854\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.0455 - acc: 0.9849\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 62s 900ms/step - loss: 0.0184 - acc: 0.9933\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 63s 906ms/step - loss: 0.0245 - acc: 0.9921\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 62s 901ms/step - loss: 0.0268 - acc: 0.9913\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 62s 903ms/step - loss: 0.0318 - acc: 0.9890\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 62s 904ms/step - loss: 0.0204 - acc: 0.9931\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 62s 904ms/step - loss: 0.0238 - acc: 0.9920\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 62s 898ms/step - loss: 0.0205 - acc: 0.9928\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 62s 899ms/step - loss: 0.0660 - acc: 0.9805\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 63s 910ms/step - loss: 0.0242 - acc: 0.9926\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.994746\n",
      "ACC: 0.986148\n",
      "MCC : 0.972443\n",
      "TPR:0.998314\n",
      "FPR:0.027567\n",
      "Pre:0.976092\n",
      "F1:0.987078\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 78s 1s/step - loss: 0.6598 - acc: 0.6053\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.5850 - acc: 0.6776\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.5191 - acc: 0.7388\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.4778 - acc: 0.7622\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.4065 - acc: 0.8106\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.3578 - acc: 0.8388\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.3041 - acc: 0.8676\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.2491 - acc: 0.8948\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.2647 - acc: 0.8886\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.2096 - acc: 0.9119\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.2437 - acc: 0.8998\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.1683 - acc: 0.9304\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.1836 - acc: 0.9252\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.1502 - acc: 0.9395\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 66s 964ms/step - loss: 0.1497 - acc: 0.9408\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.1166 - acc: 0.9520\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 67s 967ms/step - loss: 0.1277 - acc: 0.9503\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.1211 - acc: 0.9502\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.1206 - acc: 0.9514\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.1374 - acc: 0.9440\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.1054 - acc: 0.9570\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0983 - acc: 0.9627\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 66s 964ms/step - loss: 0.0874 - acc: 0.9659\n",
      "Epoch 24/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0801 - acc: 0.9695\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.0743 - acc: 0.9726\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0921 - acc: 0.9661\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.0761 - acc: 0.9710\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0658 - acc: 0.9761\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 67s 967ms/step - loss: 0.0789 - acc: 0.9697\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 67s 968ms/step - loss: 0.0525 - acc: 0.9821\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0745 - acc: 0.9720\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0473 - acc: 0.9834\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0495 - acc: 0.9837\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.0489 - acc: 0.9819\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 66s 956ms/step - loss: 0.0792 - acc: 0.9727\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.0471 - acc: 0.9828\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0372 - acc: 0.9874\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.0360 - acc: 0.9868\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0678 - acc: 0.9737\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.0387 - acc: 0.9857\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.0523 - acc: 0.9823\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 65s 943ms/step - loss: 0.0317 - acc: 0.9878\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 62s 904ms/step - loss: 0.0597 - acc: 0.9783\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 62s 892ms/step - loss: 0.0216 - acc: 0.9923\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 64s 921ms/step - loss: 0.0398 - acc: 0.9863\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.0344 - acc: 0.9877\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0434 - acc: 0.9849\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0252 - acc: 0.9926\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.0932 - acc: 0.9686\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.0353 - acc: 0.9865\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0342 - acc: 0.9883\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0280 - acc: 0.9904\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.0217 - acc: 0.9916\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0386 - acc: 0.9865\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0110 - acc: 0.9959\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 67s 968ms/step - loss: 0.0297 - acc: 0.9909\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 67s 975ms/step - loss: 0.0276 - acc: 0.9900\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0221 - acc: 0.9926\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0237 - acc: 0.9907\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0498 - acc: 0.9834\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.994068\n",
      "ACC: 0.973637\n",
      "MCC : 0.948449\n",
      "TPR:0.998155\n",
      "FPR:0.049393\n",
      "Pre:0.949956\n",
      "F1:0.973459\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 79s 1s/step - loss: 0.6752 - acc: 0.5948\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.5964 - acc: 0.6668\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.5215 - acc: 0.7321\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.4677 - acc: 0.7685\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.4085 - acc: 0.8101\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 67s 974ms/step - loss: 0.3541 - acc: 0.8391\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 65s 947ms/step - loss: 0.3086 - acc: 0.8666\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 63s 906ms/step - loss: 0.2860 - acc: 0.8739\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 61s 883ms/step - loss: 0.3376 - acc: 0.8450\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 61s 886ms/step - loss: 0.2721 - acc: 0.8796\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 62s 899ms/step - loss: 0.2044 - acc: 0.9187\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 65s 943ms/step - loss: 0.1908 - acc: 0.9205\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.1741 - acc: 0.9274\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.1733 - acc: 0.9272\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 67s 970ms/step - loss: 0.1588 - acc: 0.9357\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 67s 968ms/step - loss: 0.1514 - acc: 0.9386\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 67s 972ms/step - loss: 0.1361 - acc: 0.9440\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 67s 972ms/step - loss: 0.1133 - acc: 0.9540\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.1208 - acc: 0.9532\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.1088 - acc: 0.9564\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.1007 - acc: 0.9608\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.1124 - acc: 0.9584\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0936 - acc: 0.9620\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0919 - acc: 0.9655\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0804 - acc: 0.9686\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.0990 - acc: 0.9665\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0671 - acc: 0.9762\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.0661 - acc: 0.9757\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.0680 - acc: 0.9743\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0684 - acc: 0.9757\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0893 - acc: 0.9677\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.0480 - acc: 0.9818\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.0382 - acc: 0.9857\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 67s 970ms/step - loss: 0.0733 - acc: 0.9726\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0468 - acc: 0.9827\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 66s 964ms/step - loss: 0.0449 - acc: 0.9839\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0531 - acc: 0.9811\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0412 - acc: 0.9852\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 67s 973ms/step - loss: 0.0426 - acc: 0.9837\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0551 - acc: 0.9812\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.1065 - acc: 0.9623\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0261 - acc: 0.9904\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 67s 970ms/step - loss: 0.0424 - acc: 0.9838\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.0361 - acc: 0.9885\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.2318 - acc: 0.9189\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0838 - acc: 0.9700\n",
      "Epoch 47/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 67s 966ms/step - loss: 0.0375 - acc: 0.9873\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 65s 937ms/step - loss: 0.0401 - acc: 0.9877\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 61s 887ms/step - loss: 0.0288 - acc: 0.9901\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 61s 883ms/step - loss: 0.3351 - acc: 0.8759\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 61s 883ms/step - loss: 0.0924 - acc: 0.9660\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 61s 881ms/step - loss: 0.0579 - acc: 0.9812\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 61s 885ms/step - loss: 0.0382 - acc: 0.9868\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 61s 883ms/step - loss: 0.0308 - acc: 0.9895\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 66s 950ms/step - loss: 0.0536 - acc: 0.9814\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 64s 932ms/step - loss: 0.0246 - acc: 0.9922\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 66s 964ms/step - loss: 0.0428 - acc: 0.9847\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0222 - acc: 0.9922\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.0335 - acc: 0.9888\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 67s 967ms/step - loss: 0.0138 - acc: 0.9954\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.995891\n",
      "ACC: 0.978543\n",
      "MCC : 0.957636\n",
      "TPR:0.995366\n",
      "FPR:0.037133\n",
      "Pre:0.961504\n",
      "F1:0.978142\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 72s 1s/step - loss: 0.6514 - acc: 0.6283\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 56s 819ms/step - loss: 0.5779 - acc: 0.6849\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.5084 - acc: 0.7448\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.4478 - acc: 0.7868\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 56s 819ms/step - loss: 0.3893 - acc: 0.8225\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 56s 815ms/step - loss: 0.3517 - acc: 0.8414\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.3248 - acc: 0.8594\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.2599 - acc: 0.8887\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.2330 - acc: 0.9026\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 57s 819ms/step - loss: 0.2316 - acc: 0.9019\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.2364 - acc: 0.8973\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 57s 819ms/step - loss: 0.1788 - acc: 0.9274\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.1846 - acc: 0.9262\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.1613 - acc: 0.9350\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.2895 - acc: 0.8761\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.1652 - acc: 0.9325\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 57s 822ms/step - loss: 0.1609 - acc: 0.9381\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 56s 816ms/step - loss: 0.1131 - acc: 0.9554\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 57s 824ms/step - loss: 0.1558 - acc: 0.9390\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.1145 - acc: 0.9546\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.0889 - acc: 0.9666\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0961 - acc: 0.9626\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 56s 816ms/step - loss: 0.0886 - acc: 0.9654\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 56s 819ms/step - loss: 0.1633 - acc: 0.9351\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 56s 816ms/step - loss: 0.0768 - acc: 0.9704\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 56s 819ms/step - loss: 0.0757 - acc: 0.9724\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0714 - acc: 0.9728\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 57s 821ms/step - loss: 0.0847 - acc: 0.9668\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.0688 - acc: 0.9744\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0755 - acc: 0.9718\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.0491 - acc: 0.9823\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.0787 - acc: 0.9725\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 57s 821ms/step - loss: 0.1147 - acc: 0.9571\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 56s 819ms/step - loss: 0.0408 - acc: 0.9853\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 57s 822ms/step - loss: 0.0509 - acc: 0.9821\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 53s 775ms/step - loss: 0.0610 - acc: 0.9762\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 52s 751ms/step - loss: 0.1300 - acc: 0.9558\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 52s 751ms/step - loss: 0.0546 - acc: 0.9803\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.0380 - acc: 0.9874\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 52s 755ms/step - loss: 0.0350 - acc: 0.9874\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 52s 755ms/step - loss: 0.0533 - acc: 0.9797\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.0347 - acc: 0.9882\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 52s 753ms/step - loss: 0.0406 - acc: 0.9845\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 52s 753ms/step - loss: 0.0485 - acc: 0.9821\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 52s 757ms/step - loss: 0.0315 - acc: 0.9892\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 52s 757ms/step - loss: 0.0444 - acc: 0.9851\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 57s 824ms/step - loss: 0.0243 - acc: 0.9922\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 54s 778ms/step - loss: 0.0365 - acc: 0.9862\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.0347 - acc: 0.9881\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.0303 - acc: 0.9909\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 57s 819ms/step - loss: 0.0266 - acc: 0.9914\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 57s 826ms/step - loss: 0.0382 - acc: 0.9856\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 56s 819ms/step - loss: 0.0311 - acc: 0.9887\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 57s 822ms/step - loss: 0.0240 - acc: 0.9915\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 57s 821ms/step - loss: 0.0227 - acc: 0.9923\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 57s 829ms/step - loss: 0.0219 - acc: 0.9918\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.0840 - acc: 0.9712\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 57s 824ms/step - loss: 0.0236 - acc: 0.9918\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 57s 824ms/step - loss: 0.0175 - acc: 0.9940\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.0179 - acc: 0.9933\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.997948\n",
      "ACC: 0.987483\n",
      "MCC : 0.975275\n",
      "TPR:1.000000\n",
      "FPR:0.024889\n",
      "Pre:0.975439\n",
      "F1:0.987567\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 84s 1s/step - loss: 0.6493 - acc: 0.6284\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 67s 973ms/step - loss: 0.5795 - acc: 0.6842\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.5198 - acc: 0.7328\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 66s 956ms/step - loss: 0.4554 - acc: 0.7801\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.4074 - acc: 0.8157\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.3714 - acc: 0.8302\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.2936 - acc: 0.8712\n",
      "Epoch 8/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 66s 960ms/step - loss: 0.2912 - acc: 0.8793\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 66s 956ms/step - loss: 0.2371 - acc: 0.8973\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.2215 - acc: 0.9086\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 67s 967ms/step - loss: 0.1998 - acc: 0.9184\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.1817 - acc: 0.9248\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.1516 - acc: 0.9417\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 66s 954ms/step - loss: 0.2160 - acc: 0.9111\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.1443 - acc: 0.9400\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.1363 - acc: 0.9453\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.1949 - acc: 0.9249\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.1279 - acc: 0.9481\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.1120 - acc: 0.9561\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 66s 960ms/step - loss: 0.0883 - acc: 0.9656\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.1278 - acc: 0.9495\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 66s 956ms/step - loss: 0.0829 - acc: 0.9706\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0842 - acc: 0.9690\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 66s 964ms/step - loss: 0.0916 - acc: 0.9673\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.0952 - acc: 0.9627\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0715 - acc: 0.9740\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.1241 - acc: 0.9567\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0573 - acc: 0.9793\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0627 - acc: 0.9785\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 66s 958ms/step - loss: 0.0660 - acc: 0.9761\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.0864 - acc: 0.9688\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.0470 - acc: 0.9838\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 64s 927ms/step - loss: 0.0789 - acc: 0.9728\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 61s 881ms/step - loss: 0.0445 - acc: 0.9843\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 61s 882ms/step - loss: 0.0666 - acc: 0.9746\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 61s 882ms/step - loss: 0.0471 - acc: 0.9838\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 61s 879ms/step - loss: 0.0382 - acc: 0.9865\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 60s 876ms/step - loss: 0.0495 - acc: 0.9835\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 61s 881ms/step - loss: 0.0492 - acc: 0.9823\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 61s 881ms/step - loss: 0.0359 - acc: 0.9881\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 61s 878ms/step - loss: 0.0464 - acc: 0.9838\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 61s 879ms/step - loss: 0.0519 - acc: 0.9804\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 61s 882ms/step - loss: 0.0373 - acc: 0.9865\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 61s 880ms/step - loss: 0.0428 - acc: 0.9856\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 63s 910ms/step - loss: 0.0615 - acc: 0.9774\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 64s 927ms/step - loss: 0.0296 - acc: 0.9896\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 65s 943ms/step - loss: 0.0324 - acc: 0.9891\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 67s 965ms/step - loss: 0.0308 - acc: 0.9903\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 67s 969ms/step - loss: 0.0192 - acc: 0.9939\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 66s 959ms/step - loss: 0.0278 - acc: 0.9913\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 67s 968ms/step - loss: 0.0527 - acc: 0.9812\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 66s 963ms/step - loss: 0.0243 - acc: 0.9921\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0240 - acc: 0.9920\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 66s 961ms/step - loss: 0.0326 - acc: 0.9881\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 66s 957ms/step - loss: 0.0372 - acc: 0.9877\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 67s 968ms/step - loss: 0.0276 - acc: 0.9899\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.0177 - acc: 0.9941\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 67s 964ms/step - loss: 0.0190 - acc: 0.9935\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 67s 966ms/step - loss: 0.0370 - acc: 0.9877\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 66s 962ms/step - loss: 0.0172 - acc: 0.9955\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.996814\n",
      "ACC: 0.986142\n",
      "MCC : 0.972446\n",
      "TPR:0.995583\n",
      "FPR:0.023529\n",
      "Pre:0.977450\n",
      "F1:0.986433\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.995893\n",
      "mean ACC: 0.982391\n",
      "mean MCC : 0.965250\n",
      "mean TPR:0.997484\n",
      "mean FPR:0.032502\n",
      "mean Pre:0.968088\n",
      "mean F1:0.982536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'SC'\n",
    "for rep in range(1):\n",
    "    n_splits = 5\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    count = 0\n",
    "    for split in range(0,5):\n",
    "        train_pairs_file = 'yeast_data/new_train_valid-'+str(split) \n",
    "        test_pairs_file = 'yeast_data/new_test-'+str(split) \n",
    "#         valid_pairs_file = 'yeast_data/new_valid'+str(split) \n",
    "\n",
    "        batch_size = 128\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "#         valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        model = build_model()\n",
    "        save_model_name = 'yeast_data/sc_go_seq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "#         hist = model.fit_generator(generator=train_generator,\n",
    "#                    validation_data=valid_generator, \n",
    "#                    epochs = 100,verbose=1,callbacks=[earlyStopping, save_checkpoint] )\n",
    "\n",
    "        \n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "             epochs = 60,verbose=1)\n",
    "\n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "#         model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "#     np.savez('yeast_go_seq'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.001394\n",
      "mean ACC: 0.005396\n",
      "mean MCC : 0.010433\n",
      "mean TPR:0.001765\n",
      "mean FPR:0.009691\n",
      "mean Pre:0.010750\n",
      "mean F1:0.005706\n"
     ]
    }
   ],
   "source": [
    "print ('mean AUC: %f' % np.std(AUCs))\n",
    "print ('mean ACC: %f' % np.std(ACCs)) \n",
    "print ('mean MCC : %f' % np.std(MCCs))\n",
    "print('mean TPR:%f'% np.std(TPRs))\n",
    "print('mean FPR:%f'% np.std(FPRs))\n",
    "print('mean Pre:%f'% np.std(Precs))\n",
    "print('mean F1:%f'% np.std(F1s))\n",
    "np.savez('yeast_go_seq'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 1 layers into a model with 44 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4f49eefacf35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_ppi_pairs\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ppi_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1217\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 1 layers into a model with 44 layers."
     ]
    }
   ],
   "source": [
    "model.load_weights(save_model_name)\n",
    "with open(test_pairs_file, 'r') as f:\n",
    "    test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "test_len = len(test_ppi_pairs) \n",
    "list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "y_pred_prob = model.predict(test_x)\n",
    "\n",
    "\n",
    "y_pred = (y_pred_prob > 0.5)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "f1 = f1_score(y_test, y_pred)\n",
    "pre = precision_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = metrics.auc(recall, precision)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "total=tn+fp+fn+tp\n",
    "sen = float(tp)/float(tp+fn)\n",
    "sps = float(tn)/float((tn+fp))\n",
    "\n",
    "tpr = float(tp)/float(tp+fn)\n",
    "fpr = float(fp)/float((tn+fp))\n",
    "print('--------------------------\\n')\n",
    "print ('AUC: %f' % auc)\n",
    "print ('ACC: %f' % acc) \n",
    "# print(\"PRAUC: %f\" % pr_auc)\n",
    "print ('MCC : %f' % mcc)\n",
    "# print ('SEN: %f' % sen)\n",
    "# print ('SEP: %f' % sps)\n",
    "print('TPR:%f'%tpr)\n",
    "print('FPR:%f'%fpr)\n",
    "print('Pre:%f'%pre)\n",
    "print('F1:%f'%f1)\n",
    "print('--------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model()\n",
    "# hist = model.fit_generator(generator=train_generator,\n",
    "#                    epochs = 200,verbose=1,validation_data = valid_generator,\n",
    "#                                   callbacks=[earlyStopping, save_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
