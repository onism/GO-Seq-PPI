{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7df4094ed0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding(words4sent, max_seq_len, feature_dim,   to_reverse=0):\n",
    "    length = []\n",
    "    output = []\n",
    "    \n",
    "    for words in words4sent:\n",
    "        if to_reverse:\n",
    "            words = np.flip(words, 0)\n",
    "        length.append( words.shape[0])\n",
    "        if  words.shape[0] < max_seq_len:\n",
    "            wordList = np.concatenate([words,np.zeros([max_seq_len - words.shape[0],feature_dim])])\n",
    "        output.append(wordList)\n",
    "    return np.array(output),np.array(length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(x, lengths):\n",
    "    out = torch.FloatTensor(x.size(1), x.size(2)).zero_() # BxF\n",
    "    for i in range(x.size(1)):\n",
    "        out[i] = torch.mean(x[:lengths[i],i,:], 0)\n",
    "    return out\n",
    "\n",
    "\n",
    "class RandLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_layers, output_dim,  bidirectional=False):\n",
    "        super(RandLSTM, self).__init__()\n",
    "        \n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.max_seq_len = 128\n",
    "        self.input_dim = input_dim\n",
    "         \n",
    "\n",
    "        self.e_hid_init = torch.zeros(1, 1, output_dim)\n",
    "        self.e_cell_init = torch.zeros(1, 1, output_dim)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lm = nn.LSTM(input_dim, output_dim, num_layers=num_layers,\n",
    "                          bidirectional= self.bidirectional, batch_first=True)\n",
    "\n",
    "        self.bidirectional += 1\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "   \n",
    "\n",
    "    def lstm(self, inputs, lengths):\n",
    "        bsz, max_len, _ = inputs.size()\n",
    "        in_embs = inputs\n",
    "        lens, indices = torch.sort(lengths, 0, True)\n",
    "\n",
    "        e_hid_init = self.e_hid_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        e_cell_init = self.e_cell_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        all_hids, (enc_last_hid, _) = self.lm(pack(in_embs[indices],\n",
    "                                                        lens.tolist(), batch_first=True), (e_hid_init, e_cell_init))\n",
    "        _, _indices = torch.sort(indices, 0)\n",
    "        all_hids = unpack(all_hids, batch_first=True)[0][_indices]\n",
    "\n",
    "        return all_hids\n",
    "\n",
    "    def forward(self, words4sent):\n",
    "        \n",
    "        out, lengths = gen_embedding(words4sent, self.max_seq_len, self.input_dim)\n",
    "        out = torch.from_numpy(out).float()\n",
    "        lengths = torch.from_numpy(np.array(lengths))\n",
    "        out = self.lstm(out, lengths)\n",
    "#         print(\"output size:\",out.size())\n",
    "        out = out.transpose(1,0)\n",
    "        out = mean_pool(out, lengths)\n",
    "        return out\n",
    "\n",
    "    def encode(self, batch):\n",
    "        return self.forward(batch).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract w2v model\n",
    "w2vmodel =  Word2Vec.load('/home/xhh/PMC_model/PMC_model.txt')\n",
    "def vector_name(name): \n",
    "    s = name.split(' ')\n",
    "    vectors = [] \n",
    "    for word in s: \n",
    "        if w2vmodel.wv.__contains__(word):   \n",
    "            vectors.append(w2vmodel.wv[word]) \n",
    "    else: \n",
    "        clear_words = re.sub('[^A-Za-z0-9]+', ' ', word) \n",
    "        clear_words = clear_words.lstrip().rstrip().split(' ')\n",
    "        for w in clear_words: \n",
    "            if w2vmodel.wv.__contains__(w): \n",
    "                vectors.append(w2vmodel.wv[w]) \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read go.obo obtain ontology type\n",
    "id_type_dicts = {}\n",
    "obo_file = '../cross-species/go.obo'\n",
    "fp=open(obo_file,'r')\n",
    "obo_txt=fp.read()\n",
    "fp.close()\n",
    "obo_txt=obo_txt[obo_txt.find(\"[Term]\")-1:]\n",
    "obo_txt=obo_txt[:obo_txt.find(\"[Typedef]\")]\n",
    "# obo_dict=parse_obo_txt(obo_txt)\n",
    "id_name_dicts = {}\n",
    "for Term_txt in obo_txt.split(\"[Term]\\n\"):\n",
    "    if not Term_txt.strip():\n",
    "        continue\n",
    "    name = ''\n",
    "    ids = []\n",
    "    for line in Term_txt.splitlines():\n",
    "        if   line.startswith(\"id: \"):\n",
    "            ids.append(line[len(\"id: \"):])     \n",
    "        elif line.startswith(\"name: \"):\n",
    "             name=line[len(\"name: \"):]\n",
    "        elif line.startswith(\"alt_id: \"):\n",
    "            ids.append(line[len(\"alt_id: \"):])\n",
    "    \n",
    "    for t_id in ids:\n",
    "        id_name_dicts[t_id] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "protein2go =  load_dict('yeast_data/protein2go_dicts.pkl')\n",
    "prot2emb_w2v = {}\n",
    "project_dim = 1024\n",
    "num_layers = 1\n",
    "max_go_len = 1024\n",
    "max_protlen = 0\n",
    "biLSTM_Flag = False\n",
    "w2vlstm = RandLSTM(200,num_layers,  project_dim, bidirectional = biLSTM_Flag)\n",
    "\n",
    "\n",
    "for key, value in protein2go.items(): \n",
    "    allgos = value.split(';') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    words4sent = []\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.array(vector_name(id_name_dicts[go]))\n",
    "            if feature.shape[0] > 0:\n",
    "                words4sent.append(feature)\n",
    "            \n",
    "        count += feature.shape[0]\n",
    "    if len(words4sent) > 0:\n",
    "        sent_embedding = w2vlstm.encode(words4sent)\n",
    "    else:\n",
    "        if biLSTM_Flag:\n",
    "            sent_embedding = np.zeros((1, project_dim*2))\n",
    "        else:\n",
    "            sent_embedding = np.zeros((1, project_dim))\n",
    "\n",
    "    if max_protlen < sent_embedding.shape[0]:\n",
    "        max_protlen = sent_embedding.shape[0]\n",
    "    prot2emb_w2v[key] = sent_embedding \n",
    "\n",
    "del w2vmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "print(max_protlen)\n",
    "# w2v_len = max_protlen\n",
    "w2v_len = 256\n",
    "# w2v_len = 211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot2emb_bert = {}\n",
    "max_protlen = 0\n",
    "input_dim = 768\n",
    " \n",
    "bertlstm = RandLSTM(input_dim,num_layers,  project_dim, bidirectional = biLSTM_Flag)\n",
    "for key, value in protein2go.items():\n",
    "     \n",
    "    allgos = value.split(';') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    words4sent = []\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            words4sent.append(feature)\n",
    "        count += feature.shape[0] \n",
    "    if len(words4sent) > 0:\n",
    "        sent_embedding = bertlstm.encode(words4sent)\n",
    "    else:\n",
    "        if biLSTM_Flag:\n",
    "            sent_embedding = np.zeros((1, project_dim*2))\n",
    "        else:\n",
    "            sent_embedding = np.zeros((1, project_dim))\n",
    "\n",
    "    if max_protlen < sent_embedding.shape[0]:\n",
    "        max_protlen = sent_embedding.shape[0]\n",
    "    prot2emb_bert[key] = sent_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_protlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_len = max_protlen\n",
    "bert_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_dict(prot2emb_bert, 'yeast_data/bert_sent_2048.pkl')\n",
    "# save_dict(prot2emb_w2v, 'yeast_data/w2v_sent_2048.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "\n",
    "max_seq_len = 1000\n",
    "# max_protlen = 32\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "        input_dim = 768\n",
    "        num_layers = 1\n",
    "        projection_dim = 2048\n",
    "        if biLSTM_Flag:\n",
    "            self.projection_dim = project_dim*2\n",
    "        else:\n",
    "            self.projection_dim = project_dim\n",
    "        self.bert_len = bert_len\n",
    "        self.w2v_len = w2v_len\n",
    "\n",
    "        self.max_seqlen = max_seq_len  \n",
    "        self.protein2seq = load_dict('yeast_data/protein2seq_dicts.pkl')\n",
    "        self.read_ppi()\n",
    "         \n",
    "        self.prot2emb_bert =  prot2emb_bert\n",
    "        self.prot2emb_w2v = prot2emb_w2v\n",
    "#         self.prot2embedding() \n",
    "         \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "         \n",
    "        y = np.empty((self.batch_size))\n",
    "        X_go1 = np.empty((self.batch_size, self.bert_len + self.w2v_len,self.projection_dim))\n",
    "        X_go2 = np.empty((self.batch_size, self.bert_len + self.w2v_len,self.projection_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "             \n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "            \n",
    "            prot1emb_w2v = self.prot2emb_w2v[p1]\n",
    "            X_go1[i,prot1emb_bert.shape[0]:prot1emb_w2v.shape[0] + prot1emb_bert.shape[0]] = prot1emb_w2v\n",
    "            \n",
    "            prot2emb_w2v = self.prot2emb_w2v[p2]\n",
    "            X_go2[i,prot2emb_bert.shape[0]:prot2emb_bert.shape[0] + prot2emb_w2v.shape[0] ] = prot2emb_w2v\n",
    "            \n",
    "         \n",
    "            \n",
    "            \n",
    "        return [X_go1, X_go2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "         \n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.bert_len + self.w2v_len,self.projection_dim))\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.bert_len + self.w2v_len,self.projection_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "             \n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "            \n",
    "            prot1emb_w2v = self.prot2emb_w2v[p1]\n",
    "            X_go1[i,prot1emb_bert.shape[0]:prot1emb_w2v.shape[0] + prot1emb_bert.shape[0]] = prot1emb_w2v\n",
    "            \n",
    "            prot2emb_w2v = self.prot2emb_w2v[p2]\n",
    "            X_go2[i,prot2emb_bert.shape[0]:prot2emb_bert.shape[0] + prot2emb_w2v.shape[0] ] = prot2emb_w2v\n",
    "            \n",
    "         \n",
    "           \n",
    "        return [X_go1, X_go2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 512, 1024)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512, 1024)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 508, 32)      163872      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 508, 32)      163872      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 504, 64)      10304       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 504, 64)      10304       conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 500, 96)      30816       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 500, 96)      30816       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 96)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 96)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 96)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 96)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         394240      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1049600     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          524800      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            513         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,379,137\n",
      "Trainable params: 2,379,137\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(4)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "class ResidualConv1D:\n",
    "    \"\"\"\n",
    "    ***ResidualConv1D for use with best performing classifier***\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filters, kernel_size, pool=False):\n",
    "        self.pool = pool\n",
    "        self.kernel_size = kernel_size\n",
    "        self.params = {\n",
    "            \"padding\": \"same\",\n",
    "            \"kernel_initializer\": \"he_uniform\",\n",
    "            \"strides\": 1,\n",
    "            \"filters\": filters,\n",
    "        }\n",
    "\n",
    "    def build(self, x):\n",
    "\n",
    "        res = x\n",
    "        if self.pool:\n",
    "            x = MaxPooling1D(1, padding=\"same\")(x)\n",
    "            res = Conv1D(kernel_size=1, **self.params)(res)\n",
    "\n",
    "        out = Conv1D(kernel_size=1, **self.params)(x)\n",
    "\n",
    "#         out = BatchNormalization(momentum=0.9)(out)\n",
    "        out = Activation(\"relu\")(out)\n",
    "        out = Conv1D(kernel_size=self.kernel_size, **self.params)(out)\n",
    "\n",
    "#         out = BatchNormalization(momentum=0.9)(out)\n",
    "        out = Activation(\"relu\")(out)\n",
    "        out = Conv1D(kernel_size=self.kernel_size, **self.params)(out)\n",
    "\n",
    "        out = keras.layers.add([res, out])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.build(x)\n",
    "def build_residual_cnn(input_x):\n",
    "    \n",
    "\n",
    "    x = Conv1D(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=\"he_uniform\",\n",
    "    )(input_x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    # residual net part\n",
    "    x = ResidualConv1D(filters=32, kernel_size=3, pool=True)(x)\n",
    "    x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "    x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "    \n",
    "    \n",
    "#     x = ResidualConv1D(filters=32, kernel_size=3, pool=True)(x)\n",
    "#     x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "#     x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "\n",
    "    \n",
    "#     x = ResidualConv1D(filters=32, kernel_size=3, pool=True)(x)\n",
    "#     x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "#     x = ResidualConv1D(filters=32, kernel_size=3)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "#     x = BatchNormalization(momentum=0.9)(x)\n",
    "#     x = Activation(\"relu\")(x)\n",
    "#     x = MaxPooling1D(1, padding=\"same\")(x)\n",
    "#     x  = GlobalAveragePooling1D()(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "def build_fc_model(input_x):\n",
    "     \n",
    "    \n",
    "    x_a = GlobalAveragePooling1D()(input_x)\n",
    "    x_b = GlobalMaxPooling1D()(input_x)\n",
    "#     x_c = Attention()(input_x)\n",
    "    x = Concatenate()([x_a, x_b])\n",
    "    x = Dense(1024)(x)\n",
    "#     x = Dense(256)(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "if biLSTM_Flag:\n",
    "    input_dim = project_dim * 2\n",
    "else:\n",
    "    input_dim = project_dim\n",
    "    \n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    \n",
    "    left_input_go = Input(shape=(bert_len+w2v_len,input_dim))\n",
    "    right_input_go = Input(shape=(bert_len+w2v_len,input_dim))\n",
    "    \n",
    "    \n",
    "    \n",
    "    NUM_FILTERS = 32\n",
    "    FILTER_LENGTH1 = 5\n",
    "    FILTER_LENGTH2 = 5\n",
    "    \n",
    "    \n",
    "    left_x_go= Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(left_input_go)\n",
    "    left_x_go = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(left_x_go)\n",
    "    left_x_go = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH1,  activation='relu', padding='valid',  strides=1)(left_x_go)\n",
    "    left_x_go_max = GlobalMaxPooling1D()(left_x_go) #pool_size=pool_length[i]\n",
    "    left_x_go_avg = GlobalAveragePooling1D()(left_x_go) #pool_size=pool_length[i]\n",
    "\n",
    "\n",
    "    right_x_go = Conv1D(filters=NUM_FILTERS, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(right_input_go)\n",
    "    right_x_go = Conv1D(filters=NUM_FILTERS*2, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(right_x_go)\n",
    "    right_x_go = Conv1D(filters=NUM_FILTERS*3, kernel_size=FILTER_LENGTH2,  activation='relu', padding='valid',  strides=1)(right_x_go)\n",
    "    right_x_go_max = GlobalMaxPooling1D()(right_x_go)\n",
    "    right_x_go_avg = GlobalAveragePooling1D()(right_x_go)\n",
    "\n",
    "    \n",
    "    \n",
    "#     left_x_go = build_residual_cnn(left_input_go)\n",
    "#     right_x_go = build_residual_cnn(right_input_go)\n",
    "     \n",
    "     \n",
    "    x =   Concatenate()([left_x_go_avg, left_x_go_max  , right_x_go_avg, right_x_go_max])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "    optimizer = Lookahead(RAdam())\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "69/69 [==============================] - 58s 835ms/step - loss: 0.6652 - acc: 0.6155\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 53s 767ms/step - loss: 0.6436 - acc: 0.6259\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 53s 765ms/step - loss: 0.5931 - acc: 0.6651\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 53s 766ms/step - loss: 0.5264 - acc: 0.7313\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 53s 766ms/step - loss: 0.4671 - acc: 0.7742\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 53s 763ms/step - loss: 0.4054 - acc: 0.8106\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 0.3327 - acc: 0.8548\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.3144 - acc: 0.8585\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 51s 743ms/step - loss: 0.2468 - acc: 0.8940\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 51s 739ms/step - loss: 0.2235 - acc: 0.9002\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 53s 767ms/step - loss: 0.2023 - acc: 0.9086\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 54s 776ms/step - loss: 0.2059 - acc: 0.9058\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 52s 760ms/step - loss: 0.1826 - acc: 0.9175\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 52s 760ms/step - loss: 0.1740 - acc: 0.9238\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 52s 761ms/step - loss: 0.1553 - acc: 0.9287\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 53s 767ms/step - loss: 0.1715 - acc: 0.9246\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 53s 763ms/step - loss: 0.1314 - acc: 0.9383\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 52s 753ms/step - loss: 0.1271 - acc: 0.9437\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 52s 758ms/step - loss: 0.1915 - acc: 0.9179\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 53s 762ms/step - loss: 0.1260 - acc: 0.9464\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.1370 - acc: 0.9404\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 52s 759ms/step - loss: 0.1029 - acc: 0.9566\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 52s 755ms/step - loss: 0.0915 - acc: 0.9618\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 52s 751ms/step - loss: 0.0923 - acc: 0.9633\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 52s 759ms/step - loss: 0.1321 - acc: 0.9472\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 52s 753ms/step - loss: 0.0898 - acc: 0.9615\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 52s 752ms/step - loss: 0.0794 - acc: 0.9678\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 51s 739ms/step - loss: 0.0751 - acc: 0.9689\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 51s 735ms/step - loss: 0.1014 - acc: 0.9587\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 51s 739ms/step - loss: 0.0788 - acc: 0.9694\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 51s 737ms/step - loss: 0.0747 - acc: 0.9727\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 51s 736ms/step - loss: 0.1110 - acc: 0.9578\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.0775 - acc: 0.9717\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 51s 739ms/step - loss: 0.0539 - acc: 0.9803\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 51s 737ms/step - loss: 0.0493 - acc: 0.9818\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 51s 737ms/step - loss: 0.0534 - acc: 0.9804\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 51s 742ms/step - loss: 0.0609 - acc: 0.9784\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 51s 741ms/step - loss: 0.0540 - acc: 0.9806\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 51s 745ms/step - loss: 0.0841 - acc: 0.9675\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 55s 791ms/step - loss: 0.0387 - acc: 0.9847\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 52s 753ms/step - loss: 0.0477 - acc: 0.9821\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 51s 738ms/step - loss: 0.0672 - acc: 0.9768\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 51s 740ms/step - loss: 0.0295 - acc: 0.9898\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 51s 742ms/step - loss: 0.0291 - acc: 0.9898\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 51s 736ms/step - loss: 0.0405 - acc: 0.9851\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 51s 740ms/step - loss: 0.0501 - acc: 0.9827\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 51s 741ms/step - loss: 0.0582 - acc: 0.9779\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 52s 749ms/step - loss: 0.0583 - acc: 0.9794\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 51s 743ms/step - loss: 0.0321 - acc: 0.9872\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 51s 740ms/step - loss: 0.0232 - acc: 0.9932\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 51s 739ms/step - loss: 0.0505 - acc: 0.9819\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 51s 738ms/step - loss: 0.0282 - acc: 0.9900\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 51s 739ms/step - loss: 0.0288 - acc: 0.9901\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 51s 745ms/step - loss: 0.0274 - acc: 0.9901\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 52s 751ms/step - loss: 0.0289 - acc: 0.9908\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 52s 755ms/step - loss: 0.0308 - acc: 0.9894\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 53s 763ms/step - loss: 0.0594 - acc: 0.9783\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 0.0563 - acc: 0.9803\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 53s 771ms/step - loss: 0.0234 - acc: 0.9918\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 53s 768ms/step - loss: 0.0395 - acc: 0.9866\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.993261\n",
      "ACC: 0.976765\n",
      "MCC : 0.953937\n",
      "TPR:0.995784\n",
      "FPR:0.044677\n",
      "Pre:0.961726\n",
      "F1:0.978459\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 54s 787ms/step - loss: 0.6659 - acc: 0.6161\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 53s 768ms/step - loss: 0.6457 - acc: 0.6209\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.5968 - acc: 0.6596\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 53s 765ms/step - loss: 0.5337 - acc: 0.7180\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 52s 759ms/step - loss: 0.4734 - acc: 0.7652\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 51s 743ms/step - loss: 0.4200 - acc: 0.8012\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 51s 741ms/step - loss: 0.3316 - acc: 0.8517\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 51s 746ms/step - loss: 0.2855 - acc: 0.8748\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 51s 746ms/step - loss: 0.2386 - acc: 0.8949\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 53s 762ms/step - loss: 0.2164 - acc: 0.9035\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.2647 - acc: 0.8802\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.1680 - acc: 0.9263\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 55s 796ms/step - loss: 0.1907 - acc: 0.9125\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 55s 802ms/step - loss: 0.1501 - acc: 0.9369\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.1966 - acc: 0.9151\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 56s 810ms/step - loss: 0.1435 - acc: 0.9389\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 55s 796ms/step - loss: 0.1355 - acc: 0.9437\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.1456 - acc: 0.9330\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 55s 800ms/step - loss: 0.1244 - acc: 0.9462\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 55s 797ms/step - loss: 0.1111 - acc: 0.9548\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 55s 802ms/step - loss: 0.2148 - acc: 0.9027\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 55s 797ms/step - loss: 0.1209 - acc: 0.9484\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 55s 803ms/step - loss: 0.0994 - acc: 0.9582\n",
      "Epoch 24/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 55s 794ms/step - loss: 0.1046 - acc: 0.9578\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 56s 805ms/step - loss: 0.1145 - acc: 0.9520\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.0878 - acc: 0.9651\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 56s 805ms/step - loss: 0.0748 - acc: 0.9703\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 55s 797ms/step - loss: 0.0711 - acc: 0.9725\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 55s 795ms/step - loss: 0.0808 - acc: 0.9695\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 55s 798ms/step - loss: 0.0632 - acc: 0.9778\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.0698 - acc: 0.9729\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.0620 - acc: 0.9771\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.0554 - acc: 0.9791\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0615 - acc: 0.9767\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.0798 - acc: 0.9723\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.0464 - acc: 0.9835\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 55s 796ms/step - loss: 0.0534 - acc: 0.9805\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 55s 799ms/step - loss: 0.0466 - acc: 0.9820\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 55s 803ms/step - loss: 0.0585 - acc: 0.9770\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 55s 791ms/step - loss: 0.0628 - acc: 0.9777\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.1297 - acc: 0.9494\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.0575 - acc: 0.9793\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.0438 - acc: 0.9835\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 56s 816ms/step - loss: 0.0448 - acc: 0.9860\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 55s 801ms/step - loss: 0.0311 - acc: 0.9895\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 55s 802ms/step - loss: 0.0392 - acc: 0.9866\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 55s 801ms/step - loss: 0.0313 - acc: 0.9886\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 54s 783ms/step - loss: 0.0645 - acc: 0.9779\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 58s 841ms/step - loss: 0.0297 - acc: 0.9896\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.0270 - acc: 0.9899\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 57s 827ms/step - loss: 0.0322 - acc: 0.9886\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 0.0379 - acc: 0.9857\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 52s 760ms/step - loss: 0.0367 - acc: 0.9866\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.0351 - acc: 0.9874\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 56s 811ms/step - loss: 0.0391 - acc: 0.9840\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 56s 805ms/step - loss: 0.0255 - acc: 0.9908\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 55s 792ms/step - loss: 0.0182 - acc: 0.9941\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 53s 775ms/step - loss: 0.0241 - acc: 0.9911\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 55s 802ms/step - loss: 0.0324 - acc: 0.9885\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 57s 828ms/step - loss: 0.0215 - acc: 0.9923\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.993384\n",
      "ACC: 0.967382\n",
      "MCC : 0.936297\n",
      "TPR:0.995387\n",
      "FPR:0.058925\n",
      "Pre:0.940715\n",
      "F1:0.967279\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 59s 861ms/step - loss: 0.6660 - acc: 0.6230\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.6468 - acc: 0.6248\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 55s 798ms/step - loss: 0.6109 - acc: 0.6516\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.5507 - acc: 0.7044\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 55s 797ms/step - loss: 0.4906 - acc: 0.7569\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 56s 807ms/step - loss: 0.4125 - acc: 0.8056\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 55s 801ms/step - loss: 0.3570 - acc: 0.8384\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 55s 798ms/step - loss: 0.3045 - acc: 0.8622\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.2928 - acc: 0.8664\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 55s 800ms/step - loss: 0.2390 - acc: 0.8936\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 55s 799ms/step - loss: 0.2341 - acc: 0.8947\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 55s 800ms/step - loss: 0.1987 - acc: 0.9099\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 55s 800ms/step - loss: 0.1818 - acc: 0.9192\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 55s 801ms/step - loss: 0.1872 - acc: 0.9146\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 55s 795ms/step - loss: 0.1487 - acc: 0.9331\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 55s 800ms/step - loss: 0.1426 - acc: 0.9400\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.1909 - acc: 0.9160\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.1275 - acc: 0.9457\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 56s 812ms/step - loss: 0.1330 - acc: 0.9426\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 53s 771ms/step - loss: 0.1100 - acc: 0.9519\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.1424 - acc: 0.9360\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.1197 - acc: 0.9496\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 57s 826ms/step - loss: 0.0966 - acc: 0.9616\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0978 - acc: 0.9600\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.0822 - acc: 0.9672\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.1663 - acc: 0.9339\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.1039 - acc: 0.9614\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.0731 - acc: 0.9729\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 54s 783ms/step - loss: 0.0663 - acc: 0.9743\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 52s 757ms/step - loss: 0.0763 - acc: 0.9712\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 54s 788ms/step - loss: 0.0677 - acc: 0.9733\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.0641 - acc: 0.9748\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 56s 816ms/step - loss: 0.0552 - acc: 0.9802\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 56s 811ms/step - loss: 0.0511 - acc: 0.9805\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.0606 - acc: 0.9775\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.0758 - acc: 0.9718\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0394 - acc: 0.9858\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 53s 773ms/step - loss: 0.0434 - acc: 0.9845\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 53s 771ms/step - loss: 0.0493 - acc: 0.9819\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 56s 805ms/step - loss: 0.0434 - acc: 0.9838\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0396 - acc: 0.9857\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 57s 831ms/step - loss: 0.0431 - acc: 0.9834\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 58s 841ms/step - loss: 0.0509 - acc: 0.9806\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.0417 - acc: 0.9840\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 56s 816ms/step - loss: 0.0296 - acc: 0.9894\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 55s 801ms/step - loss: 0.0641 - acc: 0.9780\n",
      "Epoch 47/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 52s 757ms/step - loss: 0.6495 - acc: 0.6756\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.3841 - acc: 0.8222\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 57s 825ms/step - loss: 0.1466 - acc: 0.9421\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 57s 824ms/step - loss: 0.0671 - acc: 0.9750\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 57s 826ms/step - loss: 0.0652 - acc: 0.9771\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 57s 822ms/step - loss: 0.0697 - acc: 0.9754\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 53s 769ms/step - loss: 0.0554 - acc: 0.9795\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 53s 775ms/step - loss: 0.0428 - acc: 0.9852\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.1467 - acc: 0.9470\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.0531 - acc: 0.9811\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 56s 811ms/step - loss: 0.0377 - acc: 0.9862\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 56s 807ms/step - loss: 0.0376 - acc: 0.9848\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 56s 815ms/step - loss: 0.0288 - acc: 0.9904\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 54s 789ms/step - loss: 0.0440 - acc: 0.9854\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.994446\n",
      "ACC: 0.970943\n",
      "MCC : 0.943234\n",
      "TPR:0.997220\n",
      "FPR:0.053541\n",
      "Pre:0.945518\n",
      "F1:0.970681\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 55s 799ms/step - loss: 0.6647 - acc: 0.6060\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 52s 758ms/step - loss: 0.6402 - acc: 0.6303\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 0.5858 - acc: 0.6778\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 52s 755ms/step - loss: 0.5287 - acc: 0.7232\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.4825 - acc: 0.7589\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 52s 754ms/step - loss: 0.4257 - acc: 0.8024\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 53s 763ms/step - loss: 0.3548 - acc: 0.8380\n",
      "Epoch 8/60\n",
      "69/69 [==============================] - 52s 756ms/step - loss: 0.3288 - acc: 0.8534\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 53s 767ms/step - loss: 0.2748 - acc: 0.8781\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 52s 760ms/step - loss: 0.2404 - acc: 0.8958\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 52s 758ms/step - loss: 0.2263 - acc: 0.9004\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 52s 759ms/step - loss: 0.2031 - acc: 0.9109\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 52s 760ms/step - loss: 0.1935 - acc: 0.9136\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 53s 764ms/step - loss: 0.1811 - acc: 0.9221\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 53s 769ms/step - loss: 0.1532 - acc: 0.9350\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 57s 828ms/step - loss: 0.1566 - acc: 0.9317\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 57s 830ms/step - loss: 0.1236 - acc: 0.9424\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 57s 832ms/step - loss: 0.1715 - acc: 0.9215\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.1128 - acc: 0.9500\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 58s 835ms/step - loss: 0.1080 - acc: 0.9541\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 58s 840ms/step - loss: 0.1176 - acc: 0.9512\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 58s 836ms/step - loss: 0.1052 - acc: 0.9556\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 59s 855ms/step - loss: 0.0850 - acc: 0.9660\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 57s 825ms/step - loss: 0.5901 - acc: 0.7380\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.5660 - acc: 0.7169\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.4884 - acc: 0.7655\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 57s 829ms/step - loss: 0.4269 - acc: 0.7999\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 58s 834ms/step - loss: 0.3923 - acc: 0.8192\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 57s 831ms/step - loss: 0.3351 - acc: 0.8484\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.2887 - acc: 0.8721\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 52s 758ms/step - loss: 0.2560 - acc: 0.8887\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 54s 776ms/step - loss: 0.2091 - acc: 0.9130\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 56s 811ms/step - loss: 0.1809 - acc: 0.9252\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.1675 - acc: 0.9266\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 56s 815ms/step - loss: 0.1334 - acc: 0.9441\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 58s 834ms/step - loss: 0.1342 - acc: 0.9435\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.1210 - acc: 0.9503\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 57s 826ms/step - loss: 0.1071 - acc: 0.9543\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 56s 807ms/step - loss: 0.1140 - acc: 0.9514\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.0976 - acc: 0.9598\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 57s 829ms/step - loss: 0.0956 - acc: 0.9627\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 55s 804ms/step - loss: 0.0905 - acc: 0.9640\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.0725 - acc: 0.9716\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 56s 816ms/step - loss: 0.0658 - acc: 0.9731\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.1378 - acc: 0.9515\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 56s 812ms/step - loss: 0.0671 - acc: 0.9732\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 58s 841ms/step - loss: 0.0642 - acc: 0.9758\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.0587 - acc: 0.9778\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.0437 - acc: 0.9843\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.0504 - acc: 0.9815\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.0676 - acc: 0.9745\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 56s 810ms/step - loss: 0.0633 - acc: 0.9763\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 52s 758ms/step - loss: 0.0453 - acc: 0.9829\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 53s 769ms/step - loss: 0.0430 - acc: 0.9854\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 54s 786ms/step - loss: 0.0574 - acc: 0.9783\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 52s 753ms/step - loss: 0.0408 - acc: 0.9841\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 52s 755ms/step - loss: 0.0631 - acc: 0.9788\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 56s 806ms/step - loss: 0.0288 - acc: 0.9898\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.0397 - acc: 0.9846\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.0346 - acc: 0.9864\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.995957\n",
      "ACC: 0.971390\n",
      "MCC : 0.943522\n",
      "TPR:0.991007\n",
      "FPR:0.048000\n",
      "Pre:0.953287\n",
      "F1:0.971781\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/60\n",
      "69/69 [==============================] - 59s 857ms/step - loss: 0.6635 - acc: 0.6224\n",
      "Epoch 2/60\n",
      "69/69 [==============================] - 57s 831ms/step - loss: 0.6338 - acc: 0.6355\n",
      "Epoch 3/60\n",
      "69/69 [==============================] - 57s 831ms/step - loss: 0.5816 - acc: 0.6841\n",
      "Epoch 4/60\n",
      "69/69 [==============================] - 56s 810ms/step - loss: 0.5217 - acc: 0.7334\n",
      "Epoch 5/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.4780 - acc: 0.7696\n",
      "Epoch 6/60\n",
      "69/69 [==============================] - 56s 810ms/step - loss: 0.4320 - acc: 0.8003\n",
      "Epoch 7/60\n",
      "69/69 [==============================] - 56s 807ms/step - loss: 0.3346 - acc: 0.8541\n",
      "Epoch 8/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 57s 821ms/step - loss: 0.3024 - acc: 0.8644\n",
      "Epoch 9/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.2534 - acc: 0.8864\n",
      "Epoch 10/60\n",
      "69/69 [==============================] - 57s 827ms/step - loss: 0.2344 - acc: 0.8950\n",
      "Epoch 11/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.2169 - acc: 0.9021\n",
      "Epoch 12/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.3125 - acc: 0.8468\n",
      "Epoch 13/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.2119 - acc: 0.9072\n",
      "Epoch 14/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.1878 - acc: 0.9173\n",
      "Epoch 15/60\n",
      "69/69 [==============================] - 52s 759ms/step - loss: 0.1591 - acc: 0.9289\n",
      "Epoch 16/60\n",
      "69/69 [==============================] - 52s 757ms/step - loss: 0.1529 - acc: 0.9312\n",
      "Epoch 17/60\n",
      "69/69 [==============================] - 55s 795ms/step - loss: 0.1394 - acc: 0.9369\n",
      "Epoch 18/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.2079 - acc: 0.8988\n",
      "Epoch 19/60\n",
      "69/69 [==============================] - 56s 812ms/step - loss: 0.1489 - acc: 0.9357\n",
      "Epoch 20/60\n",
      "69/69 [==============================] - 57s 820ms/step - loss: 0.1399 - acc: 0.9408\n",
      "Epoch 21/60\n",
      "69/69 [==============================] - 57s 829ms/step - loss: 0.1189 - acc: 0.9494\n",
      "Epoch 22/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.2208 - acc: 0.8916\n",
      "Epoch 23/60\n",
      "69/69 [==============================] - 57s 831ms/step - loss: 0.2178 - acc: 0.9019\n",
      "Epoch 24/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.1382 - acc: 0.9370\n",
      "Epoch 25/60\n",
      "69/69 [==============================] - 57s 821ms/step - loss: 0.1339 - acc: 0.9383\n",
      "Epoch 26/60\n",
      "69/69 [==============================] - 58s 839ms/step - loss: 0.1194 - acc: 0.9443\n",
      "Epoch 27/60\n",
      "69/69 [==============================] - 59s 853ms/step - loss: 0.1067 - acc: 0.9564\n",
      "Epoch 28/60\n",
      "69/69 [==============================] - 57s 828ms/step - loss: 0.1249 - acc: 0.9477\n",
      "Epoch 29/60\n",
      "69/69 [==============================] - 56s 805ms/step - loss: 0.1005 - acc: 0.9558\n",
      "Epoch 30/60\n",
      "69/69 [==============================] - 57s 828ms/step - loss: 0.3567 - acc: 0.8366\n",
      "Epoch 31/60\n",
      "69/69 [==============================] - 57s 828ms/step - loss: 0.5013 - acc: 0.7488\n",
      "Epoch 32/60\n",
      "69/69 [==============================] - 56s 811ms/step - loss: 0.3331 - acc: 0.8509\n",
      "Epoch 33/60\n",
      "69/69 [==============================] - 56s 819ms/step - loss: 0.2514 - acc: 0.8928\n",
      "Epoch 34/60\n",
      "69/69 [==============================] - 57s 823ms/step - loss: 0.1441 - acc: 0.9364\n",
      "Epoch 35/60\n",
      "69/69 [==============================] - 56s 817ms/step - loss: 0.1237 - acc: 0.9468\n",
      "Epoch 36/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.1233 - acc: 0.9479\n",
      "Epoch 37/60\n",
      "69/69 [==============================] - 56s 807ms/step - loss: 0.1121 - acc: 0.9522\n",
      "Epoch 38/60\n",
      "69/69 [==============================] - 57s 829ms/step - loss: 0.1273 - acc: 0.9433\n",
      "Epoch 39/60\n",
      "69/69 [==============================] - 57s 831ms/step - loss: 0.1288 - acc: 0.9476\n",
      "Epoch 40/60\n",
      "69/69 [==============================] - 55s 800ms/step - loss: 0.0935 - acc: 0.9621\n",
      "Epoch 41/60\n",
      "69/69 [==============================] - 52s 761ms/step - loss: 0.0778 - acc: 0.9672\n",
      "Epoch 42/60\n",
      "69/69 [==============================] - 52s 757ms/step - loss: 0.0794 - acc: 0.9690\n",
      "Epoch 43/60\n",
      "69/69 [==============================] - 54s 786ms/step - loss: 0.0729 - acc: 0.9710\n",
      "Epoch 44/60\n",
      "69/69 [==============================] - 58s 847ms/step - loss: 0.0927 - acc: 0.9615\n",
      "Epoch 45/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0674 - acc: 0.9740\n",
      "Epoch 46/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.0660 - acc: 0.9755\n",
      "Epoch 47/60\n",
      "69/69 [==============================] - 56s 808ms/step - loss: 0.0595 - acc: 0.9768\n",
      "Epoch 48/60\n",
      "69/69 [==============================] - 56s 815ms/step - loss: 0.0617 - acc: 0.9763\n",
      "Epoch 49/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.0425 - acc: 0.9843\n",
      "Epoch 50/60\n",
      "69/69 [==============================] - 56s 814ms/step - loss: 0.0611 - acc: 0.9763\n",
      "Epoch 51/60\n",
      "69/69 [==============================] - 58s 835ms/step - loss: 0.0545 - acc: 0.9805\n",
      "Epoch 52/60\n",
      "69/69 [==============================] - 57s 821ms/step - loss: 0.0694 - acc: 0.9740\n",
      "Epoch 53/60\n",
      "69/69 [==============================] - 56s 818ms/step - loss: 0.0450 - acc: 0.9836\n",
      "Epoch 54/60\n",
      "69/69 [==============================] - 56s 809ms/step - loss: 0.0443 - acc: 0.9835\n",
      "Epoch 55/60\n",
      "69/69 [==============================] - 57s 819ms/step - loss: 0.0348 - acc: 0.9871\n",
      "Epoch 56/60\n",
      "69/69 [==============================] - 56s 813ms/step - loss: 0.0351 - acc: 0.9869\n",
      "Epoch 57/60\n",
      "69/69 [==============================] - 58s 842ms/step - loss: 0.0303 - acc: 0.9880\n",
      "Epoch 58/60\n",
      "69/69 [==============================] - 57s 827ms/step - loss: 0.3253 - acc: 0.8700\n",
      "Epoch 59/60\n",
      "69/69 [==============================] - 58s 845ms/step - loss: 0.1417 - acc: 0.9464\n",
      "Epoch 60/60\n",
      "69/69 [==============================] - 58s 840ms/step - loss: 0.0627 - acc: 0.9788\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.993574\n",
      "ACC: 0.960215\n",
      "MCC : 0.922019\n",
      "TPR:0.990283\n",
      "FPR:0.070588\n",
      "Pre:0.934946\n",
      "F1:0.961819\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.994124\n",
      "mean ACC: 0.969339\n",
      "mean MCC : 0.939802\n",
      "mean TPR:0.993936\n",
      "mean FPR:0.055146\n",
      "mean Pre:0.947239\n",
      "mean F1:0.970004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "dataset_name = 'yeast'\n",
    "for rep in range(1):\n",
    "    n_splits = 5\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "     \n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'yeast_data/new_train_valid-'+str(split)\n",
    "        test_pairs_file = 'yeast_data/new_test-'+str(split)\n",
    "#         valid_pairs_file = 'yeast_data/new_valid'+str(split)\n",
    "\n",
    "        batch_size = 128\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "#         valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'yeast_data/fusion_sent_GoplusSeq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        #  max_queue_size=16, workers=8, use_multiprocessing=True,\n",
    "        # validation_data=valid_generator,callbacks=[earlyStopping, save_checkpoint] \n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    epochs = 60,verbose=1)\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "#         model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('fusion_sent_seq_and_go__incep_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del test_x\n",
    "# del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to load a weight file containing 74 layers into a model with 10 layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ed997777bbc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtest_ppi_pairs\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ppi_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1217\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1218\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m   1169\u001b[0m                          \u001b[0;34m'containing '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m                          \u001b[0;34m' layers into a model with '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m                          str(len(filtered_layers)) + ' layers.')\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;31m# We batch weight value assignments in a single backend call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to load a weight file containing 74 layers into a model with 10 layers."
     ]
    }
   ],
   "source": [
    "model.load_weights(save_model_name)\n",
    "with open(test_pairs_file, 'r') as f:\n",
    "    test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "test_len = len(test_ppi_pairs) \n",
    "list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "y_pred_prob = model.predict(test_x)\n",
    "\n",
    "\n",
    "y_pred = (y_pred_prob > 0.5)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "f1 = f1_score(y_test, y_pred)\n",
    "pre = precision_score(y_test, y_pred)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "pr_auc = metrics.auc(recall, precision)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "total=tn+fp+fn+tp\n",
    "sen = float(tp)/float(tp+fn)\n",
    "sps = float(tn)/float((tn+fp))\n",
    "\n",
    "tpr = float(tp)/float(tp+fn)\n",
    "fpr = float(fp)/float((tn+fp))\n",
    "print('--------------------------\\n')\n",
    "print ('AUC: %f' % auc)\n",
    "print ('ACC: %f' % acc) \n",
    "# print(\"PRAUC: %f\" % pr_auc)\n",
    "print ('MCC : %f' % mcc)\n",
    "# print ('SEN: %f' % sen)\n",
    "# print ('SEP: %f' % sps)\n",
    "print('TPR:%f'%tpr)\n",
    "print('FPR:%f'%fpr)\n",
    "print('Pre:%f'%pre)\n",
    "print('F1:%f'%f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
