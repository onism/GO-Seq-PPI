{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 128\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.protein2go =  load_dict('yeast_data/protein2go_dicts.pkl')\n",
    "         \n",
    "        self.read_ppi()\n",
    "        self.prot2emb = {}\n",
    "        self.prot2embedding()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "    def prot2embedding(self):\n",
    "        for key, value in self.protein2go.items():\n",
    "            X_go1 =  np.zeros((1,768))\n",
    "            allgos = value.split(';') \n",
    "            allgos = list(set(allgos))\n",
    "            count = 0\n",
    "            for  go in  allgos:\n",
    "                if go.startswith('GO'):\n",
    "                    feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "                    if count + feature.shape[0] > max_go_len:\n",
    "                        break\n",
    "                    X_go1 = np.concatenate((X_go1,feature ))    \n",
    "                    count += feature.shape[0]\n",
    "            self.prot2emb[key] =  X_go1[1:]   \n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        y = np.empty((self.batch_size))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "#             X_go1[i] =  np.load('SC_GO/'+p1+'.npy') \n",
    "#             X_go2[i] =  np.load('SC_GO/'+p2+'.npy')\n",
    "#             values = self.protein2go[ p1] \n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go1[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "\n",
    "#             values = self.protein2go[ p2]\n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos)) \n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go2[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "        return [X_go1,X_go2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "         \n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "#             X_go1[i] =  np.load('SC_GO/'+p1+'.npy') \n",
    "#             X_go2[i] =  np.load('SC_GO/'+p2+'.npy')\n",
    "#             values = self.protein2go[ p1 ]\n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go1[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "\n",
    "#             values = self.protein2go[ p2]\n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos)) \n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go2[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "        return [X_go1,X_go2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 32)      73760       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 128, 32)      24608       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 128, 32)      73760       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 128, 32)      24608       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 32)      5152        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 128, 32)      3104        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 128, 32)      73760       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 128, 32)      24608       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 128, 32)      5152        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 128, 32)      3104        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 128, 32)      73760       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 128, 32)      24608       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 128)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 128)     320256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 128)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 128)     320256      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 128)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 128)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 512)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          131328      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          131328      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512)          0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         525312      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         1049600     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          524800      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            513         dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,413,377\n",
      "Trainable params: 3,413,377\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(2)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def create_share_model():\n",
    "    con_filters = 128\n",
    "    X_input = Input(shape=(max_go_len,feature_len))\n",
    "    x = inception_block(X_input, con_filters)\n",
    "    flat = Flatten()(x)\n",
    "    model = Model(X_input, flat)\n",
    "    return model\n",
    "\n",
    " \n",
    "def build_siamese_model():\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "    siamese_a = create_share_model()\n",
    "    siamese_b = create_share_model()\n",
    "     \n",
    "    encoded_l = siamese_a(left_input_go)\n",
    "    encoded_r = siamese_b(right_input_go)\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "\n",
    "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "\n",
    "    siamese_net = Model(inputs=[left_input_go,right_input_go],outputs=prediction)\n",
    "    \n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return siamese_net\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "    x = inception_block(left_input_go,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(64, return_sequences=True))(left_input_go)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    \n",
    "    left_x_go = Concatenate()([x_a  , x_b, x_gru_a, x_gru_b])\n",
    "    \n",
    "    \n",
    "    left_x_go = Dense(256, activation='relu')(left_x_go)\n",
    "     \n",
    " \n",
    "    x = inception_block(right_input_go,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru =Bidirectional(CuDNNGRU(64, return_sequences=True))(right_input_go)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    \n",
    "    right_x_go = Concatenate()([x_a  , x_b, x_gru_a, x_gru_b])\n",
    "    right_x_go = Dense(256, activation='relu')(right_x_go)\n",
    "    \n",
    "   \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go], output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 20s 727ms/step - loss: 0.7534 - acc: 0.6072 - val_loss: 0.6371 - val_acc: 0.6191\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 13s 473ms/step - loss: 0.5916 - acc: 0.6678 - val_loss: 0.5615 - val_acc: 0.7129\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 12s 462ms/step - loss: 0.5300 - acc: 0.7174 - val_loss: 0.5204 - val_acc: 0.7396\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 13s 484ms/step - loss: 0.4860 - acc: 0.7551 - val_loss: 0.5024 - val_acc: 0.7461\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 14s 537ms/step - loss: 0.4439 - acc: 0.7833 - val_loss: 0.5090 - val_acc: 0.7376\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 16s 578ms/step - loss: 0.3909 - acc: 0.8171 - val_loss: 0.4124 - val_acc: 0.8216\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 16s 592ms/step - loss: 0.3473 - acc: 0.8442 - val_loss: 0.3679 - val_acc: 0.8464\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 15s 556ms/step - loss: 0.3361 - acc: 0.8517 - val_loss: 0.3536 - val_acc: 0.8438\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 16s 592ms/step - loss: 0.2787 - acc: 0.8792 - val_loss: 0.3369 - val_acc: 0.8555\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 15s 569ms/step - loss: 0.2850 - acc: 0.8763 - val_loss: 0.3192 - val_acc: 0.8633\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 16s 581ms/step - loss: 0.2485 - acc: 0.8919 - val_loss: 0.4843 - val_acc: 0.8014\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 15s 568ms/step - loss: 0.2316 - acc: 0.9041 - val_loss: 0.3264 - val_acc: 0.8555\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 15s 560ms/step - loss: 0.1936 - acc: 0.9151 - val_loss: 0.3105 - val_acc: 0.8750\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 16s 578ms/step - loss: 0.1997 - acc: 0.9190 - val_loss: 0.3180 - val_acc: 0.8633\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 16s 578ms/step - loss: 0.1727 - acc: 0.9298 - val_loss: 0.3176 - val_acc: 0.8672\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 15s 563ms/step - loss: 0.1630 - acc: 0.9321 - val_loss: 0.3112 - val_acc: 0.8789\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 15s 570ms/step - loss: 0.1758 - acc: 0.9280 - val_loss: 0.3028 - val_acc: 0.8828\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 15s 549ms/step - loss: 0.1519 - acc: 0.9382 - val_loss: 0.3065 - val_acc: 0.8835\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 15s 558ms/step - loss: 0.1301 - acc: 0.9469 - val_loss: 0.3104 - val_acc: 0.8835\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 16s 576ms/step - loss: 0.1259 - acc: 0.9499 - val_loss: 0.3473 - val_acc: 0.8763\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 15s 561ms/step - loss: 0.1261 - acc: 0.9501 - val_loss: 0.2776 - val_acc: 0.8893\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 15s 562ms/step - loss: 0.1073 - acc: 0.9553 - val_loss: 0.3595 - val_acc: 0.8730\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 15s 562ms/step - loss: 0.1249 - acc: 0.9484 - val_loss: 0.2939 - val_acc: 0.8867\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 16s 578ms/step - loss: 0.1210 - acc: 0.9538 - val_loss: 0.3350 - val_acc: 0.8874\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 15s 563ms/step - loss: 0.0982 - acc: 0.9601 - val_loss: 0.3837 - val_acc: 0.8750\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 15s 557ms/step - loss: 0.1034 - acc: 0.9620 - val_loss: 0.3434 - val_acc: 0.8789\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 14s 532ms/step - loss: 0.0937 - acc: 0.9625 - val_loss: 0.3091 - val_acc: 0.8919\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 15s 573ms/step - loss: 0.0891 - acc: 0.9640 - val_loss: 0.3233 - val_acc: 0.8919\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 15s 540ms/step - loss: 0.0709 - acc: 0.9728 - val_loss: 0.3923 - val_acc: 0.8783\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 16s 580ms/step - loss: 0.0831 - acc: 0.9685 - val_loss: 0.3858 - val_acc: 0.8867\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 15s 549ms/step - loss: 0.0680 - acc: 0.9741 - val_loss: 0.3462 - val_acc: 0.8965\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.986051\n",
      "ACC: 0.944593\n",
      "MCC : 0.890006\n",
      "TPR:0.966041\n",
      "FPR:0.076854\n",
      "Pre:0.926307\n",
      "F1:0.945757\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "27/27 [==============================] - 20s 743ms/step - loss: 0.7703 - acc: 0.5904 - val_loss: 0.6569 - val_acc: 0.6491\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 15s 541ms/step - loss: 0.6130 - acc: 0.6529 - val_loss: 0.6090 - val_acc: 0.6549\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 15s 545ms/step - loss: 0.5316 - acc: 0.7209 - val_loss: 0.5097 - val_acc: 0.7363\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 15s 559ms/step - loss: 0.4936 - acc: 0.7595 - val_loss: 0.4660 - val_acc: 0.7721\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 15s 568ms/step - loss: 0.4239 - acc: 0.8025 - val_loss: 0.4345 - val_acc: 0.7845\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 15s 545ms/step - loss: 0.4032 - acc: 0.8108 - val_loss: 0.4156 - val_acc: 0.8066\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 15s 542ms/step - loss: 0.3365 - acc: 0.8526 - val_loss: 0.5005 - val_acc: 0.7611\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 14s 523ms/step - loss: 0.3299 - acc: 0.8484 - val_loss: 0.3728 - val_acc: 0.8346\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 15s 538ms/step - loss: 0.2926 - acc: 0.8704 - val_loss: 0.3713 - val_acc: 0.8307\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 15s 560ms/step - loss: 0.2636 - acc: 0.8845 - val_loss: 0.3463 - val_acc: 0.8464\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 15s 554ms/step - loss: 0.2924 - acc: 0.8681 - val_loss: 0.3870 - val_acc: 0.8327\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 15s 555ms/step - loss: 0.2332 - acc: 0.8993 - val_loss: 0.3193 - val_acc: 0.8711\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 15s 567ms/step - loss: 0.2299 - acc: 0.9021 - val_loss: 0.3686 - val_acc: 0.8451\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 15s 541ms/step - loss: 0.1866 - acc: 0.9210 - val_loss: 0.3170 - val_acc: 0.8770\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 15s 565ms/step - loss: 0.1721 - acc: 0.9285 - val_loss: 0.3032 - val_acc: 0.8776\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 14s 533ms/step - loss: 0.1774 - acc: 0.9256 - val_loss: 0.3164 - val_acc: 0.8822\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 15s 550ms/step - loss: 0.1502 - acc: 0.9379 - val_loss: 0.2941 - val_acc: 0.8874\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 15s 550ms/step - loss: 0.1463 - acc: 0.9355 - val_loss: 0.3181 - val_acc: 0.8861\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 15s 558ms/step - loss: 0.1438 - acc: 0.9405 - val_loss: 0.3948 - val_acc: 0.8574\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 15s 548ms/step - loss: 0.1393 - acc: 0.9416 - val_loss: 0.3196 - val_acc: 0.8848\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 14s 535ms/step - loss: 0.1137 - acc: 0.9520 - val_loss: 0.3297 - val_acc: 0.8861\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 15s 540ms/step - loss: 0.1058 - acc: 0.9553 - val_loss: 0.4053 - val_acc: 0.8542\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 15s 552ms/step - loss: 0.1285 - acc: 0.9470 - val_loss: 0.4053 - val_acc: 0.8581\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 15s 540ms/step - loss: 0.1101 - acc: 0.9550 - val_loss: 0.3875 - val_acc: 0.8809\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 15s 555ms/step - loss: 0.1053 - acc: 0.9579 - val_loss: 0.3440 - val_acc: 0.8828\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 15s 557ms/step - loss: 0.0972 - acc: 0.9599 - val_loss: 0.3265 - val_acc: 0.8919\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 15s 545ms/step - loss: 0.0942 - acc: 0.9605 - val_loss: 0.3870 - val_acc: 0.8848\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.977862\n",
      "ACC: 0.929848\n",
      "MCC : 0.860332\n",
      "TPR:0.949062\n",
      "FPR:0.089366\n",
      "Pre:0.913941\n",
      "F1:0.931171\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 20s 744ms/step - loss: 0.7576 - acc: 0.6036 - val_loss: 0.6249 - val_acc: 0.6452\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 15s 552ms/step - loss: 0.5751 - acc: 0.6832 - val_loss: 0.5561 - val_acc: 0.7025\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 15s 557ms/step - loss: 0.5174 - acc: 0.7338 - val_loss: 0.5259 - val_acc: 0.7279\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 16s 580ms/step - loss: 0.4707 - acc: 0.7698 - val_loss: 0.5522 - val_acc: 0.7109\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 15s 555ms/step - loss: 0.4353 - acc: 0.7928 - val_loss: 0.4684 - val_acc: 0.7799\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 15s 566ms/step - loss: 0.3768 - acc: 0.8268 - val_loss: 0.4506 - val_acc: 0.7806\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 14s 522ms/step - loss: 0.3350 - acc: 0.8487 - val_loss: 0.4455 - val_acc: 0.8027\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 15s 557ms/step - loss: 0.3114 - acc: 0.8627 - val_loss: 0.3858 - val_acc: 0.8294\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 15s 550ms/step - loss: 0.2715 - acc: 0.8850 - val_loss: 0.3620 - val_acc: 0.8464\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 15s 562ms/step - loss: 0.2767 - acc: 0.8845 - val_loss: 0.3418 - val_acc: 0.8652\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 14s 532ms/step - loss: 0.2334 - acc: 0.9041 - val_loss: 0.3563 - val_acc: 0.8483\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 15s 572ms/step - loss: 0.2001 - acc: 0.9162 - val_loss: 0.3106 - val_acc: 0.8717\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 14s 532ms/step - loss: 0.1949 - acc: 0.9190 - val_loss: 0.3599 - val_acc: 0.8522\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 15s 567ms/step - loss: 0.2003 - acc: 0.9142 - val_loss: 0.3160 - val_acc: 0.8685\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 15s 570ms/step - loss: 0.1754 - acc: 0.9277 - val_loss: 0.3310 - val_acc: 0.8470\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 15s 562ms/step - loss: 0.1671 - acc: 0.9329 - val_loss: 0.3047 - val_acc: 0.8874\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 15s 554ms/step - loss: 0.1382 - acc: 0.9431 - val_loss: 0.3815 - val_acc: 0.8633\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 15s 544ms/step - loss: 0.1388 - acc: 0.9450 - val_loss: 0.2756 - val_acc: 0.8848\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 15s 562ms/step - loss: 0.1498 - acc: 0.9358 - val_loss: 0.3075 - val_acc: 0.8945\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 14s 534ms/step - loss: 0.1378 - acc: 0.9426 - val_loss: 0.3270 - val_acc: 0.8802\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 15s 546ms/step - loss: 0.1192 - acc: 0.9488 - val_loss: 0.3288 - val_acc: 0.8783\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 16s 575ms/step - loss: 0.1173 - acc: 0.9541 - val_loss: 0.2818 - val_acc: 0.8926\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 15s 551ms/step - loss: 0.1073 - acc: 0.9572 - val_loss: 0.3147 - val_acc: 0.8900\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 15s 545ms/step - loss: 0.0937 - acc: 0.9602 - val_loss: 0.2993 - val_acc: 0.9004\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 16s 574ms/step - loss: 0.0935 - acc: 0.9631 - val_loss: 0.3789 - val_acc: 0.8704\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 14s 526ms/step - loss: 0.1082 - acc: 0.9586 - val_loss: 0.2971 - val_acc: 0.8776\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 15s 540ms/step - loss: 0.1063 - acc: 0.9572 - val_loss: 0.3239 - val_acc: 0.8822\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 15s 570ms/step - loss: 0.0773 - acc: 0.9693 - val_loss: 0.3299 - val_acc: 0.8978\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.975632\n",
      "ACC: 0.915996\n",
      "MCC : 0.835072\n",
      "TPR:0.958892\n",
      "FPR:0.126899\n",
      "Pre:0.883128\n",
      "F1:0.919452\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "27/27 [==============================] - 21s 761ms/step - loss: 0.7476 - acc: 0.5971 - val_loss: 0.6017 - val_acc: 0.6693\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 14s 525ms/step - loss: 0.5877 - acc: 0.6748 - val_loss: 0.5451 - val_acc: 0.7038\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 14s 502ms/step - loss: 0.5119 - acc: 0.7374 - val_loss: 0.4911 - val_acc: 0.7533\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 12s 445ms/step - loss: 0.4604 - acc: 0.7786 - val_loss: 0.5294 - val_acc: 0.7109\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 12s 457ms/step - loss: 0.4215 - acc: 0.8037 - val_loss: 0.4094 - val_acc: 0.8066\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 12s 444ms/step - loss: 0.3616 - acc: 0.8404 - val_loss: 0.3901 - val_acc: 0.8229\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 12s 439ms/step - loss: 0.3847 - acc: 0.8267 - val_loss: 0.3987 - val_acc: 0.8249\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 12s 452ms/step - loss: 0.3053 - acc: 0.8717 - val_loss: 0.3501 - val_acc: 0.8483\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 12s 444ms/step - loss: 0.3054 - acc: 0.8685 - val_loss: 0.3513 - val_acc: 0.8464\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 12s 433ms/step - loss: 0.2700 - acc: 0.8801 - val_loss: 0.3366 - val_acc: 0.8516\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 12s 436ms/step - loss: 0.2411 - acc: 0.8990 - val_loss: 0.3394 - val_acc: 0.8457\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 12s 440ms/step - loss: 0.2243 - acc: 0.9025 - val_loss: 0.3347 - val_acc: 0.8594\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 12s 441ms/step - loss: 0.1952 - acc: 0.9204 - val_loss: 0.3232 - val_acc: 0.8652\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 12s 440ms/step - loss: 0.2006 - acc: 0.9141 - val_loss: 0.3287 - val_acc: 0.8607\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 12s 438ms/step - loss: 0.1822 - acc: 0.9238 - val_loss: 0.3211 - val_acc: 0.8717\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 12s 448ms/step - loss: 0.1672 - acc: 0.9316 - val_loss: 0.3188 - val_acc: 0.8698\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 12s 434ms/step - loss: 0.1398 - acc: 0.9382 - val_loss: 0.3030 - val_acc: 0.8809\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 0.1334 - acc: 0.9411 - val_loss: 0.2953 - val_acc: 0.8958\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 12s 446ms/step - loss: 0.1320 - acc: 0.9453 - val_loss: 0.3160 - val_acc: 0.8737\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 0.1355 - acc: 0.9446 - val_loss: 0.3112 - val_acc: 0.8880\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 12s 438ms/step - loss: 0.1503 - acc: 0.9365 - val_loss: 0.2862 - val_acc: 0.8893\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 12s 437ms/step - loss: 0.1075 - acc: 0.9592 - val_loss: 0.3434 - val_acc: 0.8743\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 12s 440ms/step - loss: 0.1174 - acc: 0.9507 - val_loss: 0.3269 - val_acc: 0.8874\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 12s 443ms/step - loss: 0.0874 - acc: 0.9664 - val_loss: 0.3121 - val_acc: 0.8952\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 12s 451ms/step - loss: 0.0864 - acc: 0.9659 - val_loss: 0.3241 - val_acc: 0.8906\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 13s 466ms/step - loss: 0.1230 - acc: 0.9498 - val_loss: 0.2911 - val_acc: 0.8952\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 12s 445ms/step - loss: 0.0809 - acc: 0.9673 - val_loss: 0.3055 - val_acc: 0.8945\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 12s 444ms/step - loss: 0.0697 - acc: 0.9725 - val_loss: 0.3243 - val_acc: 0.8952\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 12s 457ms/step - loss: 0.0688 - acc: 0.9732 - val_loss: 0.3945 - val_acc: 0.8952\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 13s 482ms/step - loss: 0.0753 - acc: 0.9727 - val_loss: 0.3612 - val_acc: 0.9049\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 14s 503ms/step - loss: 0.0684 - acc: 0.9742 - val_loss: 0.3549 - val_acc: 0.8828\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.976796\n",
      "ACC: 0.931605\n",
      "MCC : 0.864766\n",
      "TPR:0.961538\n",
      "FPR:0.098302\n",
      "Pre:0.907173\n",
      "F1:0.933565\n",
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 19s 711ms/step - loss: 0.7386 - acc: 0.6043 - val_loss: 0.6234 - val_acc: 0.6341\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 13s 487ms/step - loss: 0.5947 - acc: 0.6727 - val_loss: 0.5998 - val_acc: 0.7012\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 14s 503ms/step - loss: 0.5337 - acc: 0.7253 - val_loss: 0.5116 - val_acc: 0.7311\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 14s 502ms/step - loss: 0.4761 - acc: 0.7688 - val_loss: 0.4653 - val_acc: 0.7708\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 13s 486ms/step - loss: 0.4307 - acc: 0.7976 - val_loss: 0.4673 - val_acc: 0.7676\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 13s 492ms/step - loss: 0.3775 - acc: 0.8270 - val_loss: 0.4113 - val_acc: 0.8236\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 13s 492ms/step - loss: 0.3761 - acc: 0.8241 - val_loss: 0.4595 - val_acc: 0.7767\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 13s 492ms/step - loss: 0.3192 - acc: 0.8602 - val_loss: 0.4022 - val_acc: 0.8125\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 13s 499ms/step - loss: 0.2842 - acc: 0.8759 - val_loss: 0.3378 - val_acc: 0.8607\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 13s 495ms/step - loss: 0.2533 - acc: 0.8899 - val_loss: 0.3404 - val_acc: 0.8522\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 13s 487ms/step - loss: 0.2228 - acc: 0.9054 - val_loss: 0.3212 - val_acc: 0.8594\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 13s 491ms/step - loss: 0.2204 - acc: 0.9058 - val_loss: 0.3213 - val_acc: 0.8568\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 13s 492ms/step - loss: 0.2086 - acc: 0.9123 - val_loss: 0.3089 - val_acc: 0.8704\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 14s 505ms/step - loss: 0.2009 - acc: 0.9136 - val_loss: 0.3368 - val_acc: 0.8568\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 14s 526ms/step - loss: 0.2083 - acc: 0.9106 - val_loss: 0.2660 - val_acc: 0.8906\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 13s 499ms/step - loss: 0.1597 - acc: 0.9358 - val_loss: 0.2827 - val_acc: 0.8835\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 13s 488ms/step - loss: 0.1743 - acc: 0.9300 - val_loss: 0.2824 - val_acc: 0.8848\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 13s 499ms/step - loss: 0.1421 - acc: 0.9434 - val_loss: 0.3177 - val_acc: 0.8646\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 14s 514ms/step - loss: 0.1201 - acc: 0.9538 - val_loss: 0.3079 - val_acc: 0.8854\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 13s 491ms/step - loss: 0.1444 - acc: 0.9411 - val_loss: 0.3301 - val_acc: 0.8587\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 13s 498ms/step - loss: 0.1340 - acc: 0.9478 - val_loss: 0.2771 - val_acc: 0.8900\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 13s 498ms/step - loss: 0.1025 - acc: 0.9591 - val_loss: 0.2928 - val_acc: 0.8893\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 14s 507ms/step - loss: 0.0939 - acc: 0.9622 - val_loss: 0.3062 - val_acc: 0.8913\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 14s 517ms/step - loss: 0.0909 - acc: 0.9641 - val_loss: 0.3031 - val_acc: 0.9023\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 13s 495ms/step - loss: 0.1061 - acc: 0.9576 - val_loss: 0.2844 - val_acc: 0.8919\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.976315\n",
      "ACC: 0.926655\n",
      "MCC : 0.854382\n",
      "TPR:0.951699\n",
      "FPR:0.098390\n",
      "Pre:0.906303\n",
      "F1:0.928447\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.978531\n",
      "mean ACC: 0.929739\n",
      "mean MCC : 0.860911\n",
      "mean TPR:0.957447\n",
      "mean FPR:0.097962\n",
      "mean Pre:0.907370\n",
      "mean F1:0.931678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'SC'\n",
    "for rep in range(1):\n",
    "    n_splits = 5\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'yeast_data/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'yeast_data/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'yeast_data/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 256\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'yeast_data/sc_go'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=valid_generator, \n",
    "                    epochs = 100,verbose=1,callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('yeast_go_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
