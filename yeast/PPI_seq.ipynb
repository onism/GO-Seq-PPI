{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 128\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        \n",
    "        self.protein2seq = load_dict('yeast_data/protein2seq_dicts.pkl')\n",
    "        self.read_ppi()\n",
    "         \n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "            \n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "#             self.protein2onehot[key]  = seq2t.embed_normalized(value,self.max_seqlen )\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    " \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        \n",
    "        y = np.empty((self.batch_size))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "            \n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "     \n",
    "        return [  X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        \n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "             \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "  \n",
    "        return [X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 16)     1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 16)     784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 16)     1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 16)     784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1000, 64)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 1000, 64)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 64)     0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1000, 64)     0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           1064        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           1064        dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 576)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 576)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          147712      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          147712      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512)          0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1024)         525312      concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 1024)         0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1024)         1049600     dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 1024)         0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 512)          524800      dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 512)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            513         dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1)            0           dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 2,475,489\n",
      "Trainable params: 2,475,489\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(2)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([x_a, x_b, x_c, x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_cnn_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    \n",
    "    x = Concatenate()([ x_a, x_b, x_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "     \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "\n",
    "      \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters//4, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters//4, gru_units)\n",
    "     \n",
    "   \n",
    "    x =   Concatenate()([ left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([ left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model)\n",
    "#     rms = RMSprop(lr=0.0001)\n",
    "#     adam = Adam(lr=0.001, amsgrad=True, epsilon=1e-5)\n",
    "    \n",
    "    optimizer = Lookahead(RAdam())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 14s 401ms/step - loss: 0.6662 - acc: 0.6202\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 8s 241ms/step - loss: 0.6613 - acc: 0.6253\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.6619 - acc: 0.6253\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6604 - acc: 0.6253\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.6617 - acc: 0.6253\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.6603 - acc: 0.6253\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6590 - acc: 0.6253\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6607 - acc: 0.6255\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.6593 - acc: 0.6253\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6573 - acc: 0.6253\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6533 - acc: 0.6267\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 8s 241ms/step - loss: 0.6563 - acc: 0.6232\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.6469 - acc: 0.6340\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.6453 - acc: 0.6405\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.6306 - acc: 0.6548\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.6231 - acc: 0.6614\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6100 - acc: 0.6756\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.5943 - acc: 0.6924\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.5866 - acc: 0.6926\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.5666 - acc: 0.7136\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.5688 - acc: 0.7085\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.5582 - acc: 0.7113\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.5489 - acc: 0.7277\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.5265 - acc: 0.7415\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.5214 - acc: 0.7461\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.4924 - acc: 0.7672\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.5097 - acc: 0.7562\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.4825 - acc: 0.7739\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 8s 241ms/step - loss: 0.4853 - acc: 0.7713\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.4637 - acc: 0.7838\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.4651 - acc: 0.7846\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 8s 242ms/step - loss: 0.4590 - acc: 0.7861\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.4478 - acc: 0.7908\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.4649 - acc: 0.7791\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.4328 - acc: 0.7973\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3956 - acc: 0.8243\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.4131 - acc: 0.8148\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.4244 - acc: 0.8024\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.4198 - acc: 0.8007\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3877 - acc: 0.8234\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.3872 - acc: 0.8254\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 8s 242ms/step - loss: 0.3756 - acc: 0.8263\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.3821 - acc: 0.8249\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3538 - acc: 0.8424\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.3687 - acc: 0.8400\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3656 - acc: 0.8373\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3725 - acc: 0.8366\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3607 - acc: 0.8415\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3368 - acc: 0.8512\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3124 - acc: 0.8616\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3195 - acc: 0.8587\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3682 - acc: 0.8315\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.3392 - acc: 0.8511\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 8s 242ms/step - loss: 0.3007 - acc: 0.8695\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.2969 - acc: 0.8717\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3125 - acc: 0.8631\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.3150 - acc: 0.8647\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2860 - acc: 0.8765\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2788 - acc: 0.8820\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2987 - acc: 0.8701\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.2448 - acc: 0.8973\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2469 - acc: 0.8977\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2730 - acc: 0.8832\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2499 - acc: 0.8959\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3279 - acc: 0.8541\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2549 - acc: 0.8959\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2716 - acc: 0.8824\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2271 - acc: 0.9052\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2435 - acc: 0.8955\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.2191 - acc: 0.9090\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 8s 242ms/step - loss: 0.2870 - acc: 0.8761\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2168 - acc: 0.9099\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2050 - acc: 0.9146\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2037 - acc: 0.9187\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2039 - acc: 0.9184\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2117 - acc: 0.9119\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2404 - acc: 0.9006\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2121 - acc: 0.9144\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2660 - acc: 0.8849\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2159 - acc: 0.9121\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2038 - acc: 0.9188\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1817 - acc: 0.9290\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.1658 - acc: 0.9339\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.1851 - acc: 0.9264\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 8s 243ms/step - loss: 0.1776 - acc: 0.9273\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.1877 - acc: 0.9276\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 8s 242ms/step - loss: 0.1684 - acc: 0.9323\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1661 - acc: 0.9332\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.1929 - acc: 0.9220\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.1610 - acc: 0.9361\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1793 - acc: 0.9260\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1448 - acc: 0.9427\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1763 - acc: 0.9293\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2255 - acc: 0.9084\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2629 - acc: 0.8927\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.1993 - acc: 0.9220\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.1676 - acc: 0.9344\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.1567 - acc: 0.9366\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.1342 - acc: 0.9478\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.1743 - acc: 0.9298\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.981717\n",
      "ACC: 0.939678\n",
      "MCC : 0.880639\n",
      "TPR:0.976391\n",
      "FPR:0.101711\n",
      "Pre:0.915415\n",
      "F1:0.944920\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 15s 431ms/step - loss: 0.6717 - acc: 0.5784\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.6621 - acc: 0.6235\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.6626 - acc: 0.6235\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.6642 - acc: 0.6235\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.6626 - acc: 0.6235\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6610 - acc: 0.6235\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6611 - acc: 0.6235\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.6600 - acc: 0.6235\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.6599 - acc: 0.6235\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.6573 - acc: 0.6247\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.6575 - acc: 0.6235\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.6536 - acc: 0.6280\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.6481 - acc: 0.6397\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.6438 - acc: 0.6434\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.6384 - acc: 0.6560\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.6267 - acc: 0.6618\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.6287 - acc: 0.6600\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.6041 - acc: 0.6842\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.5990 - acc: 0.6860\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.5945 - acc: 0.6934\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.5796 - acc: 0.7074\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.5622 - acc: 0.7209\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.5669 - acc: 0.7088\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.5430 - acc: 0.7307\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.5531 - acc: 0.7241\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.5265 - acc: 0.7466\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.5194 - acc: 0.7491\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.5407 - acc: 0.7317\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.4951 - acc: 0.7663\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.4903 - acc: 0.7695\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.4670 - acc: 0.7850\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4970 - acc: 0.7641\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4754 - acc: 0.7804\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4800 - acc: 0.7719\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4564 - acc: 0.7912\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4561 - acc: 0.7883\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.4868 - acc: 0.7690\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4286 - acc: 0.8051\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4438 - acc: 0.7932\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.4448 - acc: 0.7972\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4105 - acc: 0.8196\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4340 - acc: 0.8053\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.4032 - acc: 0.8225\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.3936 - acc: 0.8273\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3899 - acc: 0.8264\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.3804 - acc: 0.8341\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.3866 - acc: 0.8307\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.3651 - acc: 0.8410\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3769 - acc: 0.8355\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.3830 - acc: 0.8269\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.3446 - acc: 0.8506\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.3496 - acc: 0.8520\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.3253 - acc: 0.8598\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.3313 - acc: 0.8587\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.4003 - acc: 0.8277\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3240 - acc: 0.8612\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 8s 242ms/step - loss: 0.3512 - acc: 0.8460\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3152 - acc: 0.8626\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3076 - acc: 0.8666\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3056 - acc: 0.8668\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3075 - acc: 0.8679\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.3750 - acc: 0.8342\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2923 - acc: 0.8779\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3652 - acc: 0.8366\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3053 - acc: 0.8674\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.2824 - acc: 0.8834\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2951 - acc: 0.8711\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2645 - acc: 0.8857\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2878 - acc: 0.8783\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3080 - acc: 0.8655\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2733 - acc: 0.8851\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2521 - acc: 0.8988\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2454 - acc: 0.8965\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.2648 - acc: 0.8880\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2524 - acc: 0.8937\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2572 - acc: 0.8940\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2627 - acc: 0.8867\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2295 - acc: 0.9037\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2520 - acc: 0.8956\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2389 - acc: 0.8983\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3202 - acc: 0.8551\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2450 - acc: 0.8989\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 8s 243ms/step - loss: 0.2338 - acc: 0.9038\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.1993 - acc: 0.9192\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.3043 - acc: 0.8678\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2377 - acc: 0.8968\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2399 - acc: 0.8999\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2367 - acc: 0.9002\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2223 - acc: 0.9068\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2250 - acc: 0.9054\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.1964 - acc: 0.9199\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2263 - acc: 0.9058\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2568 - acc: 0.8861\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.1970 - acc: 0.9174\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.1901 - acc: 0.9244\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2096 - acc: 0.9127\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1734 - acc: 0.9313\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2173 - acc: 0.9092\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.1965 - acc: 0.9164\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1751 - acc: 0.9307\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.973174\n",
      "ACC: 0.941912\n",
      "MCC : 0.884608\n",
      "TPR:0.961255\n",
      "FPR:0.076256\n",
      "Pre:0.922124\n",
      "F1:0.941283\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 15s 448ms/step - loss: 0.6687 - acc: 0.6224\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.6627 - acc: 0.6234\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6621 - acc: 0.6234\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.6618 - acc: 0.6234\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.6618 - acc: 0.6234\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6611 - acc: 0.6234\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6605 - acc: 0.6235\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6584 - acc: 0.6232\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6590 - acc: 0.6239\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6579 - acc: 0.6242\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.6560 - acc: 0.6230\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.6468 - acc: 0.6336\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6424 - acc: 0.6438\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6464 - acc: 0.6402\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6297 - acc: 0.6612\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6080 - acc: 0.6777\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6100 - acc: 0.6762\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.5944 - acc: 0.6938\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.5909 - acc: 0.6960\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.5864 - acc: 0.7023\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.5659 - acc: 0.7167\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.5645 - acc: 0.7132\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.5549 - acc: 0.7232\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.5446 - acc: 0.7317\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.5249 - acc: 0.7454\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.5365 - acc: 0.7339\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.5378 - acc: 0.7358\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.5139 - acc: 0.7505\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.4938 - acc: 0.7696\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.5029 - acc: 0.7593\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4864 - acc: 0.7716\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.4732 - acc: 0.7787\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4573 - acc: 0.7930\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.4709 - acc: 0.7827\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4806 - acc: 0.7779\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.4798 - acc: 0.7684\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4439 - acc: 0.8033\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4522 - acc: 0.7883\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.4422 - acc: 0.7969\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.4109 - acc: 0.8151\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.4293 - acc: 0.8022\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4277 - acc: 0.8026\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3944 - acc: 0.8273\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.3882 - acc: 0.8277\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4130 - acc: 0.8161\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.3948 - acc: 0.8227\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.4313 - acc: 0.8004\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3905 - acc: 0.8253\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3817 - acc: 0.8304\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.4158 - acc: 0.8119\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 8s 247ms/step - loss: 0.4155 - acc: 0.8174\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.3606 - acc: 0.8458\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3543 - acc: 0.8469\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.3737 - acc: 0.8332\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.3794 - acc: 0.8286\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3262 - acc: 0.8617\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3313 - acc: 0.8570\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3281 - acc: 0.8559\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.3374 - acc: 0.8524\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3333 - acc: 0.8523\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3156 - acc: 0.8666\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.3021 - acc: 0.8728\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.2978 - acc: 0.8740\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2948 - acc: 0.8726\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.4651 - acc: 0.7875\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.4267 - acc: 0.7988\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.3453 - acc: 0.8495\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2987 - acc: 0.8720\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.3189 - acc: 0.8634\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2857 - acc: 0.8763\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.3141 - acc: 0.8636\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.2768 - acc: 0.8840\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2811 - acc: 0.8784\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2756 - acc: 0.8793\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2742 - acc: 0.8836\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2648 - acc: 0.8888\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2471 - acc: 0.8948\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2871 - acc: 0.8778\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.2577 - acc: 0.8925\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2581 - acc: 0.8925\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2483 - acc: 0.8996\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2557 - acc: 0.8930\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2158 - acc: 0.9110\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2235 - acc: 0.9077\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2864 - acc: 0.8802\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2215 - acc: 0.9106\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2221 - acc: 0.9115\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2156 - acc: 0.9081\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2406 - acc: 0.8999\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2448 - acc: 0.8969\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2026 - acc: 0.9184\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.1887 - acc: 0.9241\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.2110 - acc: 0.9108\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.1939 - acc: 0.9197\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2311 - acc: 0.9034\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2110 - acc: 0.9139\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.1847 - acc: 0.9267\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.1699 - acc: 0.9298\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.1978 - acc: 0.9166\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2194 - acc: 0.9090\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.959730\n",
      "ACC: 0.895843\n",
      "MCC : 0.794649\n",
      "TPR:0.934198\n",
      "FPR:0.139896\n",
      "Pre:0.861538\n",
      "F1:0.896398\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 17s 492ms/step - loss: 0.6689 - acc: 0.6012\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.6620 - acc: 0.6248\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6615 - acc: 0.6248\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6602 - acc: 0.6248\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6612 - acc: 0.6248\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.6589 - acc: 0.6248\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6594 - acc: 0.6247\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6626 - acc: 0.6240\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.6569 - acc: 0.6249\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.6545 - acc: 0.6263\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6472 - acc: 0.6360\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.6421 - acc: 0.6395\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.6342 - acc: 0.6528\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6227 - acc: 0.6605\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.6048 - acc: 0.6805\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.5956 - acc: 0.6850\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.5806 - acc: 0.6978\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 8s 246ms/step - loss: 0.5861 - acc: 0.6918\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.5771 - acc: 0.7061\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.5678 - acc: 0.7098\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.5667 - acc: 0.7129\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.5348 - acc: 0.7406\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.5312 - acc: 0.7369\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.5423 - acc: 0.7299\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.5116 - acc: 0.7585\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.5125 - acc: 0.7521\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.5222 - acc: 0.7474\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.5132 - acc: 0.7502\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.5188 - acc: 0.7483\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4874 - acc: 0.7703\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.4698 - acc: 0.7848\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.4873 - acc: 0.7696\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4594 - acc: 0.7846\n",
      "Epoch 34/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4370 - acc: 0.7981\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.4508 - acc: 0.7869\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.4603 - acc: 0.7838\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.4695 - acc: 0.7807\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.4241 - acc: 0.8097\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.5045 - acc: 0.7502\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4287 - acc: 0.8030\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.3930 - acc: 0.8238\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.4079 - acc: 0.8228\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.3939 - acc: 0.8253\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.3962 - acc: 0.8207\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.3878 - acc: 0.8239\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4025 - acc: 0.8171\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.4307 - acc: 0.7984\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.3988 - acc: 0.8205\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3830 - acc: 0.8272\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.3746 - acc: 0.8328\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.3479 - acc: 0.8469\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.3625 - acc: 0.8409\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.3688 - acc: 0.8319\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3338 - acc: 0.8557\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3408 - acc: 0.8525\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.3620 - acc: 0.8359\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.3716 - acc: 0.8354\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.3165 - acc: 0.8635\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.3195 - acc: 0.8587\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.3080 - acc: 0.8673\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.3766 - acc: 0.8280\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.3514 - acc: 0.8427\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2908 - acc: 0.8739\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.2918 - acc: 0.8717\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2783 - acc: 0.8814\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2853 - acc: 0.8763\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2844 - acc: 0.8809\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2800 - acc: 0.8806\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.2873 - acc: 0.8743\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 9s 259ms/step - loss: 0.2535 - acc: 0.8940\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2624 - acc: 0.8907\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2670 - acc: 0.8880\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2806 - acc: 0.8778\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.4002 - acc: 0.8186\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2985 - acc: 0.8690\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.2588 - acc: 0.8930\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2345 - acc: 0.8995\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2490 - acc: 0.8972\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.3654 - acc: 0.8361\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2591 - acc: 0.8891\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.2612 - acc: 0.8890\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.2548 - acc: 0.8890\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 9s 260ms/step - loss: 0.2071 - acc: 0.9151\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2509 - acc: 0.8943\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.2307 - acc: 0.9035\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.3361 - acc: 0.8451\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.3695 - acc: 0.8349\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2329 - acc: 0.9042\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.2155 - acc: 0.9120\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2498 - acc: 0.8946\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2463 - acc: 0.8969\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.2144 - acc: 0.9134\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.1988 - acc: 0.9196\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.1894 - acc: 0.9222\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 9s 256ms/step - loss: 0.1898 - acc: 0.9235\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2071 - acc: 0.9148\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2235 - acc: 0.9067\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 9s 258ms/step - loss: 0.2175 - acc: 0.9095\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.1961 - acc: 0.9227\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2265 - acc: 0.9048\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.975042\n",
      "ACC: 0.928029\n",
      "MCC : 0.856774\n",
      "TPR:0.906475\n",
      "FPR:0.050667\n",
      "Pre:0.946479\n",
      "F1:0.926045\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 17s 498ms/step - loss: 0.6727 - acc: 0.5848\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.6614 - acc: 0.6257\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.6619 - acc: 0.6257\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.6611 - acc: 0.6257\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.6609 - acc: 0.6257\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.6604 - acc: 0.6257\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.6602 - acc: 0.6257\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.6587 - acc: 0.6257\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.6580 - acc: 0.6260\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6567 - acc: 0.6258\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.6517 - acc: 0.6261\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.6530 - acc: 0.6298\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.6459 - acc: 0.6372\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.6306 - acc: 0.6545\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.6363 - acc: 0.6521\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.6236 - acc: 0.6560\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 9s 252ms/step - loss: 0.6003 - acc: 0.6870\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.5922 - acc: 0.6842\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.5632 - acc: 0.7139\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.5634 - acc: 0.7122\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.5449 - acc: 0.7277\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.5437 - acc: 0.7267\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.5372 - acc: 0.7344\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.5335 - acc: 0.7335\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.5089 - acc: 0.7554\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4946 - acc: 0.7645\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.5197 - acc: 0.7468\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.4894 - acc: 0.7673\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.5082 - acc: 0.7507\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4584 - acc: 0.7914\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.4682 - acc: 0.7790\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4744 - acc: 0.7785\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.4471 - acc: 0.7948\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.4696 - acc: 0.7779\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4284 - acc: 0.8071\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.4488 - acc: 0.7898\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.4201 - acc: 0.8087\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.4406 - acc: 0.7914\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.3990 - acc: 0.8250\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.4160 - acc: 0.8110\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 8s 244ms/step - loss: 0.4022 - acc: 0.8211\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3974 - acc: 0.8233\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.3763 - acc: 0.8374\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.3849 - acc: 0.8276\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3571 - acc: 0.8482\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3701 - acc: 0.8357\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.4006 - acc: 0.8143\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.4371 - acc: 0.7943\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.3883 - acc: 0.8278\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.3592 - acc: 0.8452\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3395 - acc: 0.8524\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.3718 - acc: 0.8379\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.3194 - acc: 0.8643\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3534 - acc: 0.8435\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3406 - acc: 0.8497\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.2972 - acc: 0.8760\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.3134 - acc: 0.8645\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.4728 - acc: 0.7780\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.3666 - acc: 0.8348\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2939 - acc: 0.8722\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 9s 257ms/step - loss: 0.2933 - acc: 0.8727\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2874 - acc: 0.8773\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2832 - acc: 0.8791\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.3013 - acc: 0.8727\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.2720 - acc: 0.8835\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2602 - acc: 0.8895\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2836 - acc: 0.8781\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 8s 250ms/step - loss: 0.3003 - acc: 0.8698\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 8s 247ms/step - loss: 0.2848 - acc: 0.8747\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 9s 255ms/step - loss: 0.3072 - acc: 0.8660\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2797 - acc: 0.8812\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2464 - acc: 0.9006\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2431 - acc: 0.9011\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2695 - acc: 0.8872\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2404 - acc: 0.8977\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2647 - acc: 0.8873\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.2380 - acc: 0.8972\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.2376 - acc: 0.8982\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2270 - acc: 0.9075\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2539 - acc: 0.8945\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2607 - acc: 0.8914\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.3724 - acc: 0.8498\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.3375 - acc: 0.8513\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2273 - acc: 0.9076\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2367 - acc: 0.9010\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 9s 253ms/step - loss: 0.2243 - acc: 0.9026\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 8s 249ms/step - loss: 0.2092 - acc: 0.9153\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2006 - acc: 0.9168\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 8s 245ms/step - loss: 0.2176 - acc: 0.9092\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2201 - acc: 0.9091\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 9s 252ms/step - loss: 0.2056 - acc: 0.9114\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2803 - acc: 0.8872\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.4638 - acc: 0.7803\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.3695 - acc: 0.8332\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.2277 - acc: 0.9048\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 9s 254ms/step - loss: 0.2105 - acc: 0.9162\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 8s 248ms/step - loss: 0.2114 - acc: 0.9103\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.1956 - acc: 0.9220\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 9s 251ms/step - loss: 0.2180 - acc: 0.9076\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 9s 250ms/step - loss: 0.2466 - acc: 0.8937\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.973502\n",
      "ACC: 0.931158\n",
      "MCC : 0.862689\n",
      "TPR:0.947880\n",
      "FPR:0.085973\n",
      "Pre:0.918664\n",
      "F1:0.933043\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.972633\n",
      "mean ACC: 0.927324\n",
      "mean MCC : 0.855872\n",
      "mean TPR:0.945240\n",
      "mean FPR:0.090901\n",
      "mean Pre:0.912844\n",
      "mean F1:0.928338\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'SC'\n",
    "for rep in range(1):\n",
    "    n_splits = 5\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'yeast_data/new_train_valid-'+str(split) \n",
    "        test_pairs_file = 'yeast_data/new_test-'+str(split) \n",
    "#         valid_pairs_file = 'yeast_data/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 256\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "#         valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'yeast_data/sc_seq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_loss', patience=20, verbose=0, mode='min')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_loss', mode='min', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    epochs = 100,verbose=1)\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "#         model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('yeast_seq'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
