{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermVectors:\n",
    "    def __init__(self):\n",
    "        self.termVectorDict=dict()\n",
    "        self.terms=None\n",
    "        self.vectors=None\n",
    "\n",
    "    def parse_term_embedding_file(self,file_path):\n",
    "        with open(file_path) as f:\n",
    "            lines=f.readlines()\n",
    "            self.vectors=lines[1:] # 第一个是term的数量，不是termID\n",
    "            terms = []\n",
    "            for line in lines[1:]:\n",
    "                term=line.split()[0]\n",
    "                terms.append(term)\n",
    "            self.terms = terms\n",
    "\n",
    "    def str_to_vector(self,Str):\n",
    "        Str = Str.strip('\\n')\n",
    "        nums = Str.split()\n",
    "        vec = []\n",
    "        for num in nums:\n",
    "            vec.append(float(num))\n",
    "        return np.array(vec)\n",
    "\n",
    "    def construct_term_vector_dict(self):\n",
    "        for term in self.terms:\n",
    "            termindex = self.terms.index(term)\n",
    "            line=self.vectors[termindex]\n",
    "            s =line[len(term):].lstrip(' ')\n",
    "            s_vec =self.str_to_vector(s)\n",
    "            self.termVectorDict[term]=s_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29699\n",
      "4202\n",
      "11148\n"
     ]
    }
   ],
   "source": [
    "# construct Node2Vec termVector\n",
    "BP_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/BP.emb'\n",
    "CC_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/CC.emb'\n",
    "MF_TERM_EMB_FILE_PATH = '../Node2Vec/EDGELIST/MF.emb'\n",
    "Node2Vec_dim = 300\n",
    "\n",
    "\n",
    "BPTermVectors = TermVectors()\n",
    "BPTermVectors.parse_term_embedding_file(BP_TERM_EMB_FILE_PATH)\n",
    "BPTermVectors.construct_term_vector_dict()\n",
    "print(len(BPTermVectors.termVectorDict))\n",
    "\n",
    "CCTermVectors = TermVectors()\n",
    "CCTermVectors.parse_term_embedding_file(CC_TERM_EMB_FILE_PATH)\n",
    "CCTermVectors.construct_term_vector_dict()\n",
    "print(len(CCTermVectors.termVectorDict))\n",
    "\n",
    "MFTermVectors = TermVectors()\n",
    "MFTermVectors.parse_term_embedding_file(MF_TERM_EMB_FILE_PATH)\n",
    "MFTermVectors.construct_term_vector_dict()\n",
    "print(len(MFTermVectors.termVectorDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read go.obo obtain ontology type\n",
    "id_type_dicts = {}\n",
    "obo_file = '../cross-species/go.obo'\n",
    "fp=open(obo_file,'r')\n",
    "obo_txt=fp.read()\n",
    "fp.close()\n",
    "obo_txt=obo_txt[obo_txt.find(\"[Term]\")-1:]\n",
    "obo_txt=obo_txt[:obo_txt.find(\"[Typedef]\")]\n",
    "# obo_dict=parse_obo_txt(obo_txt)\n",
    "id_type_dicts = {}\n",
    "for Term_txt in obo_txt.split(\"[Term]\\n\"):\n",
    "    if not Term_txt.strip():\n",
    "        continue\n",
    "    name = ''\n",
    "    ids = []\n",
    "    for line in Term_txt.splitlines():\n",
    "        if   line.startswith(\"id: \"):\n",
    "            ids.append(line[len(\"id: \"):])     \n",
    "        elif line.startswith(\"namespace: \"):\n",
    "             name=line[len(\"namespace: \"):]\n",
    "        elif line.startswith(\"alt_id: \"):\n",
    "            ids.append(line[len(\"alt_id: \"):])\n",
    "    \n",
    "    for t_id in ids:\n",
    "        id_type_dicts[t_id] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 512\n",
    "max_seq_len = 2000\n",
    "max_node_len = 256\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.max_node_len = max_node_len\n",
    "        self.protein2go =  load_dict('yeast_data/protein2go_dicts.pkl')\n",
    " \n",
    "        self.protein2seq = load_dict('yeast_data/protein2seq_dicts.pkl')\n",
    "        self.read_ppi()\n",
    "         \n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.prot2nodevec = {}\n",
    "        self.prot2nodevec_fun()\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def prot2nodevec_fun(self):\n",
    "        for key, value in self.protein2go.items():\n",
    "            X_go1 =  np.zeros((1,Node2Vec_dim))\n",
    "            allgos = value.split(';') \n",
    "            allgos = list(set(allgos))\n",
    "            count = 0\n",
    "            for  go in  allgos:\n",
    "                if go.startswith('GO'):\n",
    "                    if id_type_dicts[go] == '':\n",
    "                        termVectors = BPTermVectors\n",
    "                        term_ID=go[3:].lstrip('0')\n",
    "                        feature = termVectors.termVectorDict[term_ID]\n",
    "                    elif id_type_dicts[go] == '':\n",
    "                        termVectors = CCTermVectors\n",
    "                        term_ID=go[3:].lstrip('0')\n",
    "                        feature = termVectors.termVectorDict[term_ID]\n",
    "                    elif id_type_dicts[go] == '':\n",
    "                        termVectors = MFTermVectors\n",
    "                        term_ID=go[3:].lstrip('0')\n",
    "                        feature = termVectors.termVectorDict[term_ID]\n",
    "                    else:\n",
    "                        feature = np.zeros((1,Node2Vec_dim))\n",
    "                    \n",
    "                     \n",
    "                    if count + feature.shape[0] > max_go_len:\n",
    "                        break\n",
    "                    X_go1 = np.concatenate((X_go1,feature ))    \n",
    "                    count += feature.shape[0]\n",
    "            self.prot2nodevec[key] =  X_go1[1:]   \n",
    "        \n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "            \n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "     \n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        \n",
    "        y = np.empty((self.batch_size))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "        X_node1 = np.empty((self.batch_size, self.max_node_len,Node2Vec_dim))\n",
    "        X_node2 = np.empty((self.batch_size, self.max_node_len,Node2Vec_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "             \n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_node = self.prot2nodevec[p1]\n",
    "            X_node1[i,:prot1emb_node.shape[0]] = prot1emb_node\n",
    "            \n",
    "            prot2emb_node = self.prot2nodevec[p2]\n",
    "            X_node2[i,:prot2emb_node.shape[0]] = prot2emb_node\n",
    "            \n",
    "            \n",
    "            \n",
    "     \n",
    "        return [  X_node1, X_node2, X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "        X_node1 = np.empty((len(list_IDs_temp), self.max_node_len,Node2Vec_dim))\n",
    "        X_node2 = np.empty((len(list_IDs_temp), self.max_node_len,Node2Vec_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_node = self.prot2nodevec[p1]\n",
    "            X_node1[i,:prot1emb_node.shape[0]] = prot1emb_node\n",
    "            \n",
    "            prot2emb_node = self.prot2nodevec[p2]\n",
    "            X_node2[i,:prot2emb_node.shape[0]] = prot2emb_node\n",
    "            \n",
    "  \n",
    "        return [  X_node1, X_node2, X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 256, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 256, 300)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 2000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 2000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 256, 16)      14416       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 256, 16)      4816        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 256, 16)      14416       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 256, 16)      4816        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2000, 32)     1952        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2000, 32)     672         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2000, 32)     1952        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 2000, 32)     672         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 256, 16)      1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 256, 16)      784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 256, 16)      14416       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 256, 16)      4816        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 256, 16)      1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 256, 16)      784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 256, 16)      14416       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 256, 16)      4816        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2000, 32)     5152        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 2000, 32)     3104        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2000, 32)     1952        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 2000, 32)     672         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 2000, 32)     5152        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 2000, 32)     3104        conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 2000, 32)     1952        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 2000, 32)     672         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 256, 64)      0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 64)      0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2000, 128)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 2000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 2000, 128)    0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 2000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 256, 64)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256, 64)      0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2000, 128)    0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2000, 128)    0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 2000, 128)    0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2000, 128)    0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           320         dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 64)           320         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 128)          2128        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          2128        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 128)          2128        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          2128        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 192)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 192)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 768)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 attention_2[0][0]                \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 768)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 attention_4[0][0]                \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          49408       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          49408       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          196864      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            513         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,300,353\n",
      "Trainable params: 3,300,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(2)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    " \n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([ x_a, x_b, x_c, x_gru_c, x_gru_b,  x_gru_a])\n",
    "    x = Dense(256)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_cnn_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    \n",
    "    x = Concatenate()([ x_a, x_b, x_c])\n",
    "    x = Dense(256)(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 128\n",
    "    gru_units = 64\n",
    "    \n",
    "    \n",
    "    left_input_node = Input(shape=(max_node_len,Node2Vec_dim))\n",
    "    right_input_node = Input(shape=(max_node_len,Node2Vec_dim))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "     \n",
    "\n",
    "     \n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters, gru_units)\n",
    "    \n",
    "    left_x_node = build_cnn_model(left_input_node, con_filters//2, gru_units)\n",
    "    right_x_node = build_cnn_model(right_input_node, con_filters//2,gru_units)\n",
    "    \n",
    "      \n",
    "     \n",
    "    \n",
    "    \n",
    "   \n",
    "   \n",
    "    x =   Concatenate()([ left_x_node,  right_x_node, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([  left_input_node, right_input_node,     left_input_seq, right_input_seq], output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "# siamese_a = create_share_model()\n",
    "# siamese_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55/55 [==============================] - 44s 796ms/step - loss: 0.6727 - acc: 0.6246 - val_loss: 0.6649 - val_acc: 0.6172\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 31s 563ms/step - loss: 0.6634 - acc: 0.6260 - val_loss: 0.6638 - val_acc: 0.6172\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 33s 600ms/step - loss: 0.6623 - acc: 0.6260 - val_loss: 0.6622 - val_acc: 0.6172\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 33s 596ms/step - loss: 0.6589 - acc: 0.6260 - val_loss: 0.6620 - val_acc: 0.6172\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.6459 - acc: 0.6368 - val_loss: 0.6396 - val_acc: 0.6695\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.6581 - acc: 0.6304 - val_loss: 0.6661 - val_acc: 0.6172\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 32s 576ms/step - loss: 0.6557 - acc: 0.6253 - val_loss: 0.6568 - val_acc: 0.6196\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 33s 603ms/step - loss: 0.6469 - acc: 0.6361 - val_loss: 0.6565 - val_acc: 0.7001\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 32s 579ms/step - loss: 0.5968 - acc: 0.6984 - val_loss: 0.5815 - val_acc: 0.6917\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 33s 608ms/step - loss: 0.6067 - acc: 0.6984 - val_loss: 0.6325 - val_acc: 0.7127\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 32s 581ms/step - loss: 0.4920 - acc: 0.7737 - val_loss: 0.5533 - val_acc: 0.7224\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 33s 598ms/step - loss: 0.4900 - acc: 0.7643 - val_loss: 0.5713 - val_acc: 0.7037\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 32s 588ms/step - loss: 0.4213 - acc: 0.8108 - val_loss: 0.5790 - val_acc: 0.7272\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 32s 589ms/step - loss: 0.3958 - acc: 0.8278 - val_loss: 0.5248 - val_acc: 0.7470\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 32s 575ms/step - loss: 0.3729 - acc: 0.8384 - val_loss: 0.5398 - val_acc: 0.7410\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 31s 558ms/step - loss: 0.3343 - acc: 0.8595 - val_loss: 0.4967 - val_acc: 0.7837\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 32s 583ms/step - loss: 0.3527 - acc: 0.8486 - val_loss: 0.5054 - val_acc: 0.7740\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 31s 573ms/step - loss: 0.3077 - acc: 0.8685 - val_loss: 0.5644 - val_acc: 0.7560\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 31s 572ms/step - loss: 0.3357 - acc: 0.8563 - val_loss: 0.4911 - val_acc: 0.7542\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.2850 - acc: 0.8795 - val_loss: 0.4956 - val_acc: 0.7903\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.2754 - acc: 0.8857 - val_loss: 0.4625 - val_acc: 0.8101\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.2738 - acc: 0.8834 - val_loss: 0.5097 - val_acc: 0.8059\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 29s 528ms/step - loss: 0.2420 - acc: 0.9013 - val_loss: 0.5020 - val_acc: 0.8053\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 30s 537ms/step - loss: 0.2348 - acc: 0.9003 - val_loss: 0.4667 - val_acc: 0.8053\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 29s 532ms/step - loss: 0.2669 - acc: 0.8869 - val_loss: 0.4244 - val_acc: 0.8371\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 29s 520ms/step - loss: 0.2210 - acc: 0.9067 - val_loss: 0.5576 - val_acc: 0.8125\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 29s 522ms/step - loss: 0.2313 - acc: 0.9006 - val_loss: 0.4681 - val_acc: 0.8365\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 29s 519ms/step - loss: 0.2390 - acc: 0.8987 - val_loss: 0.5256 - val_acc: 0.8203\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 29s 518ms/step - loss: 0.2580 - acc: 0.8930 - val_loss: 0.5462 - val_acc: 0.7969\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 29s 526ms/step - loss: 0.2031 - acc: 0.9163 - val_loss: 0.4932 - val_acc: 0.8269\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 29s 524ms/step - loss: 0.2133 - acc: 0.9084 - val_loss: 0.4389 - val_acc: 0.8425\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.2152 - acc: 0.9099 - val_loss: 0.4577 - val_acc: 0.8486\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 32s 576ms/step - loss: 0.1936 - acc: 0.9205 - val_loss: 0.5952 - val_acc: 0.8221\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 31s 563ms/step - loss: 0.1856 - acc: 0.9189 - val_loss: 0.5304 - val_acc: 0.8425\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 29s 536ms/step - loss: 0.1818 - acc: 0.9234 - val_loss: 0.4382 - val_acc: 0.8576\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.1909 - acc: 0.9232 - val_loss: 0.5164 - val_acc: 0.8456\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.1634 - acc: 0.9311 - val_loss: 0.5170 - val_acc: 0.8540\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.1712 - acc: 0.9283 - val_loss: 0.6108 - val_acc: 0.8456\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 33s 600ms/step - loss: 0.1609 - acc: 0.9308 - val_loss: 0.4035 - val_acc: 0.8474\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 31s 567ms/step - loss: 0.1718 - acc: 0.9266 - val_loss: 0.5751 - val_acc: 0.8323\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 32s 581ms/step - loss: 0.1688 - acc: 0.9261 - val_loss: 0.4013 - val_acc: 0.8684\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 31s 560ms/step - loss: 0.1532 - acc: 0.9384 - val_loss: 0.4959 - val_acc: 0.8606\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 32s 576ms/step - loss: 0.1644 - acc: 0.9332 - val_loss: 0.4852 - val_acc: 0.8558\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 30s 553ms/step - loss: 0.1554 - acc: 0.9382 - val_loss: 0.4459 - val_acc: 0.8618\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 33s 596ms/step - loss: 0.1403 - acc: 0.9446 - val_loss: 0.4206 - val_acc: 0.8720\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 31s 559ms/step - loss: 0.1419 - acc: 0.9389 - val_loss: 0.4182 - val_acc: 0.8582\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 31s 569ms/step - loss: 0.1656 - acc: 0.9322 - val_loss: 0.4678 - val_acc: 0.8594\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.1359 - acc: 0.9439 - val_loss: 0.5440 - val_acc: 0.8606\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 32s 577ms/step - loss: 0.1314 - acc: 0.9487 - val_loss: 0.4837 - val_acc: 0.8738\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.1257 - acc: 0.9504 - val_loss: 0.4616 - val_acc: 0.8660\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.1400 - acc: 0.9411 - val_loss: 0.4978 - val_acc: 0.8570\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 32s 582ms/step - loss: 0.1349 - acc: 0.9449 - val_loss: 0.5081 - val_acc: 0.8504\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 31s 560ms/step - loss: 0.1466 - acc: 0.9428 - val_loss: 0.3992 - val_acc: 0.8648\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 30s 552ms/step - loss: 0.1219 - acc: 0.9520 - val_loss: 0.5393 - val_acc: 0.8684\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.1215 - acc: 0.9517 - val_loss: 0.5280 - val_acc: 0.8702\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 30s 549ms/step - loss: 0.1245 - acc: 0.9496 - val_loss: 0.6136 - val_acc: 0.8732\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 30s 548ms/step - loss: 0.1111 - acc: 0.9555 - val_loss: 0.5191 - val_acc: 0.8678\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 30s 553ms/step - loss: 0.1072 - acc: 0.9580 - val_loss: 0.4903 - val_acc: 0.8738\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 30s 548ms/step - loss: 0.1245 - acc: 0.9504 - val_loss: 0.4748 - val_acc: 0.8726\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.1398 - acc: 0.9473 - val_loss: 0.4566 - val_acc: 0.8750\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.1127 - acc: 0.9557 - val_loss: 0.4314 - val_acc: 0.8774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.0939 - acc: 0.9632 - val_loss: 0.4590 - val_acc: 0.8798\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.1026 - acc: 0.9619 - val_loss: 0.5766 - val_acc: 0.8654\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 31s 569ms/step - loss: 0.0847 - acc: 0.9682 - val_loss: 0.5275 - val_acc: 0.8804\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 33s 595ms/step - loss: 0.1082 - acc: 0.9605 - val_loss: 0.4861 - val_acc: 0.8780\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.0955 - acc: 0.9670 - val_loss: 0.5994 - val_acc: 0.8768\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.0842 - acc: 0.9690 - val_loss: 0.5445 - val_acc: 0.8708\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.0955 - acc: 0.9641 - val_loss: 0.5471 - val_acc: 0.8413\n",
      "Epoch 69/100\n",
      "55/55 [==============================] - 32s 577ms/step - loss: 0.1176 - acc: 0.9548 - val_loss: 0.4996 - val_acc: 0.8660\n",
      "Epoch 70/100\n",
      "55/55 [==============================] - 30s 546ms/step - loss: 0.0800 - acc: 0.9707 - val_loss: 0.6838 - val_acc: 0.8558\n",
      "Epoch 71/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.0895 - acc: 0.9668 - val_loss: 0.4885 - val_acc: 0.8774\n",
      "Epoch 72/100\n",
      "55/55 [==============================] - 32s 573ms/step - loss: 0.0898 - acc: 0.9652 - val_loss: 0.4818 - val_acc: 0.8720\n",
      "Epoch 73/100\n",
      "55/55 [==============================] - 32s 575ms/step - loss: 0.0881 - acc: 0.9645 - val_loss: 0.4619 - val_acc: 0.8690\n",
      "Epoch 74/100\n",
      "55/55 [==============================] - 32s 573ms/step - loss: 0.0715 - acc: 0.9743 - val_loss: 0.7526 - val_acc: 0.8606\n",
      "Epoch 75/100\n",
      "55/55 [==============================] - 31s 572ms/step - loss: 0.0945 - acc: 0.9653 - val_loss: 0.5695 - val_acc: 0.8744\n",
      "Epoch 76/100\n",
      "55/55 [==============================] - 33s 596ms/step - loss: 0.0743 - acc: 0.9719 - val_loss: 0.6486 - val_acc: 0.8684\n",
      "Epoch 77/100\n",
      "55/55 [==============================] - 32s 582ms/step - loss: 0.0832 - acc: 0.9669 - val_loss: 0.4544 - val_acc: 0.8750\n",
      "Epoch 78/100\n",
      "55/55 [==============================] - 32s 575ms/step - loss: 0.0564 - acc: 0.9781 - val_loss: 0.5954 - val_acc: 0.8738\n",
      "Epoch 79/100\n",
      "55/55 [==============================] - 30s 537ms/step - loss: 0.0704 - acc: 0.9766 - val_loss: 0.5255 - val_acc: 0.8768\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.970973\n",
      "ACC: 0.937444\n",
      "MCC : 0.878840\n",
      "TPR:0.984808\n",
      "FPR:0.109920\n",
      "Pre:0.899592\n",
      "F1:0.940273\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "55/55 [==============================] - 38s 690ms/step - loss: 0.6780 - acc: 0.6080 - val_loss: 0.6576 - val_acc: 0.6532\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 30s 540ms/step - loss: 0.6673 - acc: 0.6182 - val_loss: 0.6522 - val_acc: 0.6532\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 31s 555ms/step - loss: 0.6651 - acc: 0.6182 - val_loss: 0.6503 - val_acc: 0.6532\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.6626 - acc: 0.6182 - val_loss: 0.6404 - val_acc: 0.6532\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 31s 568ms/step - loss: 0.6676 - acc: 0.6179 - val_loss: 0.6413 - val_acc: 0.6532\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 30s 545ms/step - loss: 0.6614 - acc: 0.6229 - val_loss: 0.6466 - val_acc: 0.6532\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 30s 545ms/step - loss: 0.6505 - acc: 0.6229 - val_loss: 0.6169 - val_acc: 0.6514\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.6544 - acc: 0.6298 - val_loss: 0.6131 - val_acc: 0.6581\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 31s 555ms/step - loss: 0.6201 - acc: 0.6470 - val_loss: 0.5895 - val_acc: 0.6701\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.5652 - acc: 0.7135 - val_loss: 0.6184 - val_acc: 0.6593\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 30s 543ms/step - loss: 0.6446 - acc: 0.6783 - val_loss: 0.6476 - val_acc: 0.6514\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 31s 567ms/step - loss: 0.5146 - acc: 0.7453 - val_loss: 0.5686 - val_acc: 0.7266\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.4584 - acc: 0.7878 - val_loss: 0.5538 - val_acc: 0.7344\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.4207 - acc: 0.8141 - val_loss: 0.5241 - val_acc: 0.7470\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.3851 - acc: 0.8318 - val_loss: 0.4896 - val_acc: 0.7734\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 30s 553ms/step - loss: 0.3401 - acc: 0.8588 - val_loss: 0.4844 - val_acc: 0.7806\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 31s 563ms/step - loss: 0.3532 - acc: 0.8550 - val_loss: 0.4806 - val_acc: 0.7849\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.3138 - acc: 0.8689 - val_loss: 0.4644 - val_acc: 0.7861\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.3141 - acc: 0.8672 - val_loss: 0.4421 - val_acc: 0.7951\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 30s 548ms/step - loss: 0.2801 - acc: 0.8885 - val_loss: 0.4694 - val_acc: 0.8023\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.2713 - acc: 0.8844 - val_loss: 0.4543 - val_acc: 0.7945\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.2712 - acc: 0.8885 - val_loss: 0.4287 - val_acc: 0.8125\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.2611 - acc: 0.8901 - val_loss: 0.5107 - val_acc: 0.7716\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.2570 - acc: 0.8950 - val_loss: 0.4797 - val_acc: 0.7861\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 30s 549ms/step - loss: 0.2587 - acc: 0.8926 - val_loss: 0.4054 - val_acc: 0.8215\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.2402 - acc: 0.8984 - val_loss: 0.4080 - val_acc: 0.8353\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.2214 - acc: 0.9048 - val_loss: 0.4367 - val_acc: 0.8221\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.2199 - acc: 0.9078 - val_loss: 0.3821 - val_acc: 0.8431\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 31s 555ms/step - loss: 0.2329 - acc: 0.8997 - val_loss: 0.3833 - val_acc: 0.8425\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.2101 - acc: 0.9119 - val_loss: 0.4038 - val_acc: 0.8413\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 31s 555ms/step - loss: 0.2276 - acc: 0.9070 - val_loss: 0.4365 - val_acc: 0.8305\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.1846 - acc: 0.9247 - val_loss: 0.4301 - val_acc: 0.8305\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 31s 566ms/step - loss: 0.1846 - acc: 0.9206 - val_loss: 0.4096 - val_acc: 0.8389\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.1661 - acc: 0.9294 - val_loss: 0.4968 - val_acc: 0.8431\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.1798 - acc: 0.9274 - val_loss: 0.4501 - val_acc: 0.8287\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 30s 550ms/step - loss: 0.1602 - acc: 0.9321 - val_loss: 0.4150 - val_acc: 0.8504\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.1814 - acc: 0.9237 - val_loss: 0.4240 - val_acc: 0.8425\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.1572 - acc: 0.9345 - val_loss: 0.4264 - val_acc: 0.8419\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 30s 541ms/step - loss: 0.1628 - acc: 0.9339 - val_loss: 0.4126 - val_acc: 0.8594\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.1506 - acc: 0.9335 - val_loss: 0.4480 - val_acc: 0.8510\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 31s 573ms/step - loss: 0.1407 - acc: 0.9392 - val_loss: 0.4984 - val_acc: 0.8179\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 30s 545ms/step - loss: 0.1632 - acc: 0.9324 - val_loss: 0.4012 - val_acc: 0.8552\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 30s 547ms/step - loss: 0.1361 - acc: 0.9419 - val_loss: 0.4626 - val_acc: 0.8558\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 30s 550ms/step - loss: 0.1488 - acc: 0.9375 - val_loss: 0.4126 - val_acc: 0.8654\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 29s 536ms/step - loss: 0.1400 - acc: 0.9425 - val_loss: 0.3921 - val_acc: 0.8642\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 29s 529ms/step - loss: 0.1360 - acc: 0.9442 - val_loss: 0.4145 - val_acc: 0.8582\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 28s 507ms/step - loss: 0.1480 - acc: 0.9398 - val_loss: 0.4336 - val_acc: 0.8624\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 28s 503ms/step - loss: 0.1261 - acc: 0.9477 - val_loss: 0.4653 - val_acc: 0.8624\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 28s 515ms/step - loss: 0.1277 - acc: 0.9499 - val_loss: 0.4642 - val_acc: 0.8576\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 28s 504ms/step - loss: 0.1446 - acc: 0.9412 - val_loss: 0.4114 - val_acc: 0.8576\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 28s 510ms/step - loss: 0.1176 - acc: 0.9551 - val_loss: 0.4900 - val_acc: 0.8564\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 28s 508ms/step - loss: 0.1139 - acc: 0.9540 - val_loss: 0.4624 - val_acc: 0.8654\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 29s 535ms/step - loss: 0.1126 - acc: 0.9528 - val_loss: 0.4513 - val_acc: 0.8600\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.1066 - acc: 0.9551 - val_loss: 0.5073 - val_acc: 0.8564\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.1095 - acc: 0.9547 - val_loss: 0.4839 - val_acc: 0.8636\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 28s 503ms/step - loss: 0.1032 - acc: 0.9587 - val_loss: 0.4234 - val_acc: 0.8690\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 29s 528ms/step - loss: 0.1326 - acc: 0.9464 - val_loss: 0.4782 - val_acc: 0.8636\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 30s 549ms/step - loss: 0.0976 - acc: 0.9634 - val_loss: 0.4609 - val_acc: 0.8702\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.0986 - acc: 0.9616 - val_loss: 0.3946 - val_acc: 0.8648\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 30s 548ms/step - loss: 0.1120 - acc: 0.9567 - val_loss: 0.4703 - val_acc: 0.8540\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.1092 - acc: 0.9577 - val_loss: 0.4187 - val_acc: 0.8431\n",
      "Epoch 62/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.1007 - acc: 0.9602 - val_loss: 0.4285 - val_acc: 0.8750\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 31s 559ms/step - loss: 0.0917 - acc: 0.9659 - val_loss: 0.5263 - val_acc: 0.8522\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 30s 540ms/step - loss: 0.1053 - acc: 0.9577 - val_loss: 0.5388 - val_acc: 0.8395\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 32s 573ms/step - loss: 0.0984 - acc: 0.9611 - val_loss: 0.5256 - val_acc: 0.8636\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.0726 - acc: 0.9703 - val_loss: 0.6388 - val_acc: 0.8690\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 31s 560ms/step - loss: 0.0924 - acc: 0.9619 - val_loss: 0.5305 - val_acc: 0.8642\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.0777 - acc: 0.9682 - val_loss: 0.5760 - val_acc: 0.8570\n",
      "Epoch 69/100\n",
      "55/55 [==============================] - 30s 545ms/step - loss: 0.0809 - acc: 0.9680 - val_loss: 0.7474 - val_acc: 0.8456\n",
      "Epoch 70/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.0912 - acc: 0.9679 - val_loss: 0.5037 - val_acc: 0.8678\n",
      "Epoch 71/100\n",
      "55/55 [==============================] - 30s 549ms/step - loss: 0.0763 - acc: 0.9720 - val_loss: 0.5491 - val_acc: 0.8528\n",
      "Epoch 72/100\n",
      "55/55 [==============================] - 30s 553ms/step - loss: 0.0808 - acc: 0.9685 - val_loss: 0.6104 - val_acc: 0.8654\n",
      "Epoch 73/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.0936 - acc: 0.9656 - val_loss: 0.4812 - val_acc: 0.8678\n",
      "Epoch 74/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.0752 - acc: 0.9746 - val_loss: 0.5049 - val_acc: 0.8732\n",
      "Epoch 75/100\n",
      "55/55 [==============================] - 30s 553ms/step - loss: 0.1032 - acc: 0.9597 - val_loss: 0.4282 - val_acc: 0.8744\n",
      "Epoch 76/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.0715 - acc: 0.9730 - val_loss: 0.5380 - val_acc: 0.8690\n",
      "Epoch 77/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.0696 - acc: 0.9730 - val_loss: 0.4491 - val_acc: 0.8768\n",
      "Epoch 78/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.0716 - acc: 0.9723 - val_loss: 0.4765 - val_acc: 0.8720\n",
      "Epoch 79/100\n",
      "55/55 [==============================] - 30s 550ms/step - loss: 0.0563 - acc: 0.9804 - val_loss: 0.6251 - val_acc: 0.8750\n",
      "Epoch 80/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.0811 - acc: 0.9678 - val_loss: 0.4881 - val_acc: 0.8774\n",
      "Epoch 81/100\n",
      "55/55 [==============================] - 29s 532ms/step - loss: 0.0709 - acc: 0.9753 - val_loss: 0.5747 - val_acc: 0.8714\n",
      "Epoch 82/100\n",
      "55/55 [==============================] - 31s 558ms/step - loss: 0.0628 - acc: 0.9771 - val_loss: 0.5983 - val_acc: 0.8654\n",
      "Epoch 83/100\n",
      "55/55 [==============================] - 30s 547ms/step - loss: 0.0635 - acc: 0.9770 - val_loss: 0.4789 - val_acc: 0.8642\n",
      "Epoch 84/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.0557 - acc: 0.9793 - val_loss: 0.5465 - val_acc: 0.8726\n",
      "Epoch 85/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.0563 - acc: 0.9811 - val_loss: 0.5451 - val_acc: 0.8672\n",
      "Epoch 86/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.0611 - acc: 0.9778 - val_loss: 0.5188 - val_acc: 0.8654\n",
      "Epoch 87/100\n",
      "55/55 [==============================] - 31s 566ms/step - loss: 0.0707 - acc: 0.9726 - val_loss: 0.6886 - val_acc: 0.8678\n",
      "Epoch 88/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.0625 - acc: 0.9774 - val_loss: 0.5396 - val_acc: 0.8714\n",
      "Epoch 89/100\n",
      "55/55 [==============================] - 32s 579ms/step - loss: 0.0521 - acc: 0.9803 - val_loss: 0.4893 - val_acc: 0.8816\n",
      "Epoch 90/100\n",
      "55/55 [==============================] - 31s 558ms/step - loss: 0.0595 - acc: 0.9776 - val_loss: 0.4859 - val_acc: 0.8756\n",
      "Epoch 91/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.0528 - acc: 0.9820 - val_loss: 0.5077 - val_acc: 0.8744\n",
      "Epoch 92/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.0615 - acc: 0.9777 - val_loss: 0.4940 - val_acc: 0.8780\n",
      "Epoch 93/100\n",
      "55/55 [==============================] - 30s 550ms/step - loss: 0.0559 - acc: 0.9797 - val_loss: 0.5768 - val_acc: 0.8726\n",
      "Epoch 94/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.0511 - acc: 0.9828 - val_loss: 0.5765 - val_acc: 0.8738\n",
      "Epoch 95/100\n",
      "55/55 [==============================] - 30s 540ms/step - loss: 0.0531 - acc: 0.9808 - val_loss: 0.5310 - val_acc: 0.8618\n",
      "Epoch 96/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.0580 - acc: 0.9774 - val_loss: 0.5068 - val_acc: 0.8750\n",
      "Epoch 97/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.0682 - acc: 0.9727 - val_loss: 0.5698 - val_acc: 0.8762\n",
      "Epoch 98/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.0633 - acc: 0.9744 - val_loss: 0.5790 - val_acc: 0.8780\n",
      "Epoch 99/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.0606 - acc: 0.9768 - val_loss: 0.5034 - val_acc: 0.8660\n",
      "Epoch 100/100\n",
      "55/55 [==============================] - 30s 553ms/step - loss: 0.0778 - acc: 0.9699 - val_loss: 0.6415 - val_acc: 0.8738\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.967071\n",
      "ACC: 0.933870\n",
      "MCC : 0.871367\n",
      "TPR:0.979446\n",
      "FPR:0.111707\n",
      "Pre:0.897625\n",
      "F1:0.936752\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 39s 715ms/step - loss: 0.6772 - acc: 0.6190 - val_loss: 0.6734 - val_acc: 0.6376\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 30s 552ms/step - loss: 0.6645 - acc: 0.6227 - val_loss: 0.6596 - val_acc: 0.6376\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.6644 - acc: 0.6227 - val_loss: 0.6599 - val_acc: 0.6376\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 31s 569ms/step - loss: 0.6593 - acc: 0.6227 - val_loss: 0.6572 - val_acc: 0.6376\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.6837 - acc: 0.6230 - val_loss: 0.6846 - val_acc: 0.6376\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 33s 594ms/step - loss: 0.6681 - acc: 0.6156 - val_loss: 0.6537 - val_acc: 0.6376\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.6639 - acc: 0.6227 - val_loss: 0.6583 - val_acc: 0.6376\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.6581 - acc: 0.6227 - val_loss: 0.6514 - val_acc: 0.6376\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.6481 - acc: 0.6341 - val_loss: 0.6351 - val_acc: 0.6364\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 31s 563ms/step - loss: 0.6449 - acc: 0.6524 - val_loss: 0.6410 - val_acc: 0.6376\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 31s 555ms/step - loss: 0.6405 - acc: 0.6588 - val_loss: 0.6264 - val_acc: 0.6629\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 31s 569ms/step - loss: 0.5814 - acc: 0.7054 - val_loss: 0.8098 - val_acc: 0.4663\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 31s 569ms/step - loss: 0.5339 - acc: 0.7399 - val_loss: 0.6879 - val_acc: 0.5433\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 32s 573ms/step - loss: 0.4923 - acc: 0.7673 - val_loss: 0.6622 - val_acc: 0.6118\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.4436 - acc: 0.7999 - val_loss: 0.6399 - val_acc: 0.6328\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.4173 - acc: 0.8175 - val_loss: 0.6489 - val_acc: 0.6965\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 31s 572ms/step - loss: 0.3862 - acc: 0.8344 - val_loss: 0.5035 - val_acc: 0.7620\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 30s 539ms/step - loss: 0.3741 - acc: 0.8402 - val_loss: 0.4644 - val_acc: 0.7933\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.3134 - acc: 0.8670 - val_loss: 0.4825 - val_acc: 0.7788\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 31s 560ms/step - loss: 0.3491 - acc: 0.8514 - val_loss: 0.5259 - val_acc: 0.7302\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.3070 - acc: 0.8700 - val_loss: 0.4666 - val_acc: 0.7782\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.2985 - acc: 0.8726 - val_loss: 0.5212 - val_acc: 0.7163\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 31s 573ms/step - loss: 0.2958 - acc: 0.8726 - val_loss: 0.5396 - val_acc: 0.7127\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 31s 567ms/step - loss: 0.2920 - acc: 0.8821 - val_loss: 0.4729 - val_acc: 0.7614\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.2901 - acc: 0.8810 - val_loss: 0.4162 - val_acc: 0.8227\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 31s 572ms/step - loss: 0.2402 - acc: 0.9044 - val_loss: 0.4044 - val_acc: 0.8227\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 29s 535ms/step - loss: 0.2312 - acc: 0.9021 - val_loss: 0.4744 - val_acc: 0.7626\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 29s 520ms/step - loss: 0.2195 - acc: 0.9064 - val_loss: 0.4289 - val_acc: 0.8185\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 29s 520ms/step - loss: 0.2275 - acc: 0.9033 - val_loss: 0.3893 - val_acc: 0.8251\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 29s 518ms/step - loss: 0.2253 - acc: 0.9074 - val_loss: 0.4643 - val_acc: 0.7825\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 28s 515ms/step - loss: 0.2488 - acc: 0.8928 - val_loss: 0.4575 - val_acc: 0.8131\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 28s 512ms/step - loss: 0.2001 - acc: 0.9158 - val_loss: 0.4430 - val_acc: 0.8203\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 28s 516ms/step - loss: 0.1990 - acc: 0.9149 - val_loss: 0.4032 - val_acc: 0.8438\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 29s 519ms/step - loss: 0.1833 - acc: 0.9249 - val_loss: 0.4719 - val_acc: 0.7975\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 28s 515ms/step - loss: 0.1888 - acc: 0.9202 - val_loss: 0.4055 - val_acc: 0.8323\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 29s 520ms/step - loss: 0.1849 - acc: 0.9240 - val_loss: 0.3822 - val_acc: 0.8528\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 27s 498ms/step - loss: 0.1799 - acc: 0.9270 - val_loss: 0.5095 - val_acc: 0.8275\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 27s 497ms/step - loss: 0.1820 - acc: 0.9237 - val_loss: 0.4076 - val_acc: 0.8582\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 28s 516ms/step - loss: 0.1776 - acc: 0.9259 - val_loss: 0.3960 - val_acc: 0.8456\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 28s 512ms/step - loss: 0.1678 - acc: 0.9308 - val_loss: 0.4038 - val_acc: 0.8522\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 28s 512ms/step - loss: 0.1609 - acc: 0.9361 - val_loss: 0.4075 - val_acc: 0.8299\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 28s 514ms/step - loss: 0.1895 - acc: 0.9240 - val_loss: 0.4107 - val_acc: 0.8498\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 29s 529ms/step - loss: 0.1657 - acc: 0.9322 - val_loss: 0.4530 - val_acc: 0.8149\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 29s 528ms/step - loss: 0.1534 - acc: 0.9389 - val_loss: 0.4119 - val_acc: 0.8534\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 29s 535ms/step - loss: 0.1442 - acc: 0.9460 - val_loss: 0.4366 - val_acc: 0.8558\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 30s 538ms/step - loss: 0.1619 - acc: 0.9348 - val_loss: 0.4065 - val_acc: 0.8624\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 29s 532ms/step - loss: 0.1354 - acc: 0.9466 - val_loss: 0.4068 - val_acc: 0.8600\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 29s 536ms/step - loss: 0.1539 - acc: 0.9396 - val_loss: 0.4025 - val_acc: 0.8582\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.1390 - acc: 0.9457 - val_loss: 0.4494 - val_acc: 0.8528\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 30s 550ms/step - loss: 0.1393 - acc: 0.9455 - val_loss: 0.3872 - val_acc: 0.8546\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.1224 - acc: 0.9534 - val_loss: 0.4623 - val_acc: 0.8149\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 33s 596ms/step - loss: 0.1246 - acc: 0.9516 - val_loss: 0.3849 - val_acc: 0.8630\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 32s 577ms/step - loss: 0.1384 - acc: 0.9466 - val_loss: 0.3741 - val_acc: 0.8582\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 32s 579ms/step - loss: 0.1289 - acc: 0.9474 - val_loss: 0.3871 - val_acc: 0.8738\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 32s 575ms/step - loss: 0.1207 - acc: 0.9518 - val_loss: 0.4050 - val_acc: 0.8480\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 31s 555ms/step - loss: 0.1279 - acc: 0.9521 - val_loss: 0.4198 - val_acc: 0.8570\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 31s 567ms/step - loss: 0.1212 - acc: 0.9543 - val_loss: 0.4463 - val_acc: 0.8660\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 32s 584ms/step - loss: 0.1186 - acc: 0.9534 - val_loss: 0.4319 - val_acc: 0.8744\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 32s 588ms/step - loss: 0.1161 - acc: 0.9531 - val_loss: 0.3749 - val_acc: 0.8600\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 32s 589ms/step - loss: 0.1111 - acc: 0.9592 - val_loss: 0.4524 - val_acc: 0.8083\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.1163 - acc: 0.9564 - val_loss: 0.4127 - val_acc: 0.8660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "55/55 [==============================] - 30s 553ms/step - loss: 0.1267 - acc: 0.9536 - val_loss: 0.3942 - val_acc: 0.8732\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.1014 - acc: 0.9598 - val_loss: 0.4493 - val_acc: 0.8690\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 32s 586ms/step - loss: 0.0963 - acc: 0.9628 - val_loss: 0.4389 - val_acc: 0.8726\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 33s 594ms/step - loss: 0.0947 - acc: 0.9631 - val_loss: 0.4745 - val_acc: 0.8612\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 32s 586ms/step - loss: 0.0881 - acc: 0.9658 - val_loss: 0.4369 - val_acc: 0.8660\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 32s 581ms/step - loss: 0.0879 - acc: 0.9692 - val_loss: 0.4636 - val_acc: 0.8762\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 33s 599ms/step - loss: 0.0865 - acc: 0.9675 - val_loss: 0.4936 - val_acc: 0.8510\n",
      "Epoch 69/100\n",
      "55/55 [==============================] - 32s 585ms/step - loss: 0.0985 - acc: 0.9632 - val_loss: 0.3991 - val_acc: 0.8780\n",
      "Epoch 70/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.0772 - acc: 0.9699 - val_loss: 0.5575 - val_acc: 0.8744\n",
      "Epoch 71/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.0975 - acc: 0.9631 - val_loss: 0.4543 - val_acc: 0.8780\n",
      "Epoch 72/100\n",
      "55/55 [==============================] - 30s 539ms/step - loss: 0.1013 - acc: 0.9624 - val_loss: 0.5118 - val_acc: 0.8774\n",
      "Epoch 73/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.0847 - acc: 0.9665 - val_loss: 0.5814 - val_acc: 0.8726\n",
      "Epoch 74/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.0912 - acc: 0.9656 - val_loss: 0.4175 - val_acc: 0.8738\n",
      "Epoch 75/100\n",
      "55/55 [==============================] - 32s 583ms/step - loss: 0.0992 - acc: 0.9631 - val_loss: 0.5399 - val_acc: 0.8696\n",
      "Epoch 76/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.0886 - acc: 0.9668 - val_loss: 0.4597 - val_acc: 0.8756\n",
      "Epoch 77/100\n",
      "55/55 [==============================] - 32s 582ms/step - loss: 0.0827 - acc: 0.9703 - val_loss: 0.4366 - val_acc: 0.8744\n",
      "Epoch 78/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.0784 - acc: 0.9703 - val_loss: 0.5505 - val_acc: 0.8630\n",
      "Epoch 79/100\n",
      "55/55 [==============================] - 32s 579ms/step - loss: 0.0906 - acc: 0.9651 - val_loss: 0.5176 - val_acc: 0.8786\n",
      "Epoch 80/100\n",
      "55/55 [==============================] - 30s 546ms/step - loss: 0.0731 - acc: 0.9707 - val_loss: 0.5022 - val_acc: 0.8750\n",
      "Epoch 81/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.0804 - acc: 0.9693 - val_loss: 0.4401 - val_acc: 0.8792\n",
      "Epoch 82/100\n",
      "55/55 [==============================] - 30s 541ms/step - loss: 0.0569 - acc: 0.9790 - val_loss: 0.5310 - val_acc: 0.8690\n",
      "Epoch 83/100\n",
      "55/55 [==============================] - 32s 586ms/step - loss: 0.0698 - acc: 0.9747 - val_loss: 0.4357 - val_acc: 0.8558\n",
      "Epoch 84/100\n",
      "55/55 [==============================] - 33s 597ms/step - loss: 0.0870 - acc: 0.9672 - val_loss: 0.4540 - val_acc: 0.8648\n",
      "Epoch 85/100\n",
      "55/55 [==============================] - 32s 584ms/step - loss: 0.0719 - acc: 0.9727 - val_loss: 0.4861 - val_acc: 0.8786\n",
      "Epoch 86/100\n",
      "55/55 [==============================] - 32s 588ms/step - loss: 0.0662 - acc: 0.9754 - val_loss: 0.4886 - val_acc: 0.8822\n",
      "Epoch 87/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.0646 - acc: 0.9751 - val_loss: 0.5220 - val_acc: 0.8732\n",
      "Epoch 88/100\n",
      "55/55 [==============================] - 33s 593ms/step - loss: 0.0812 - acc: 0.9706 - val_loss: 0.4932 - val_acc: 0.8552\n",
      "Epoch 89/100\n",
      "55/55 [==============================] - 32s 575ms/step - loss: 0.0878 - acc: 0.9632 - val_loss: 0.4614 - val_acc: 0.8816\n",
      "Epoch 90/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.0699 - acc: 0.9740 - val_loss: 0.5937 - val_acc: 0.8624\n",
      "Epoch 91/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.0610 - acc: 0.9780 - val_loss: 0.5258 - val_acc: 0.8624\n",
      "Epoch 92/100\n",
      "55/55 [==============================] - 32s 589ms/step - loss: 0.0560 - acc: 0.9798 - val_loss: 0.5418 - val_acc: 0.8810\n",
      "Epoch 93/100\n",
      "55/55 [==============================] - 30s 546ms/step - loss: 0.0717 - acc: 0.9727 - val_loss: 0.4895 - val_acc: 0.8666\n",
      "Epoch 94/100\n",
      "55/55 [==============================] - 30s 547ms/step - loss: 0.0590 - acc: 0.9788 - val_loss: 0.5182 - val_acc: 0.8678\n",
      "Epoch 95/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.0506 - acc: 0.9815 - val_loss: 0.5397 - val_acc: 0.8774\n",
      "Epoch 96/100\n",
      "55/55 [==============================] - 30s 545ms/step - loss: 0.0611 - acc: 0.9763 - val_loss: 0.7251 - val_acc: 0.8750\n",
      "Epoch 97/100\n",
      "55/55 [==============================] - 30s 550ms/step - loss: 0.0563 - acc: 0.9815 - val_loss: 0.5743 - val_acc: 0.8714\n",
      "Epoch 98/100\n",
      "55/55 [==============================] - 32s 583ms/step - loss: 0.0535 - acc: 0.9784 - val_loss: 0.4984 - val_acc: 0.8756\n",
      "Epoch 99/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.0579 - acc: 0.9777 - val_loss: 0.5057 - val_acc: 0.8666\n",
      "Epoch 100/100\n",
      "55/55 [==============================] - 32s 587ms/step - loss: 0.0688 - acc: 0.9741 - val_loss: 0.5292 - val_acc: 0.8804\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.959879\n",
      "ACC: 0.929401\n",
      "MCC : 0.862982\n",
      "TPR:0.978552\n",
      "FPR:0.119750\n",
      "Pre:0.890968\n",
      "F1:0.932709\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "55/55 [==============================] - 42s 765ms/step - loss: 0.6831 - acc: 0.6126 - val_loss: 0.6574 - val_acc: 0.6502\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.6669 - acc: 0.6188 - val_loss: 0.6603 - val_acc: 0.6502\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 32s 574ms/step - loss: 0.6644 - acc: 0.6188 - val_loss: 0.6460 - val_acc: 0.6502\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.6635 - acc: 0.6188 - val_loss: 0.6474 - val_acc: 0.6502\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 30s 545ms/step - loss: 0.6601 - acc: 0.6178 - val_loss: 0.6395 - val_acc: 0.6599\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 30s 537ms/step - loss: 0.6564 - acc: 0.6209 - val_loss: 0.6413 - val_acc: 0.6496\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 30s 546ms/step - loss: 0.6506 - acc: 0.6330 - val_loss: 0.6313 - val_acc: 0.6502\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.5809 - acc: 0.6997 - val_loss: 0.5878 - val_acc: 0.6815\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.5570 - acc: 0.7210 - val_loss: 0.5706 - val_acc: 0.6977\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 31s 569ms/step - loss: 0.5288 - acc: 0.7334 - val_loss: 0.6244 - val_acc: 0.6881\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.4685 - acc: 0.7837 - val_loss: 0.5420 - val_acc: 0.7212\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 32s 575ms/step - loss: 0.4226 - acc: 0.8124 - val_loss: 0.5266 - val_acc: 0.7404\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 32s 575ms/step - loss: 0.4028 - acc: 0.8179 - val_loss: 0.5367 - val_acc: 0.7500\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 32s 573ms/step - loss: 0.3769 - acc: 0.8324 - val_loss: 0.6472 - val_acc: 0.7188\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 31s 568ms/step - loss: 0.3725 - acc: 0.8337 - val_loss: 0.5109 - val_acc: 0.7458\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 32s 574ms/step - loss: 0.3637 - acc: 0.8418 - val_loss: 0.5121 - val_acc: 0.7440\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.3041 - acc: 0.8716 - val_loss: 0.5433 - val_acc: 0.7656\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 30s 539ms/step - loss: 0.3021 - acc: 0.8710 - val_loss: 0.4861 - val_acc: 0.7903\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 30s 549ms/step - loss: 0.3216 - acc: 0.8653 - val_loss: 0.4465 - val_acc: 0.8065\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 30s 540ms/step - loss: 0.2701 - acc: 0.8895 - val_loss: 0.5036 - val_acc: 0.7903\n",
      "Epoch 21/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 30s 551ms/step - loss: 0.2831 - acc: 0.8808 - val_loss: 0.4175 - val_acc: 0.8119\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 31s 566ms/step - loss: 0.2727 - acc: 0.8810 - val_loss: 0.5258 - val_acc: 0.7831\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.2665 - acc: 0.8842 - val_loss: 0.4260 - val_acc: 0.8113\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 33s 593ms/step - loss: 0.2667 - acc: 0.8918 - val_loss: 0.4597 - val_acc: 0.8113\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 32s 583ms/step - loss: 0.2410 - acc: 0.8994 - val_loss: 0.4359 - val_acc: 0.8185\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.2455 - acc: 0.8960 - val_loss: 0.4577 - val_acc: 0.8101\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.2339 - acc: 0.9013 - val_loss: 0.4165 - val_acc: 0.8077\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 31s 573ms/step - loss: 0.2203 - acc: 0.9085 - val_loss: 0.4076 - val_acc: 0.8281\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 31s 566ms/step - loss: 0.2086 - acc: 0.9122 - val_loss: 0.4916 - val_acc: 0.8203\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.2194 - acc: 0.9095 - val_loss: 0.4092 - val_acc: 0.8365\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 30s 541ms/step - loss: 0.2081 - acc: 0.9122 - val_loss: 0.5032 - val_acc: 0.8275\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 30s 552ms/step - loss: 0.1797 - acc: 0.9257 - val_loss: 0.4454 - val_acc: 0.8329\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 30s 543ms/step - loss: 0.1930 - acc: 0.9151 - val_loss: 0.4220 - val_acc: 0.8269\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 30s 546ms/step - loss: 0.2316 - acc: 0.8994 - val_loss: 0.4047 - val_acc: 0.8329\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 31s 572ms/step - loss: 0.1786 - acc: 0.9276 - val_loss: 0.4764 - val_acc: 0.8450\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 32s 583ms/step - loss: 0.1890 - acc: 0.9226 - val_loss: 0.4453 - val_acc: 0.8407\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 32s 585ms/step - loss: 0.1836 - acc: 0.9241 - val_loss: 0.4134 - val_acc: 0.8251\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 32s 577ms/step - loss: 0.1655 - acc: 0.9304 - val_loss: 0.4903 - val_acc: 0.8371\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 31s 557ms/step - loss: 0.1627 - acc: 0.9321 - val_loss: 0.4019 - val_acc: 0.8480\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 29s 533ms/step - loss: 0.1539 - acc: 0.9376 - val_loss: 0.4582 - val_acc: 0.8486\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 30s 537ms/step - loss: 0.1530 - acc: 0.9362 - val_loss: 0.5045 - val_acc: 0.8317\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 29s 521ms/step - loss: 0.1588 - acc: 0.9320 - val_loss: 0.5189 - val_acc: 0.8377\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.1492 - acc: 0.9399 - val_loss: 0.4845 - val_acc: 0.8425\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 29s 528ms/step - loss: 0.1606 - acc: 0.9320 - val_loss: 0.4723 - val_acc: 0.8444\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 30s 542ms/step - loss: 0.1399 - acc: 0.9428 - val_loss: 0.4460 - val_acc: 0.8528\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.1514 - acc: 0.9376 - val_loss: 0.4480 - val_acc: 0.8468\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 31s 565ms/step - loss: 0.1389 - acc: 0.9453 - val_loss: 0.3891 - val_acc: 0.8468\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 32s 576ms/step - loss: 0.1255 - acc: 0.9503 - val_loss: 0.4581 - val_acc: 0.8564\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 30s 547ms/step - loss: 0.1454 - acc: 0.9375 - val_loss: 0.4413 - val_acc: 0.8576\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.1221 - acc: 0.9511 - val_loss: 0.6194 - val_acc: 0.8239\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 31s 567ms/step - loss: 0.1489 - acc: 0.9391 - val_loss: 0.4422 - val_acc: 0.8498\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 30s 552ms/step - loss: 0.1277 - acc: 0.9486 - val_loss: 0.4070 - val_acc: 0.8540\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 30s 554ms/step - loss: 0.1299 - acc: 0.9506 - val_loss: 0.5521 - val_acc: 0.8341\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.1345 - acc: 0.9487 - val_loss: 0.3764 - val_acc: 0.8594\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 30s 552ms/step - loss: 0.1262 - acc: 0.9466 - val_loss: 0.4381 - val_acc: 0.8630\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 30s 552ms/step - loss: 0.1126 - acc: 0.9568 - val_loss: 0.4864 - val_acc: 0.8552\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 30s 547ms/step - loss: 0.1108 - acc: 0.9595 - val_loss: 0.4507 - val_acc: 0.8606\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.1219 - acc: 0.9521 - val_loss: 0.4396 - val_acc: 0.8534\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 31s 559ms/step - loss: 0.1014 - acc: 0.9592 - val_loss: 0.5073 - val_acc: 0.8552\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.1169 - acc: 0.9518 - val_loss: 0.4764 - val_acc: 0.8600\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.1218 - acc: 0.9497 - val_loss: 0.4635 - val_acc: 0.8690\n",
      "Epoch 62/100\n",
      "55/55 [==============================] - 32s 574ms/step - loss: 0.1043 - acc: 0.9598 - val_loss: 0.7878 - val_acc: 0.8317\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 30s 537ms/step - loss: 0.1794 - acc: 0.9286 - val_loss: 0.3883 - val_acc: 0.8504\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.0953 - acc: 0.9625 - val_loss: 0.5625 - val_acc: 0.8582\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 30s 543ms/step - loss: 0.1316 - acc: 0.9470 - val_loss: 0.3979 - val_acc: 0.8774\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 30s 550ms/step - loss: 0.0897 - acc: 0.9651 - val_loss: 0.7068 - val_acc: 0.8510\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.1277 - acc: 0.9499 - val_loss: 0.5185 - val_acc: 0.8528\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 30s 545ms/step - loss: 0.0995 - acc: 0.9615 - val_loss: 0.4696 - val_acc: 0.8612\n",
      "Epoch 69/100\n",
      "55/55 [==============================] - 31s 563ms/step - loss: 0.0848 - acc: 0.9678 - val_loss: 0.5629 - val_acc: 0.8588\n",
      "Epoch 70/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.0803 - acc: 0.9682 - val_loss: 0.5652 - val_acc: 0.8522\n",
      "Epoch 71/100\n",
      "55/55 [==============================] - 31s 562ms/step - loss: 0.0953 - acc: 0.9616 - val_loss: 0.4443 - val_acc: 0.8684\n",
      "Epoch 72/100\n",
      "55/55 [==============================] - 30s 551ms/step - loss: 0.0783 - acc: 0.9685 - val_loss: 0.7320 - val_acc: 0.8540\n",
      "Epoch 73/100\n",
      "55/55 [==============================] - 32s 579ms/step - loss: 0.1038 - acc: 0.9594 - val_loss: 0.5584 - val_acc: 0.8690\n",
      "Epoch 74/100\n",
      "55/55 [==============================] - 31s 570ms/step - loss: 0.0823 - acc: 0.9680 - val_loss: 0.6347 - val_acc: 0.8612\n",
      "Epoch 75/100\n",
      "55/55 [==============================] - 31s 561ms/step - loss: 0.0787 - acc: 0.9716 - val_loss: 0.4797 - val_acc: 0.8672\n",
      "Epoch 76/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.0799 - acc: 0.9716 - val_loss: 0.5596 - val_acc: 0.8756\n",
      "Epoch 77/100\n",
      "55/55 [==============================] - 31s 564ms/step - loss: 0.0849 - acc: 0.9682 - val_loss: 0.5308 - val_acc: 0.8666\n",
      "Epoch 78/100\n",
      "55/55 [==============================] - 30s 540ms/step - loss: 0.0681 - acc: 0.9750 - val_loss: 0.5411 - val_acc: 0.8630\n",
      "Epoch 79/100\n",
      "55/55 [==============================] - 30s 546ms/step - loss: 0.0792 - acc: 0.9713 - val_loss: 0.4532 - val_acc: 0.8576\n",
      "Epoch 80/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.0748 - acc: 0.9744 - val_loss: 0.5596 - val_acc: 0.8558\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.965689\n",
      "ACC: 0.913277\n",
      "MCC : 0.833114\n",
      "TPR:0.975850\n",
      "FPR:0.149240\n",
      "Pre:0.867250\n",
      "F1:0.918350\n",
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55/55 [==============================] - 42s 770ms/step - loss: 0.6747 - acc: 0.6159 - val_loss: 0.6555 - val_acc: 0.6358\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 31s 560ms/step - loss: 0.6647 - acc: 0.6229 - val_loss: 0.6548 - val_acc: 0.6358\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 31s 556ms/step - loss: 0.6654 - acc: 0.6229 - val_loss: 0.6597 - val_acc: 0.6358\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 31s 560ms/step - loss: 0.6595 - acc: 0.6229 - val_loss: 0.6559 - val_acc: 0.6358\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 32s 586ms/step - loss: 0.6594 - acc: 0.6203 - val_loss: 0.6707 - val_acc: 0.6605\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 31s 568ms/step - loss: 0.6298 - acc: 0.6626 - val_loss: 0.9488 - val_acc: 0.6358\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.6615 - acc: 0.6342 - val_loss: 0.6446 - val_acc: 0.6358\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 29s 533ms/step - loss: 0.6304 - acc: 0.6263 - val_loss: 0.5956 - val_acc: 0.7007\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 29s 532ms/step - loss: 0.5542 - acc: 0.7222 - val_loss: 0.5969 - val_acc: 0.7181\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 29s 531ms/step - loss: 0.5040 - acc: 0.7618 - val_loss: 0.6090 - val_acc: 0.6911\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 30s 540ms/step - loss: 0.4472 - acc: 0.7984 - val_loss: 0.5598 - val_acc: 0.7362\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 30s 544ms/step - loss: 0.3908 - acc: 0.8307 - val_loss: 0.5492 - val_acc: 0.7320\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 32s 581ms/step - loss: 0.3692 - acc: 0.8402 - val_loss: 0.4752 - val_acc: 0.7885\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 33s 598ms/step - loss: 0.4203 - acc: 0.8126 - val_loss: 0.5006 - val_acc: 0.7698\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 33s 600ms/step - loss: 0.3638 - acc: 0.8504 - val_loss: 0.5350 - val_acc: 0.7422\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 31s 566ms/step - loss: 0.3049 - acc: 0.8727 - val_loss: 0.6410 - val_acc: 0.7242\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 33s 594ms/step - loss: 0.2926 - acc: 0.8793 - val_loss: 0.5693 - val_acc: 0.7788\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 33s 605ms/step - loss: 0.2757 - acc: 0.8847 - val_loss: 0.8605 - val_acc: 0.7133\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 33s 606ms/step - loss: 0.3215 - acc: 0.8580 - val_loss: 0.4362 - val_acc: 0.8125\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 33s 605ms/step - loss: 0.2671 - acc: 0.8898 - val_loss: 0.4547 - val_acc: 0.8107\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 33s 603ms/step - loss: 0.2499 - acc: 0.8943 - val_loss: 0.5932 - val_acc: 0.7969\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.2437 - acc: 0.8982 - val_loss: 0.4436 - val_acc: 0.8167\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 32s 588ms/step - loss: 0.2434 - acc: 0.8977 - val_loss: 0.4477 - val_acc: 0.8203\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 32s 578ms/step - loss: 0.2194 - acc: 0.9108 - val_loss: 0.4191 - val_acc: 0.8425\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 33s 593ms/step - loss: 0.2186 - acc: 0.9071 - val_loss: 0.4460 - val_acc: 0.8287\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 33s 605ms/step - loss: 0.2013 - acc: 0.9170 - val_loss: 0.4059 - val_acc: 0.8468\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 34s 610ms/step - loss: 0.2664 - acc: 0.8865 - val_loss: 0.4507 - val_acc: 0.8347\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 33s 599ms/step - loss: 0.2121 - acc: 0.9067 - val_loss: 0.4629 - val_acc: 0.8425\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.2132 - acc: 0.9112 - val_loss: 0.4388 - val_acc: 0.8365\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 33s 599ms/step - loss: 0.1890 - acc: 0.9203 - val_loss: 0.4409 - val_acc: 0.8425\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 33s 594ms/step - loss: 0.2014 - acc: 0.9116 - val_loss: 0.4139 - val_acc: 0.8546\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 33s 594ms/step - loss: 0.1955 - acc: 0.9180 - val_loss: 0.4091 - val_acc: 0.8582\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 32s 577ms/step - loss: 0.1774 - acc: 0.9236 - val_loss: 0.4353 - val_acc: 0.8564\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 31s 572ms/step - loss: 0.1783 - acc: 0.9226 - val_loss: 0.6213 - val_acc: 0.8197\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 32s 589ms/step - loss: 0.1887 - acc: 0.9162 - val_loss: 0.4462 - val_acc: 0.8450\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 32s 585ms/step - loss: 0.1755 - acc: 0.9263 - val_loss: 0.5317 - val_acc: 0.8293\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 33s 602ms/step - loss: 0.1836 - acc: 0.9195 - val_loss: 0.4659 - val_acc: 0.8540\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 33s 606ms/step - loss: 0.1617 - acc: 0.9339 - val_loss: 0.4828 - val_acc: 0.8450\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.1630 - acc: 0.9271 - val_loss: 0.4175 - val_acc: 0.8528\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 33s 606ms/step - loss: 0.1694 - acc: 0.9271 - val_loss: 0.3836 - val_acc: 0.8570\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 33s 606ms/step - loss: 0.1572 - acc: 0.9261 - val_loss: 0.6364 - val_acc: 0.8083\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 33s 593ms/step - loss: 0.1593 - acc: 0.9351 - val_loss: 0.4213 - val_acc: 0.8672\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.1592 - acc: 0.9331 - val_loss: 0.4530 - val_acc: 0.8462\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 32s 584ms/step - loss: 0.1446 - acc: 0.9398 - val_loss: 0.4935 - val_acc: 0.8630\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 32s 582ms/step - loss: 0.1470 - acc: 0.9406 - val_loss: 0.4291 - val_acc: 0.8660\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 32s 583ms/step - loss: 0.1421 - acc: 0.9409 - val_loss: 0.5374 - val_acc: 0.8678\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 32s 587ms/step - loss: 0.1352 - acc: 0.9409 - val_loss: 0.4171 - val_acc: 0.8738\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.1300 - acc: 0.9474 - val_loss: 0.4870 - val_acc: 0.8582\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 32s 586ms/step - loss: 0.1367 - acc: 0.9443 - val_loss: 0.3890 - val_acc: 0.8702\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 33s 597ms/step - loss: 0.1412 - acc: 0.9435 - val_loss: 0.4498 - val_acc: 0.8666\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.1617 - acc: 0.9344 - val_loss: 0.4402 - val_acc: 0.8564\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 32s 581ms/step - loss: 0.1233 - acc: 0.9479 - val_loss: 0.6721 - val_acc: 0.8552\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 33s 594ms/step - loss: 0.1366 - acc: 0.9466 - val_loss: 0.3959 - val_acc: 0.8846\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 32s 586ms/step - loss: 0.1231 - acc: 0.9490 - val_loss: 0.4785 - val_acc: 0.8612\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 32s 579ms/step - loss: 0.1217 - acc: 0.9510 - val_loss: 0.4125 - val_acc: 0.8786\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 32s 590ms/step - loss: 0.1288 - acc: 0.9477 - val_loss: 0.5896 - val_acc: 0.8606\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 31s 571ms/step - loss: 0.1218 - acc: 0.9530 - val_loss: 0.4734 - val_acc: 0.8744\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 32s 574ms/step - loss: 0.1374 - acc: 0.9473 - val_loss: 0.5112 - val_acc: 0.8726\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 33s 593ms/step - loss: 0.0982 - acc: 0.9614 - val_loss: 0.6486 - val_acc: 0.8462\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 32s 587ms/step - loss: 0.1574 - acc: 0.9372 - val_loss: 0.3548 - val_acc: 0.8762\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 32s 573ms/step - loss: 0.1153 - acc: 0.9571 - val_loss: 0.5629 - val_acc: 0.8762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "55/55 [==============================] - 33s 592ms/step - loss: 0.0889 - acc: 0.9645 - val_loss: 0.5291 - val_acc: 0.8720\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 31s 558ms/step - loss: 0.1061 - acc: 0.9551 - val_loss: 0.4972 - val_acc: 0.8768\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 31s 567ms/step - loss: 0.1145 - acc: 0.9548 - val_loss: 0.5734 - val_acc: 0.8594\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 32s 573ms/step - loss: 0.1187 - acc: 0.9530 - val_loss: 0.4890 - val_acc: 0.8810\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 32s 580ms/step - loss: 0.0853 - acc: 0.9673 - val_loss: 0.6290 - val_acc: 0.8654\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 32s 586ms/step - loss: 0.0995 - acc: 0.9614 - val_loss: 0.4528 - val_acc: 0.8786\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 32s 579ms/step - loss: 0.1013 - acc: 0.9622 - val_loss: 0.6175 - val_acc: 0.8672\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.960750\n",
      "ACC: 0.923971\n",
      "MCC : 0.851927\n",
      "TPR:0.972272\n",
      "FPR:0.124329\n",
      "Pre:0.886623\n",
      "F1:0.927474\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.964872\n",
      "mean ACC: 0.927593\n",
      "mean MCC : 0.859646\n",
      "mean TPR:0.978186\n",
      "mean FPR:0.122989\n",
      "mean Pre:0.888412\n",
      "mean F1:0.931112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "dataset_name = 'yeast'\n",
    "for rep in range(1):\n",
    "    n_splits = 5\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "    skf = StratifiedKFold(n_splits=n_splits)\n",
    "    count = 0\n",
    "    for split in range(0,5):\n",
    "        train_pairs_file = 'yeast_data/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'yeast_data/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'yeast_data/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 128\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'yeast_data/node_seq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=15, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                   validation_data=valid_generator, \n",
    "                   epochs = 100,verbose=1,callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "#     np.savez('yeast_go_seq'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('yeast_seq_node2vec'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
