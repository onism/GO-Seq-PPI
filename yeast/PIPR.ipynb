{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM, Bidirectional, BatchNormalization, add,  Input,CuDNNGRU \n",
    "from keras.layers.core import Flatten, Reshape\n",
    "from keras.layers.merge import Concatenate, concatenate, subtract, multiply\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, AveragePooling1D, GlobalAveragePooling1D\n",
    "\n",
    "from keras.optimizers import Adam,  RMSprop\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    seq_input1 = Input(shape=(seq_size, dim), name='seq1')\n",
    "    seq_input2 = Input(shape=(seq_size, dim), name='seq2')\n",
    "    l1=Conv1D(hidden_dim, 3)\n",
    "    r1=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    l2=Conv1D(hidden_dim, 3)\n",
    "    r2=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    l3=Conv1D(hidden_dim, 3)\n",
    "    r3=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    l4=Conv1D(hidden_dim, 3)\n",
    "    r4=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    l5=Conv1D(hidden_dim, 3)\n",
    "    r5=Bidirectional(CuDNNGRU(hidden_dim, return_sequences=True))\n",
    "    l6=Conv1D(hidden_dim, 3)\n",
    "    s1=MaxPooling1D(3)(l1(seq_input1))\n",
    "    s1=concatenate([r1(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l2(s1))\n",
    "    s1=concatenate([r2(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l3(s1))\n",
    "    s1=concatenate([r3(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l4(s1))\n",
    "    s1=concatenate([r4(s1), s1])\n",
    "    s1=MaxPooling1D(3)(l5(s1))\n",
    "    s1=concatenate([r5(s1), s1])\n",
    "    s1=l6(s1)\n",
    "    s1=GlobalAveragePooling1D()(s1)\n",
    "    s2=MaxPooling1D(3)(l1(seq_input2))\n",
    "    s2=concatenate([r1(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l2(s2))\n",
    "    s2=concatenate([r2(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l3(s2))\n",
    "    s2=concatenate([r3(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l4(s2))\n",
    "    s2=concatenate([r4(s2), s2])\n",
    "    s2=MaxPooling1D(3)(l5(s2))\n",
    "    s2=concatenate([r5(s2), s2])\n",
    "    s2=l6(s2)\n",
    "    s2=GlobalAveragePooling1D()(s2)\n",
    "    merge_text = multiply([s1, s2])\n",
    "    x = Dense(100, activation='linear')(merge_text)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = Dense(int((hidden_dim+7)/2), activation='linear')(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    main_output = Dense(2, activation='softmax')(x)\n",
    "    merge_model = Model(inputs=[seq_input1, seq_input2], outputs=[main_output])\n",
    "    return merge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "seq_size = 2000\n",
    "protein2seq = load_dict('yeast_data/protein2seq_dicts.pkl')\n",
    "protein2onehot = {}\n",
    "for key, value in protein2seq.items():\n",
    "    protein2onehot[key] =  protein_one_hot(value, seq_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7160/7160 [==============================] - 18s 3ms/step - loss: 0.6728 - acc: 0.6214\n",
      "Epoch 2/10\n",
      "7160/7160 [==============================] - 13s 2ms/step - loss: 0.6614 - acc: 0.6258\n",
      "Epoch 3/10\n",
      "7160/7160 [==============================] - 13s 2ms/step - loss: 0.6618 - acc: 0.6258\n",
      "Epoch 4/10\n",
      "7160/7160 [==============================] - 14s 2ms/step - loss: 0.6605 - acc: 0.6258\n",
      "Epoch 5/10\n",
      "7160/7160 [==============================] - 14s 2ms/step - loss: 0.6608 - acc: 0.6258\n",
      "Epoch 6/10\n",
      "7160/7160 [==============================] - 14s 2ms/step - loss: 0.6606 - acc: 0.6258\n",
      "Epoch 7/10\n",
      "7160/7160 [==============================] - 15s 2ms/step - loss: 0.6617 - acc: 0.6258\n",
      "Epoch 8/10\n",
      "7160/7160 [==============================] - 14s 2ms/step - loss: 0.6609 - acc: 0.6258\n",
      "Epoch 9/10\n",
      "7160/7160 [==============================] - 14s 2ms/step - loss: 0.6607 - acc: 0.6258\n",
      "Epoch 10/10\n",
      "7160/7160 [==============================] - 14s 2ms/step - loss: 0.6595 - acc: 0.6258\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ba80c7336cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_true_pos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_true_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_false_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_true_pos\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_true_neg\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_true_neg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_false_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprec\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mmcc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_true_pos\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_true_neg\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnum_false_pos\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_false_neg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_true_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_true_neg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_true_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_false_neg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_false_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_true_neg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_false_pos\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_false_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "n_splits = 5\n",
    "TPRs =  np.zeros(n_splits)\n",
    "FPRs = np.zeros(n_splits)\n",
    "Precs = np.zeros(n_splits)\n",
    "ACCs = np.zeros(n_splits)\n",
    "F1s = np.zeros(n_splits)\n",
    "MCCs = np.zeros(n_splits)\n",
    "AUCs = np.zeros(n_splits)\n",
    "count = 0\n",
    "\n",
    "\n",
    "#copy below\n",
    "num_hit = 0.\n",
    "num_total = 0.\n",
    "num_pos = 0.\n",
    "num_true_pos = 0.\n",
    "num_false_pos = 0.\n",
    "num_true_neg = 0.\n",
    "num_false_neg = 0.\n",
    "rep = 0\n",
    "class_map = {'0':1,'1':0}    \n",
    "\n",
    "    \n",
    "for split in range(0,5):\n",
    "    train_pairs_file = 'yeast_data/train'+str(rep)+'-'+str(split)\n",
    "    test_pairs_file = 'yeast_data/test'+str(rep)+'-'+str(split)\n",
    "    valid_pairs_file = 'yeast_data/valid'+str(rep)+'-'+str(split)\n",
    "    \n",
    "    with open(train_pairs_file) as infile:\n",
    "        train_ppis = infile.readlines()\n",
    "        \n",
    "    with open(test_pairs_file) as infile:\n",
    "        test_ppis = infile.readlines()\n",
    "        \n",
    "    with open(valid_pairs_file) as infile:\n",
    "        valid_ppis = infile.readlines()\n",
    "    \n",
    "    train_class_labels = np.zeros((len(train_ppis), 2))\n",
    "    train_seq1 = np.zeros((len(train_ppis), seq_size, 20 ))\n",
    "    train_seq2 = np.zeros((len(train_ppis), seq_size, 20 ))\n",
    "    for i, line in  enumerate(train_ppis) :     \n",
    "        p1, p2, label = line.rstrip().split('\\t') \n",
    "        train_class_labels[i][class_map[label]] = 1\n",
    "        train_seq1[i] = protein2onehot[p1]\n",
    "        train_seq2[i] = protein2onehot[p1]\n",
    "        \n",
    "        \n",
    "#     valid_class_labels = np.zeros((len(valid_ppis), 2))\n",
    "#     valid_seq1 = np.zeros((len(valid_ppis), seq_size, 20 ))\n",
    "#     valid_seq2 = np.zeros((len(valid_ppis), seq_size, 20 ))\n",
    "#     for i, line in  enumerate(valid_ppis) :     \n",
    "#         p1, p2, label = line.rstrip().split('\\t') \n",
    "#         valid_class_labels[i][class_map[label]] = 1\n",
    "#         valid_seq1[i] = protein2onehot[p1]\n",
    "#         valid_seq2[i] = protein2onehot[p1]\n",
    "        \n",
    "    \n",
    "    test_class_labels = np.zeros((len(test_ppis), 2))\n",
    "    test_seq1 = np.zeros((len(test_ppis), seq_size, 20 ))\n",
    "    test_seq2 = np.zeros((len(test_ppis), seq_size, 20 ))\n",
    "    for i, line in  enumerate(test_ppis) :     \n",
    "        p1, p2, label = line.rstrip().split('\\t') \n",
    "        test_class_labels[i][class_map[label]] = 1\n",
    "        test_seq1[i] = protein2onehot[p1]\n",
    "        test_seq2[i] = protein2onehot[p1]\n",
    "    \n",
    "    batch_size1 = 256\n",
    "    adam = Adam(lr=0.001, amsgrad=True, epsilon=1e-6)\n",
    "    rms = RMSprop(lr=0.001)\n",
    "    \n",
    "    dim = 20\n",
    "    hidden_dim = 50\n",
    "    merge_model = None\n",
    "    merge_model = build_model()\n",
    "    adam = Adam(lr=0.001, amsgrad=True)\n",
    "    rms = RMSprop(lr=0.001)\n",
    "\n",
    "    merge_model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    merge_model.fit([ train_seq1, train_seq2], train_class_labels, batch_size=batch_size1, epochs=10)\n",
    "    #result1 = merge_model.evaluate([seq_tensor1[test], seq_tensor2[test]], class_labels[test])\n",
    "    pred = merge_model.predict([test_seq1, test_seq2])\n",
    "    y_test = np.zeros((test_class_labels.shape[0],1))\n",
    "    y_pred = np.zeros((test_class_labels.shape[0],1))\n",
    "    for i in range( test_class_labels.shape[0]):\n",
    "        y_test[i] = np.argmax(test_class_labels[i])\n",
    "        y_pred[i] = np.argmax(pred[i])\n",
    "        num_total += 1\n",
    "        if np.argmax(test_class_labels[i]) == np.argmax(pred[i]):\n",
    "            num_hit += 1\n",
    "        if train_class_labels[i][0] > 0.:\n",
    "            num_pos += 1.\n",
    "            if pred[i][0] > pred[i][1]:\n",
    "                num_true_pos += 1\n",
    "            else:\n",
    "                num_false_neg += 1\n",
    "        else:\n",
    "            if pred[i][0] > pred[i][1]:\n",
    "                num_false_pos += 1\n",
    "            else:\n",
    "                num_true_neg += 1\n",
    "    accuracy = num_hit / num_total\n",
    "    prec = num_true_pos / (num_true_pos + num_false_pos)\n",
    "    recall = num_true_pos / num_pos\n",
    "    spec = num_true_neg / (num_true_neg + num_false_neg)\n",
    "    f1 = 2. * prec * recall / (prec + recall)\n",
    "    mcc = (num_true_pos * num_true_neg - num_false_pos * num_false_neg) / ((num_true_pos + num_true_neg) * (num_true_pos + num_false_neg) * (num_false_pos + num_true_neg) * (num_false_pos + num_false_neg)) ** 0.5\n",
    "    print (accuracy, prec, recall, spec, f1, mcc)\n",
    "    \n",
    "     \n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    pre = precision_score(y_test, y_pred)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    total=tn+fp+fn+tp\n",
    "    sen = float(tp)/float(tp+fn)\n",
    "    sps = float(tn)/float((tn+fp))\n",
    "\n",
    "    tpr = float(tp)/float(tp+fn)\n",
    "    fpr = float(fp)/float((tn+fp))\n",
    "    print('--------------------------\\n')\n",
    "    print ('AUC: %f' % auc)\n",
    "    print ('ACC: %f' % acc) \n",
    "    # print(\"PRAUC: %f\" % pr_auc)\n",
    "    print ('MCC : %f' % mcc)\n",
    "    # print ('SEN: %f' % sen)\n",
    "    # print ('SEP: %f' % sps)\n",
    "    print('TPR:%f'%tpr)\n",
    "    print('FPR:%f'%fpr)\n",
    "    print('Pre:%f'%pre)\n",
    "    print('F1:%f'%f1)\n",
    "    print('--------------------------\\n')\n",
    "    TPRs[count] = tpr\n",
    "    FPRs[count] = fpr\n",
    "    Precs[count] =pre\n",
    "    ACCs[count] =acc\n",
    "    F1s[count] =f1\n",
    "    MCCs[count] =mcc\n",
    "    AUCs[count] =auc\n",
    "    count += 1\n",
    "\n",
    "accuracy = num_hit / num_total\n",
    "prec = num_true_pos / (num_true_pos + num_false_pos)\n",
    "recall = num_true_pos / num_pos\n",
    "spec = num_true_neg / (num_true_neg + num_false_neg)\n",
    "f1 = 2. * prec * recall / (prec + recall)\n",
    "mcc = (num_true_pos * num_true_neg - num_false_pos * num_false_neg) / ((num_true_pos + num_true_neg) * (num_true_pos + num_false_neg) * (num_false_pos + num_true_neg) * (num_false_pos + num_false_neg)) ** 0.5\n",
    "print (accuracy, prec, recall, f1)\n",
    " \n",
    "    \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
