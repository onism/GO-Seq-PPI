{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 128\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "protein2seq = load_dict('SC_protein_seqs.pkl')\n",
    "\n",
    "prot2emb = {}\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,768))\n",
    "    allgos = value.split(';') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "        if count + feature.shape[0] > max_go_len:\n",
    "            break\n",
    "        X_go1 = np.concatenate((X_go1,feature ))    \n",
    "        count += feature.shape[0]\n",
    "    prot2emb[key] =  X_go1[1:] \n",
    "protein2onehot = {}\n",
    "for key, value in protein2seq.items():\n",
    "    protein2onehot[key] =  protein_one_hot(value, max_seq_len)\n",
    "        \n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "        self.protein2seq = load_dict('SC_protein_seqs.pkl')\n",
    "        self.read_ppi()\n",
    "        self.protein2onehot = protein2onehot\n",
    "        self.prot2emb = prot2emb\n",
    "#         self.onehot_seqs()\n",
    "#         self.prot2embedding() \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        y = np.empty((self.batch_size))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "\n",
    "        return [X_go1,X_go2, X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    " \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "        return [X_go1,X_go2, X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 128, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 128, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 128, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 64)      20544       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 128, 64)      12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 128, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 128, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 128, 64)      20544       conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 128, 64)      12352       conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 128, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 128, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 16)     1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 16)     784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 16)     1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 16)     784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 256)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 128)     320256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 256)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 128)     320256      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 64)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 64)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 256)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 256)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000, 64)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 64)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          384         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          256         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          384         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          256         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           1064        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           1064        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1152)         0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1152)         0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 576)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 576)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          295168      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          295168      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          147712      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          147712      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            513         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,084,641\n",
      "Trainable params: 5,084,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(4)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([x_a, x_b, x_c, x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "     \n",
    " \n",
    "     \n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters//4, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters//4, gru_units)\n",
    "     \n",
    "    \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go, left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    optimizer = Lookahead(RAdam())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('SC_CV/sc_GoplusSeq1-0.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 1\n",
    "split = 0\n",
    "test_pairs_file = 'SC_CV/test'+str(rep)+'-'+str(split)\n",
    "batch_size = 256\n",
    "         \n",
    "test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1_output = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('concatenate_6').output)\n",
    "seq2_output = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('concatenate_8').output)\n",
    "\n",
    "go1_output = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('concatenate_2').output)\n",
    "go2_output = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('concatenate_4').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_pairs_file, 'r') as f:\n",
    "    test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "test_len = len(test_ppi_pairs) \n",
    "list_IDs_temp = np.arange(test_len)\n",
    "test_x, y_test = test_generator.all_data(list_IDs_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1_feature = seq1_output.predict(test_x)\n",
    "seq2_feature = seq2_output.predict(test_x)\n",
    "go1_feature = go1_output.predict(test_x)\n",
    "go2_feature = go2_output.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8628, 576) (8628, 576) (8628, 1152) (8628, 1152)\n"
     ]
    }
   ],
   "source": [
    "print(seq1_feature.shape, seq2_feature.shape, go1_feature.shape, go2_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer('dense_7').output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 128, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 128, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 128, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 64)      20544       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 128, 64)      12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 128, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 128, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 128, 64)      20544       conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 128, 64)      12352       conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 128, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 128, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 16)     1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 16)     784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 16)     1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 16)     784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 256)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 128)     320256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 256)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 128)     320256      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 64)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 64)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 256)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 256)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000, 64)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 64)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          384         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          256         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          384         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          256         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           1064        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           1064        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1152)         0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1152)         0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 576)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 576)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          295168      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          295168      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          147712      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          147712      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1024)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 5,084,128\n",
      "Trainable params: 5,084,128\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "intermediate_layer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_pairs_file, 'r') as f:\n",
    "    test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "test_len = len(test_ppi_pairs) \n",
    "list_IDs_temp = np.arange(test_len)\n",
    "test_x, y_test = test_generator.all_data(list_IDs_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = intermediate_layer_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8628, 512)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, random_state=0, verbose=1, perplexity=40, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 121 nearest neighbors...\n",
      "[t-SNE] Indexed 8628 samples in 0.268s...\n",
      "[t-SNE] Computed neighbors for 8628 samples in 15.396s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 8628\n",
      "[t-SNE] Computed conditional probabilities for sample 8628 / 8628\n",
      "[t-SNE] Mean sigma: 0.384193\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 70.444397\n",
      "[t-SNE] KL divergence after 1000 iterations: 1.133756\n"
     ]
    }
   ],
   "source": [
    "X_2d = tsne.fit_transform(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "colors = 'r',   'b'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd1ab89a198>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "for i, c, label in zip([0,1], colors, ['Positive', 'Negative']):\n",
    "    plt.scatter(X_2d[y_test == i, 0], X_2d [ y_test == i, 1], c=c, label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
