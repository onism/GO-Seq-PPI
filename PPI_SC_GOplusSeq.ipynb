{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 128\n",
    "max_seq_len = 1000\n",
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    "\n",
    "protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "protein2seq = load_dict('SC_protein_seqs.pkl')\n",
    "\n",
    "prot2emb = {}\n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.zeros((1,768))\n",
    "    allgos = value.split(';') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "        if count + feature.shape[0] > max_go_len:\n",
    "            break\n",
    "        X_go1 = np.concatenate((X_go1,feature ))    \n",
    "        count += feature.shape[0]\n",
    "    prot2emb[key] =  X_go1[1:] \n",
    "protein2onehot = {}\n",
    "for key, value in protein2seq.items():\n",
    "    protein2onehot[key] =  protein_one_hot(value, max_seq_len)\n",
    "        \n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "        self.protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "        self.protein2seq = load_dict('SC_protein_seqs.pkl')\n",
    "        self.read_ppi()\n",
    "        self.protein2onehot = protein2onehot\n",
    "        self.prot2emb = prot2emb\n",
    "#         self.onehot_seqs()\n",
    "#         self.prot2embedding() \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "#     def onehot_seqs(self):\n",
    "#         for key, value in self.protein2seq.items():\n",
    "#             self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "    \n",
    "#     def prot2embedding(self):\n",
    "#         for key, value in self.protein2go.items():\n",
    "#             X_go1 =  np.zeros((1,768))\n",
    "#             allgos = value.split(';') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > max_go_len:\n",
    "#                     break\n",
    "#                 X_go1 = np.concatenate((X_go1,feature ))    \n",
    "#                 count += feature.shape[0]\n",
    "#             self.prot2emb[key] =  X_go1[1:]   \n",
    "            \n",
    "    \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "\n",
    "        X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        y = np.empty((self.batch_size))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "#             X_go1[i] =  np.load('SC_GO/'+p1+'.npy') \n",
    "#             X_go2[i] =  np.load('SC_GO/'+p2+'.npy')\n",
    "             \n",
    "#             values = self.protein2go[ p1 ]\n",
    "            \n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go1[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "\n",
    "#             values = self.protein2go[ p2]\n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos)) \n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go2[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "        return [X_go1,X_go2, X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split(',')\n",
    "            if label == '1':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            \n",
    " \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            prot1emb = self.prot2emb[p1]\n",
    "            X_go1[i,:prot1emb.shape[0]] = prot1emb\n",
    "            \n",
    "            prot2emb = self.prot2emb[p2]\n",
    "            X_go2[i,:prot2emb.shape[0]] = prot2emb\n",
    "            \n",
    "#             X_go1[i] =  np.load('SC_GO/'+p1+'.npy') \n",
    "#             X_go2[i] =  np.load('SC_GO/'+p2+'.npy')\n",
    "#             values = self.protein2go[ p1 ]\n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go1[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "\n",
    "#             values = self.protein2go[ p2]\n",
    "#             allgos = values.split(';') \n",
    "#             allgos = list(set(allgos)) \n",
    "#             count = 0\n",
    "#             for  go in  allgos:\n",
    "#                 feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                 if count + feature.shape[0] > self.max_golen:\n",
    "#                     break\n",
    "#                 X_go2[i, count:count+feature.shape[0]] = feature\n",
    "#                 count += feature.shape[0]\n",
    "        return [X_go1,X_go2, X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protein2go =  load_dict('SC_protein2go_dicts.pkl')\n",
    "# for key, value in protein2go.items():\n",
    "#     X_go1 =  np.zeros((1,768))\n",
    "#     allgos = value.split(';') \n",
    "#     allgos = list(set(allgos))\n",
    "#     count = 0\n",
    "#     for  go in  allgos:\n",
    "#         feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#         if count + feature.shape[0] > max_go_len:\n",
    "#             break\n",
    "#         X_go1 = np.concatenate((X_go1,feature ))    \n",
    "#         count += feature.shape[0]\n",
    "# #     np.save('SC_GO/'+key,X_go1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_go1 =  np.zeros((1,768))\n",
    "# allgos = value.split(';') \n",
    "# allgos = list(set(allgos))\n",
    "# count = 0\n",
    "# for  go in  allgos:\n",
    "#     feature = np.load('ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#     if count + feature.shape[0] > max_go_len:\n",
    "#         break\n",
    "#     X_go1 = np.concatenate((X_go1,feature ))    \n",
    "#     count += feature.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 128, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 128, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 128, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 128, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 128, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 128, 64)      20544       conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 128, 64)      12352       conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 128, 64)      147520      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 128, 64)      49216       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 128, 64)      20544       conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 128, 64)      12352       conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 128, 64)      147520      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 128, 64)      49216       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 1000, 16)     1296        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 1000, 16)     784         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 1000, 16)     976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 1000, 16)     336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 1000, 16)     1296        conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 1000, 16)     784         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 1000, 16)     976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 1000, 16)     336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 128, 256)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 128, 128)     320256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128, 256)     0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 128)     320256      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1000, 64)     0           conv1d_14[0][0]                  \n",
      "                                                                 conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 1000, 128)    33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1000, 64)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 1000, 128)    33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128, 256)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128, 256)     0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1000, 64)     0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1000, 128)    0           bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1000, 64)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1000, 128)    0           bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          384         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          256         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 256)          384         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 128)          256         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 64)           1064        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 128)          1128        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_7 (Glo (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 64)           0           dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 64)           1064        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          1128        dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1152)         0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1152)         0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 576)          0           global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "                                                                 global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 576)          0           global_average_pooling1d_7[0][0] \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 attention_7[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          295168      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          295168      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          147712      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          147712      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 1024)         0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1024)         1049600     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1024)         0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1024)         1049600     dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 1024)         0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          524800      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            513         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,084,641\n",
      "Trainable params: 5,084,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(4)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([x_a, x_b, x_c, x_gru_a, x_gru_b,   x_gru_c])\n",
    "    x = Dense(256,activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "     \n",
    " \n",
    "     \n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters//4, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters//4, gru_units)\n",
    "     \n",
    "    \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go, left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    optimizer = Lookahead(RAdam())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !rm -rf  SC_CV\n",
    "# !mkdir SC_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppi_pairs[1].rstrip().split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "90/90 [==============================] - 73s 815ms/step - loss: 0.5446 - acc: 0.7200 - val_loss: 0.3952 - val_acc: 0.8337\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.3267 - acc: 0.8607 - val_loss: 0.2628 - val_acc: 0.9039\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.2294 - acc: 0.9087 - val_loss: 0.2217 - val_acc: 0.9157\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.1938 - acc: 0.9234 - val_loss: 0.1732 - val_acc: 0.9408\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 60s 668ms/step - loss: 0.1569 - acc: 0.9407 - val_loss: 0.1546 - val_acc: 0.9433\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.1376 - acc: 0.9495 - val_loss: 0.1568 - val_acc: 0.9433\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 60s 668ms/step - loss: 0.1603 - acc: 0.9377 - val_loss: 0.1564 - val_acc: 0.9434\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.1114 - acc: 0.9594 - val_loss: 0.1527 - val_acc: 0.9435\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 60s 669ms/step - loss: 0.1083 - acc: 0.9591 - val_loss: 0.1475 - val_acc: 0.9479\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.1141 - acc: 0.9569 - val_loss: 0.1486 - val_acc: 0.9464\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 60s 665ms/step - loss: 0.1027 - acc: 0.9627 - val_loss: 0.1381 - val_acc: 0.9517\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0931 - acc: 0.9658 - val_loss: 0.1271 - val_acc: 0.9569\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0968 - acc: 0.9642 - val_loss: 0.1437 - val_acc: 0.9463\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0812 - acc: 0.9703 - val_loss: 0.1330 - val_acc: 0.9532\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0731 - acc: 0.9741 - val_loss: 0.1555 - val_acc: 0.9441\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0760 - acc: 0.9712 - val_loss: 0.1300 - val_acc: 0.9543\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0760 - acc: 0.9720 - val_loss: 0.1358 - val_acc: 0.9515\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 58s 648ms/step - loss: 0.0686 - acc: 0.9751 - val_loss: 0.1409 - val_acc: 0.9508\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 58s 649ms/step - loss: 0.0787 - acc: 0.9707 - val_loss: 0.1443 - val_acc: 0.9479\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 58s 650ms/step - loss: 0.0678 - acc: 0.9742 - val_loss: 0.1288 - val_acc: 0.9553\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 58s 648ms/step - loss: 0.0793 - acc: 0.9705 - val_loss: 0.3299 - val_acc: 0.8506\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 58s 647ms/step - loss: 0.0908 - acc: 0.9658 - val_loss: 0.1630 - val_acc: 0.9418\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 59s 652ms/step - loss: 0.0629 - acc: 0.9770 - val_loss: 0.1564 - val_acc: 0.9506\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 58s 648ms/step - loss: 0.0608 - acc: 0.9779 - val_loss: 0.1378 - val_acc: 0.9492\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0527 - acc: 0.9805 - val_loss: 0.1535 - val_acc: 0.9518\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0560 - acc: 0.9791 - val_loss: 0.1301 - val_acc: 0.9561\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0623 - acc: 0.9763 - val_loss: 0.1312 - val_acc: 0.9554\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 61s 672ms/step - loss: 0.0489 - acc: 0.9813 - val_loss: 0.1445 - val_acc: 0.9544\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0564 - acc: 0.9785 - val_loss: 0.1329 - val_acc: 0.9547\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0527 - acc: 0.9806 - val_loss: 0.1382 - val_acc: 0.9544\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0458 - acc: 0.9830 - val_loss: 0.1379 - val_acc: 0.9568\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0396 - acc: 0.9854 - val_loss: 0.1432 - val_acc: 0.9545\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.989100\n",
      "ACC: 0.958507\n",
      "MCC : 0.917747\n",
      "TPR:0.938096\n",
      "FPR:0.021242\n",
      "Pre:0.977686\n",
      "F1:0.957482\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.989100\n",
      "mean ACC: 0.958507\n",
      "mean MCC : 0.917747\n",
      "mean TPR:0.938096\n",
      "mean FPR:0.021242\n",
      "mean Pre:0.977686\n",
      "mean F1:0.957482\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 72s 796ms/step - loss: 0.5518 - acc: 0.7057 - val_loss: 0.3807 - val_acc: 0.8415\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.3454 - acc: 0.8517 - val_loss: 0.2744 - val_acc: 0.8993\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.2262 - acc: 0.9119 - val_loss: 0.2010 - val_acc: 0.9271\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.1907 - acc: 0.9262 - val_loss: 0.1768 - val_acc: 0.9419\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.1544 - acc: 0.9441 - val_loss: 0.2329 - val_acc: 0.9111\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.1424 - acc: 0.9478 - val_loss: 0.1471 - val_acc: 0.9479\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.1369 - acc: 0.9490 - val_loss: 0.1642 - val_acc: 0.9394\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.1201 - acc: 0.9553 - val_loss: 0.1375 - val_acc: 0.9509\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 60s 666ms/step - loss: 0.1195 - acc: 0.9554 - val_loss: 0.1487 - val_acc: 0.9429\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 60s 668ms/step - loss: 0.1297 - acc: 0.9518 - val_loss: 0.1287 - val_acc: 0.9551\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0920 - acc: 0.9665 - val_loss: 0.1521 - val_acc: 0.9421\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0931 - acc: 0.9657 - val_loss: 0.1364 - val_acc: 0.9528\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.1006 - acc: 0.9623 - val_loss: 0.1373 - val_acc: 0.9531\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 60s 669ms/step - loss: 0.0986 - acc: 0.9634 - val_loss: 0.1448 - val_acc: 0.9499\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 60s 672ms/step - loss: 0.0846 - acc: 0.9691 - val_loss: 0.1257 - val_acc: 0.9564\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 60s 669ms/step - loss: 0.0763 - acc: 0.9711 - val_loss: 0.1433 - val_acc: 0.9508\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.0840 - acc: 0.9691 - val_loss: 0.1456 - val_acc: 0.9476\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0735 - acc: 0.9716 - val_loss: 0.1441 - val_acc: 0.9504\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0809 - acc: 0.9683 - val_loss: 0.1372 - val_acc: 0.9536\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0687 - acc: 0.9738 - val_loss: 0.1251 - val_acc: 0.9569\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0750 - acc: 0.9722 - val_loss: 0.1340 - val_acc: 0.9541\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 60s 669ms/step - loss: 0.0620 - acc: 0.9762 - val_loss: 0.1432 - val_acc: 0.9467\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 60s 672ms/step - loss: 0.0663 - acc: 0.9754 - val_loss: 0.1588 - val_acc: 0.9516\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0726 - acc: 0.9711 - val_loss: 0.1394 - val_acc: 0.9538\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 61s 680ms/step - loss: 0.0568 - acc: 0.9768 - val_loss: 0.1338 - val_acc: 0.9554\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0540 - acc: 0.9792 - val_loss: 0.1436 - val_acc: 0.9457\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0573 - acc: 0.9780 - val_loss: 0.3690 - val_acc: 0.8918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "90/90 [==============================] - 60s 668ms/step - loss: 0.0769 - acc: 0.9695 - val_loss: 0.1516 - val_acc: 0.9541\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0442 - acc: 0.9821 - val_loss: 0.1545 - val_acc: 0.9535\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 62s 684ms/step - loss: 0.0442 - acc: 0.9822 - val_loss: 0.1439 - val_acc: 0.9556\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.0503 - acc: 0.9801 - val_loss: 0.1413 - val_acc: 0.9553\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0457 - acc: 0.9829 - val_loss: 0.1661 - val_acc: 0.9516\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0512 - acc: 0.9807 - val_loss: 0.1460 - val_acc: 0.9553\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.0388 - acc: 0.9852 - val_loss: 0.1497 - val_acc: 0.9562\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0443 - acc: 0.9825 - val_loss: 0.1769 - val_acc: 0.9515\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 61s 680ms/step - loss: 0.0370 - acc: 0.9861 - val_loss: 0.1635 - val_acc: 0.9529\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 61s 672ms/step - loss: 0.0413 - acc: 0.9840 - val_loss: 0.1690 - val_acc: 0.9538\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0351 - acc: 0.9855 - val_loss: 0.1953 - val_acc: 0.9454\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 60s 672ms/step - loss: 0.0399 - acc: 0.9848 - val_loss: 0.1617 - val_acc: 0.9562\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0391 - acc: 0.9858 - val_loss: 0.1802 - val_acc: 0.9545\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.989187\n",
      "ACC: 0.958855\n",
      "MCC : 0.917929\n",
      "TPR:0.948952\n",
      "FPR:0.030896\n",
      "Pre:0.969499\n",
      "F1:0.959116\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.989187\n",
      "mean ACC: 0.958855\n",
      "mean MCC : 0.917929\n",
      "mean TPR:0.948952\n",
      "mean FPR:0.030896\n",
      "mean Pre:0.969499\n",
      "mean F1:0.959116\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 74s 822ms/step - loss: 0.5492 - acc: 0.7150 - val_loss: 0.4026 - val_acc: 0.8259\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.3408 - acc: 0.8545 - val_loss: 0.2548 - val_acc: 0.9015\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.2540 - acc: 0.8982 - val_loss: 0.2039 - val_acc: 0.9237\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 60s 672ms/step - loss: 0.1901 - acc: 0.9259 - val_loss: 0.2665 - val_acc: 0.8906\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.1623 - acc: 0.9390 - val_loss: 0.1461 - val_acc: 0.9471\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 60s 665ms/step - loss: 0.1433 - acc: 0.9450 - val_loss: 0.1640 - val_acc: 0.9414\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 58s 647ms/step - loss: 0.1368 - acc: 0.9498 - val_loss: 0.1395 - val_acc: 0.9476\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 58s 645ms/step - loss: 0.1291 - acc: 0.9523 - val_loss: 0.1716 - val_acc: 0.9336\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 58s 644ms/step - loss: 0.1280 - acc: 0.9532 - val_loss: 0.2233 - val_acc: 0.9053\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 58s 646ms/step - loss: 0.1020 - acc: 0.9627 - val_loss: 0.1373 - val_acc: 0.9491\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 58s 645ms/step - loss: 0.1001 - acc: 0.9639 - val_loss: 0.1260 - val_acc: 0.9558\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 58s 647ms/step - loss: 0.1026 - acc: 0.9617 - val_loss: 0.1793 - val_acc: 0.9295\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 58s 647ms/step - loss: 0.0914 - acc: 0.9665 - val_loss: 0.2018 - val_acc: 0.9212\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 58s 645ms/step - loss: 0.0851 - acc: 0.9684 - val_loss: 0.1293 - val_acc: 0.9511\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 58s 646ms/step - loss: 0.0791 - acc: 0.9719 - val_loss: 0.1532 - val_acc: 0.9454\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 58s 642ms/step - loss: 0.0838 - acc: 0.9682 - val_loss: 0.1426 - val_acc: 0.9472\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 58s 647ms/step - loss: 0.0689 - acc: 0.9759 - val_loss: 0.1233 - val_acc: 0.9542\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 60s 663ms/step - loss: 0.0646 - acc: 0.9762 - val_loss: 0.1359 - val_acc: 0.9498\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0784 - acc: 0.9713 - val_loss: 0.1243 - val_acc: 0.9549\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0873 - acc: 0.9661 - val_loss: 0.1469 - val_acc: 0.9487\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0642 - acc: 0.9774 - val_loss: 0.1359 - val_acc: 0.9515\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0574 - acc: 0.9781 - val_loss: 0.1386 - val_acc: 0.9497\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.1027 - acc: 0.9612 - val_loss: 0.1235 - val_acc: 0.9570\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 60s 662ms/step - loss: 0.0596 - acc: 0.9800 - val_loss: 0.1218 - val_acc: 0.9574\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0535 - acc: 0.9812 - val_loss: 0.1245 - val_acc: 0.9582\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 60s 666ms/step - loss: 0.0539 - acc: 0.9798 - val_loss: 0.1592 - val_acc: 0.9416\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 60s 669ms/step - loss: 0.0548 - acc: 0.9808 - val_loss: 0.1266 - val_acc: 0.9544\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0523 - acc: 0.9803 - val_loss: 0.2059 - val_acc: 0.9412\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 60s 668ms/step - loss: 0.0496 - acc: 0.9815 - val_loss: 0.1313 - val_acc: 0.9568\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0412 - acc: 0.9848 - val_loss: 0.1277 - val_acc: 0.9589\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0435 - acc: 0.9839 - val_loss: 0.1463 - val_acc: 0.9536\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0466 - acc: 0.9825 - val_loss: 0.1502 - val_acc: 0.9551\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 60s 669ms/step - loss: 0.0421 - acc: 0.9846 - val_loss: 0.1348 - val_acc: 0.9560\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0604 - acc: 0.9777 - val_loss: 0.1337 - val_acc: 0.9530\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0373 - acc: 0.9861 - val_loss: 0.1512 - val_acc: 0.9537\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0329 - acc: 0.9876 - val_loss: 0.1427 - val_acc: 0.9570\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.0371 - acc: 0.9858 - val_loss: 0.1796 - val_acc: 0.9469\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0429 - acc: 0.9839 - val_loss: 0.1381 - val_acc: 0.9561\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0382 - acc: 0.9856 - val_loss: 0.1364 - val_acc: 0.9568\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0329 - acc: 0.9867 - val_loss: 0.1679 - val_acc: 0.9555\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0349 - acc: 0.9862 - val_loss: 0.1494 - val_acc: 0.9575\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0293 - acc: 0.9895 - val_loss: 0.1850 - val_acc: 0.9480\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0393 - acc: 0.9858 - val_loss: 0.1444 - val_acc: 0.9532\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0280 - acc: 0.9898 - val_loss: 0.2165 - val_acc: 0.9470\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0347 - acc: 0.9872 - val_loss: 0.1684 - val_acc: 0.9541\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.0309 - acc: 0.9896 - val_loss: 0.1884 - val_acc: 0.9550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100\n",
      "90/90 [==============================] - 60s 672ms/step - loss: 0.0236 - acc: 0.9911 - val_loss: 0.1780 - val_acc: 0.9516\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 60s 672ms/step - loss: 0.0269 - acc: 0.9908 - val_loss: 0.1682 - val_acc: 0.9545\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0250 - acc: 0.9911 - val_loss: 0.1593 - val_acc: 0.9553\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.0238 - acc: 0.9911 - val_loss: 0.1652 - val_acc: 0.9545\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.986054\n",
      "ACC: 0.953871\n",
      "MCC : 0.908010\n",
      "TPR:0.941012\n",
      "FPR:0.033418\n",
      "Pre:0.965319\n",
      "F1:0.953011\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.986054\n",
      "mean ACC: 0.953871\n",
      "mean MCC : 0.908010\n",
      "mean TPR:0.941012\n",
      "mean FPR:0.033418\n",
      "mean Pre:0.965319\n",
      "mean F1:0.953011\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 75s 833ms/step - loss: 0.5439 - acc: 0.7231 - val_loss: 0.4068 - val_acc: 0.8268\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.3367 - acc: 0.8580 - val_loss: 0.2870 - val_acc: 0.8890\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.2540 - acc: 0.8967 - val_loss: 0.2073 - val_acc: 0.9210\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.2000 - acc: 0.9208 - val_loss: 0.2399 - val_acc: 0.9018\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 61s 682ms/step - loss: 0.1632 - acc: 0.9375 - val_loss: 0.1711 - val_acc: 0.9357\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 60s 664ms/step - loss: 0.1422 - acc: 0.9456 - val_loss: 0.2152 - val_acc: 0.9158\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 60s 665ms/step - loss: 0.1495 - acc: 0.9438 - val_loss: 0.1549 - val_acc: 0.9440\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.1225 - acc: 0.9550 - val_loss: 0.1489 - val_acc: 0.9427\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 60s 669ms/step - loss: 0.1114 - acc: 0.9600 - val_loss: 0.1593 - val_acc: 0.9396\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 59s 656ms/step - loss: 0.1073 - acc: 0.9605 - val_loss: 0.1382 - val_acc: 0.9482\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0943 - acc: 0.9652 - val_loss: 0.1848 - val_acc: 0.9343\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.0944 - acc: 0.9665 - val_loss: 0.1310 - val_acc: 0.9491\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 60s 671ms/step - loss: 0.0874 - acc: 0.9685 - val_loss: 0.1227 - val_acc: 0.9537\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 59s 651ms/step - loss: 0.0788 - acc: 0.9713 - val_loss: 0.1443 - val_acc: 0.9466\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 59s 652ms/step - loss: 0.0779 - acc: 0.9706 - val_loss: 0.1308 - val_acc: 0.9496\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 59s 656ms/step - loss: 0.0923 - acc: 0.9659 - val_loss: 0.1329 - val_acc: 0.9519\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 59s 658ms/step - loss: 0.0819 - acc: 0.9700 - val_loss: 0.1655 - val_acc: 0.9382\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 59s 656ms/step - loss: 0.0676 - acc: 0.9758 - val_loss: 0.1288 - val_acc: 0.9509\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 59s 652ms/step - loss: 0.0634 - acc: 0.9769 - val_loss: 0.1351 - val_acc: 0.9540\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 59s 658ms/step - loss: 0.0646 - acc: 0.9759 - val_loss: 0.1303 - val_acc: 0.9528\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 59s 657ms/step - loss: 0.0803 - acc: 0.9701 - val_loss: 0.3860 - val_acc: 0.8672\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 58s 645ms/step - loss: 0.0730 - acc: 0.9726 - val_loss: 0.1310 - val_acc: 0.9543\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 59s 653ms/step - loss: 0.0498 - acc: 0.9819 - val_loss: 0.1309 - val_acc: 0.9532\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 59s 652ms/step - loss: 0.0533 - acc: 0.9802 - val_loss: 0.1369 - val_acc: 0.9523\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 59s 657ms/step - loss: 0.0531 - acc: 0.9803 - val_loss: 0.1338 - val_acc: 0.9530\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 59s 653ms/step - loss: 0.0462 - acc: 0.9837 - val_loss: 0.1409 - val_acc: 0.9504\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 59s 656ms/step - loss: 0.0540 - acc: 0.9802 - val_loss: 0.1387 - val_acc: 0.9551\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.0477 - acc: 0.9817 - val_loss: 0.2004 - val_acc: 0.9445\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 62s 688ms/step - loss: 0.0456 - acc: 0.9830 - val_loss: 0.2032 - val_acc: 0.9433\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 61s 682ms/step - loss: 0.0504 - acc: 0.9809 - val_loss: 0.1481 - val_acc: 0.9543\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.0448 - acc: 0.9818 - val_loss: 0.1488 - val_acc: 0.9524\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0420 - acc: 0.9848 - val_loss: 0.1566 - val_acc: 0.9505\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0366 - acc: 0.9859 - val_loss: 0.1938 - val_acc: 0.9460\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0394 - acc: 0.9853 - val_loss: 0.1580 - val_acc: 0.9513\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.0359 - acc: 0.9874 - val_loss: 0.1588 - val_acc: 0.9536\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 62s 683ms/step - loss: 0.0388 - acc: 0.9866 - val_loss: 0.1566 - val_acc: 0.9543\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 61s 683ms/step - loss: 0.0404 - acc: 0.9857 - val_loss: 0.1593 - val_acc: 0.9516\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 61s 682ms/step - loss: 0.0345 - acc: 0.9873 - val_loss: 0.1615 - val_acc: 0.9498\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0269 - acc: 0.9898 - val_loss: 0.2051 - val_acc: 0.9508\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0320 - acc: 0.9884 - val_loss: 0.1661 - val_acc: 0.9512\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.0342 - acc: 0.9866 - val_loss: 0.1670 - val_acc: 0.9538\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 61s 683ms/step - loss: 0.0309 - acc: 0.9885 - val_loss: 0.1612 - val_acc: 0.9553\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 61s 680ms/step - loss: 0.0312 - acc: 0.9889 - val_loss: 0.1794 - val_acc: 0.9521\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0248 - acc: 0.9913 - val_loss: 0.1883 - val_acc: 0.9464\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0329 - acc: 0.9874 - val_loss: 0.1743 - val_acc: 0.9510\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0318 - acc: 0.9887 - val_loss: 0.1728 - val_acc: 0.9519\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0257 - acc: 0.9902 - val_loss: 0.1603 - val_acc: 0.9484\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0189 - acc: 0.9928 - val_loss: 0.2159 - val_acc: 0.9493\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.0347 - acc: 0.9873 - val_loss: 0.1820 - val_acc: 0.9460\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 61s 682ms/step - loss: 0.0216 - acc: 0.9922 - val_loss: 0.1876 - val_acc: 0.9528\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0203 - acc: 0.9922 - val_loss: 0.1940 - val_acc: 0.9516\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0172 - acc: 0.9929 - val_loss: 0.2024 - val_acc: 0.9497\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0185 - acc: 0.9937 - val_loss: 0.2040 - val_acc: 0.9540\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0251 - acc: 0.9906 - val_loss: 0.1820 - val_acc: 0.9466\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0219 - acc: 0.9924 - val_loss: 0.2092 - val_acc: 0.9510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0170 - acc: 0.9942 - val_loss: 0.1891 - val_acc: 0.9525\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.0231 - acc: 0.9923 - val_loss: 0.2106 - val_acc: 0.9512\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.0383 - acc: 0.9870 - val_loss: 0.1533 - val_acc: 0.9513\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 61s 680ms/step - loss: 0.0231 - acc: 0.9930 - val_loss: 0.2200 - val_acc: 0.9498\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.0142 - acc: 0.9947 - val_loss: 0.2012 - val_acc: 0.9523\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.0168 - acc: 0.9942 - val_loss: 0.2067 - val_acc: 0.9545\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0326 - acc: 0.9890 - val_loss: 0.1794 - val_acc: 0.9538\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.987630\n",
      "ACC: 0.956189\n",
      "MCC : 0.912736\n",
      "TPR:0.943301\n",
      "FPR:0.030559\n",
      "Pre:0.969455\n",
      "F1:0.956199\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.987630\n",
      "mean ACC: 0.956189\n",
      "mean MCC : 0.912736\n",
      "mean TPR:0.943301\n",
      "mean FPR:0.030559\n",
      "mean Pre:0.969455\n",
      "mean F1:0.956199\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 77s 854ms/step - loss: 0.5574 - acc: 0.7070 - val_loss: 0.4161 - val_acc: 0.8363\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.3461 - acc: 0.8543 - val_loss: 0.3359 - val_acc: 0.8684\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.2404 - acc: 0.9046 - val_loss: 0.2029 - val_acc: 0.9268\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 62s 687ms/step - loss: 0.1743 - acc: 0.9334 - val_loss: 0.2063 - val_acc: 0.9209\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.1686 - acc: 0.9369 - val_loss: 0.1762 - val_acc: 0.9334\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 62s 684ms/step - loss: 0.1501 - acc: 0.9424 - val_loss: 0.1500 - val_acc: 0.9465\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.1363 - acc: 0.9488 - val_loss: 0.2000 - val_acc: 0.9215\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.1219 - acc: 0.9560 - val_loss: 0.1672 - val_acc: 0.9369\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.1207 - acc: 0.9553 - val_loss: 0.1351 - val_acc: 0.9503\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.1120 - acc: 0.9582 - val_loss: 0.1289 - val_acc: 0.9538\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 61s 675ms/step - loss: 0.1137 - acc: 0.9588 - val_loss: 0.1507 - val_acc: 0.9442\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0926 - acc: 0.9681 - val_loss: 0.1440 - val_acc: 0.9469\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0996 - acc: 0.9624 - val_loss: 0.1420 - val_acc: 0.9469\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0848 - acc: 0.9701 - val_loss: 0.1425 - val_acc: 0.9472\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.0784 - acc: 0.9719 - val_loss: 0.2792 - val_acc: 0.8952\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0967 - acc: 0.9638 - val_loss: 0.1457 - val_acc: 0.9450\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0741 - acc: 0.9743 - val_loss: 0.1330 - val_acc: 0.9513\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 60s 666ms/step - loss: 0.0745 - acc: 0.9728 - val_loss: 0.1944 - val_acc: 0.9369\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 60s 661ms/step - loss: 0.0718 - acc: 0.9742 - val_loss: 0.1331 - val_acc: 0.9487\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0708 - acc: 0.9730 - val_loss: 0.1324 - val_acc: 0.9517\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0609 - acc: 0.9783 - val_loss: 0.1338 - val_acc: 0.9523\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 60s 661ms/step - loss: 0.0650 - acc: 0.9754 - val_loss: 0.1303 - val_acc: 0.9545\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 61s 678ms/step - loss: 0.0600 - acc: 0.9782 - val_loss: 0.1376 - val_acc: 0.9531\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 62s 687ms/step - loss: 0.0654 - acc: 0.9760 - val_loss: 0.1250 - val_acc: 0.9568\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 60s 672ms/step - loss: 0.0491 - acc: 0.9816 - val_loss: 0.1261 - val_acc: 0.9551\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 60s 664ms/step - loss: 0.0518 - acc: 0.9807 - val_loss: 0.1266 - val_acc: 0.9542\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 59s 659ms/step - loss: 0.0598 - acc: 0.9777 - val_loss: 0.4175 - val_acc: 0.7946\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 61s 681ms/step - loss: 0.0612 - acc: 0.9755 - val_loss: 0.1435 - val_acc: 0.9543\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 60s 664ms/step - loss: 0.0450 - acc: 0.9830 - val_loss: 0.1334 - val_acc: 0.9535\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 59s 653ms/step - loss: 0.0460 - acc: 0.9822 - val_loss: 0.1441 - val_acc: 0.9530\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 59s 660ms/step - loss: 0.0422 - acc: 0.9835 - val_loss: 0.1525 - val_acc: 0.9525\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 59s 655ms/step - loss: 0.0481 - acc: 0.9821 - val_loss: 0.1627 - val_acc: 0.9519\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 60s 667ms/step - loss: 0.0514 - acc: 0.9807 - val_loss: 0.1493 - val_acc: 0.9463\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0426 - acc: 0.9833 - val_loss: 0.1714 - val_acc: 0.9531\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 61s 673ms/step - loss: 0.0365 - acc: 0.9856 - val_loss: 0.1363 - val_acc: 0.9564\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 61s 674ms/step - loss: 0.0391 - acc: 0.9859 - val_loss: 0.1784 - val_acc: 0.9517\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 60s 670ms/step - loss: 0.0329 - acc: 0.9864 - val_loss: 0.2597 - val_acc: 0.9428\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0367 - acc: 0.9866 - val_loss: 0.1658 - val_acc: 0.9511\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0257 - acc: 0.9904 - val_loss: 0.2169 - val_acc: 0.9489\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 61s 680ms/step - loss: 0.0335 - acc: 0.9884 - val_loss: 0.1505 - val_acc: 0.9527\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0291 - acc: 0.9897 - val_loss: 0.1511 - val_acc: 0.9535\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 61s 677ms/step - loss: 0.0362 - acc: 0.9868 - val_loss: 0.1624 - val_acc: 0.9531\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 61s 676ms/step - loss: 0.0360 - acc: 0.9870 - val_loss: 0.1497 - val_acc: 0.9551\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 61s 679ms/step - loss: 0.0401 - acc: 0.9858 - val_loss: 0.1666 - val_acc: 0.9534\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.987769\n",
      "ACC: 0.955726\n",
      "MCC : 0.911518\n",
      "TPR:0.949302\n",
      "FPR:0.037893\n",
      "Pre:0.961375\n",
      "F1:0.955301\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.987769\n",
      "mean ACC: 0.955726\n",
      "mean MCC : 0.911518\n",
      "mean TPR:0.949302\n",
      "mean FPR:0.037893\n",
      "mean Pre:0.961375\n",
      "mean F1:0.955301\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "dataset_name = 'SC'\n",
    "for rep in range(0,5):\n",
    "    n_splits = 1\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "     \n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'SC_CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'SC_CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'SC_CV/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 192\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'SC_CV/sc_GoplusSeq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "\n",
    "         \n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    validation_data=valid_generator,\n",
    "                    epochs = 100,verbose=1,callbacks=[earlyStopping, save_checkpoint] )\n",
    "         \n",
    "        \n",
    "         \n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('new_seq_and_go__incep_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(save_model_name)\n",
    "# with open(test_pairs_file, 'r') as f:\n",
    "#     test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "# test_len = int(len(test_ppi_pairs))\n",
    "# list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "# list_IDs_temp = np.random.choice(list_IDs_temp, 4000).tolist()\n",
    "\n",
    "# test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "# y_pred_prob = model.predict(test_x)\n",
    "\n",
    "\n",
    "# y_pred = (y_pred_prob > 0.5)\n",
    "# auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "# f1 = f1_score(y_test, y_pred)\n",
    "# pre = precision_score(y_test, y_pred)\n",
    "# acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "# pr_auc = metrics.auc(recall, precision)\n",
    "# mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "# tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "# total=tn+fp+fn+tp\n",
    "# sen = float(tp)/float(tp+fn)\n",
    "# sps = float(tn)/float((tn+fp))\n",
    "\n",
    "# tpr = float(tp)/float(tp+fn)\n",
    "# fpr = float(fp)/float((tn+fp))\n",
    "# print('--------------------------\\n')\n",
    "# print ('AUC: %f' % auc)\n",
    "# print ('ACC: %f' % acc) \n",
    "# # print(\"PRAUC: %f\" % pr_auc)\n",
    "# print ('MCC : %f' % mcc)\n",
    "# # print ('SEN: %f' % sen)\n",
    "# # print ('SEP: %f' % sps)\n",
    "# print('TPR:%f'%tpr)\n",
    "# print('FPR:%f'%fpr)\n",
    "# print('Pre:%f'%pre)\n",
    "# print('F1:%f'%f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.987948\n",
      "mean ACC: 0.956630\n",
      "mean MCC : 0.913588\n",
      "mean TPR:0.944133\n",
      "mean FPR:0.030802\n",
      "mean Pre:0.968667\n",
      "mean F1:0.956222\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "results1 =   np.load( 'new_seq_and_go__incep_1.npz')\n",
    "results2 =   np.load( 'new_seq_and_go__incep_0.npz')\n",
    "results3 =   np.load( 'new_seq_and_go__incep_2.npz')\n",
    "results4 =   np.load( 'new_seq_and_go__incep_4.npz')\n",
    "results5 =   np.load( 'new_seq_and_go__incep_3.npz')\n",
    "print ('mean AUC: %f' %  (  (np.mean( results4[ 'AUCs' ] )  +  np.mean( results5[ 'AUCs' ] )  + np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/5     ) )\n",
    "print ('mean ACC: %f' %   (  ( np.mean( results4[ 'ACCs' ] )  + np.mean(  results5[ 'ACCs' ] )  +   np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/5) )\n",
    "print ('mean MCC : %f' %  ( ( np.mean( results4[ 'MCCs' ] )  + np.mean(  results5[ 'MCCs' ] )  + np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/5))\n",
    "print('mean TPR:%f'%    (( np.mean( results4[ 'TPRs' ] )  + np.mean(  results5[ 'TPRs' ] )  + np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/5))\n",
    "print('mean FPR:%f'%   (  (np.mean( results4[ 'FPRs' ] )  + np.mean(  results5[ 'FPRs' ] )  + np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/5))\n",
    "print('mean Pre:%f'%    ( (np.mean( results4[ 'Precs' ] )  + np.mean(  results5[ 'Precs' ] )  + np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/5))\n",
    "print('mean F1:%f'%    (  (np.mean( results4[ 'F1s' ] )  + np.mean(  results5[ 'F1s' ] )  +np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean AUC: 0.981603\n",
      "mean ACC: 0.938093\n",
      "mean MCC : 0.850220\n",
      "mean TPR:0.949721\n",
      "mean FPR:0.066037\n",
      "mean Pre:0.836816\n",
      "mean F1:0.889516\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np \n",
    "# results1 =   np.load( 'all_seq_and_go__incep_0.npz')\n",
    "# results2 =   np.load( 'all_seq_and_go__incep_1.npz')\n",
    "# results3 =   np.load( 'all_seq_and_go__incep_2.npz')\n",
    "# print ('mean AUC: %f' %  ( (np.mean( results1[ 'AUCs' ] )  + np.mean(  results2[ 'AUCs' ] )  + np.mean(results3[ 'AUCs' ]))/3     ) )\n",
    "# print ('mean ACC: %f' %   ( (np.mean( results1[ 'ACCs' ] )  + np.mean(  results2[ 'ACCs' ] )  + np.mean(results3[ 'ACCs' ]))/3) )\n",
    "# print ('mean MCC : %f' %  (  (np.mean( results1[ 'MCCs' ] )  + np.mean(  results2[ 'MCCs' ] )  + np.mean(results3[ 'MCCs' ])     )/3))\n",
    "# print('mean TPR:%f'%    ((np.mean( results1[ 'TPRs' ] )  + np.mean(  results2[ 'TPRs' ] )  + np.mean(results3[ 'TPRs' ])     )/3))\n",
    "# print('mean FPR:%f'%   ( (np.mean( results1[ 'FPRs' ] )  + np.mean(  results2[ 'FPRs' ] )  + np.mean(results3[ 'FPRs' ])     )/3))\n",
    "# print('mean Pre:%f'%    ((np.mean( results1[ 'Precs' ] )  + np.mean(  results2[ 'Precs' ] )  + np.mean(results3[ 'Precs' ])     )/3))\n",
    "# print('mean F1:%f'%    ((np.mean( results1[ 'F1s' ] )  + np.mean(  results2[ 'F1s' ] )  + np.mean(results3[ 'F1s' ])     )/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
