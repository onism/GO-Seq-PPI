{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# from keras.utils import multi_gpu_model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "max_go_len = 256\n",
    "max_seq_len = 512\n",
    " \n",
    "dataset_name = 'SHS27k'\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di\n",
    " \n",
    "protein2go =  load_dict('ensemble2go_terms_'+dataset_name+'.pkl')\n",
    "\n",
    "prot2emb = {}\n",
    " \n",
    "for key, value in protein2go.items():\n",
    "    X_go1 =  np.random.randn(1,768)\n",
    "    allgos = value.split(';') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    for  go in  allgos:\n",
    "        if go.startswith('GO') and os.path.exists('../ncbi_allfeatures4go/'+go+'_0.npy'):\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            \n",
    "            if count + feature.shape[0] > max_go_len:\n",
    "                print(count + feature.shape[0])\n",
    "                break\n",
    "            X_go1 = np.concatenate((X_go1,feature ))    \n",
    "            count += feature.shape[0]\n",
    "        else:\n",
    "            feature = np.random.randn(1,768)\n",
    "            if count + feature.shape[0] > max_go_len:\n",
    "                print(count + feature.shape[0])\n",
    "                break\n",
    "            X_go1 = np.concatenate((X_go1,feature ))    \n",
    "            count += feature.shape[0]\n",
    "            \n",
    "    \n",
    "    prot2emb[key] =  X_go1[1:]   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "         \n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.max_golen = max_go_len\n",
    "         \n",
    "        self.protein2go =  load_dict('ensemble2go_terms_'+dataset_name+'.pkl')\n",
    "        self.protein2seq = load_dict('ensemble2seqs_'+dataset_name+'.pkl')\n",
    "         \n",
    "         \n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "         \n",
    "        self.data = np.load(ppi_pair_file)\n",
    "        self.ensp1 = self.data['ensp1']\n",
    "        self.ensp2 = self.data['ensp2']\n",
    "        self.labels = self.data['labels']\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "      \n",
    "     \n",
    "            \n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "\n",
    "                \n",
    "    def project2gpu_go(self, protemb_tmp):\n",
    "        X_go1 = np.zeros((self.max_golen,768))\n",
    "        protemb_len = protemb_tmp.shape[0]\n",
    "        X_go1[:protemb_len] = protemb_tmp\n",
    "        return X_go1\n",
    "\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.labels.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "     \n",
    "            \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.labels.shape[0])\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        \n",
    "        y = self.labels[list_IDs_temp]\n",
    "        \n",
    "        X_seq1 = np.array([self.protein2onehot[p1] for p1 in self.ensp1[list_IDs_temp]])\n",
    "        X_seq2 = np.array([self.protein2onehot[p2] for p2 in self.ensp2[list_IDs_temp]])\n",
    "        X_go1 =  np.array([self.project2gpu_go(prot2emb[p1]) for p1 in self.ensp1[list_IDs_temp]])\n",
    "        X_go2 =  np.array([self.project2gpu_go(prot2emb[p2]) for p2 in self.ensp2[list_IDs_temp]])\n",
    "\n",
    "#         X_go1 = np.empty((self.batch_size, self.max_golen,768))\n",
    "#         X_go2 = np.empty((self.batch_size, self.max_golen,768))\n",
    "#         y = np.empty((self.batch_size, 7))\n",
    "#         X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "#         X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "         \n",
    "\n",
    "#         # Generate data\n",
    "#         for i, ID in enumerate(list_IDs_temp):\n",
    "             \n",
    "#             p1 = self.ensp1[ID]\n",
    "#             p2 = self.ensp2[ID]\n",
    "#             y[i] = self.labels[ID]\n",
    "#             prot1emb_tmp = prot2emb[p1]\n",
    "#             X_go1[i,:prot1emb_tmp.shape[0]] = prot1emb_tmp\n",
    "            \n",
    "#             prot2emb_tmp = prot2emb[p2]\n",
    "#             X_go2[i,:prot2emb_tmp.shape[0]] = prot2emb_tmp\n",
    "            \n",
    "#             X_seq1[i] =  self.protein2onehot[p1]\n",
    "#             X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "             \n",
    "            \n",
    "            \n",
    "            \n",
    "     \n",
    "        return [X_go1,X_go2,    X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "         \n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.max_golen,768))\n",
    "        y = np.empty((len(list_IDs_temp),7))\n",
    "        \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        \n",
    "        \n",
    "         \n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            p1 = self.ensp1[ID]\n",
    "            p2 = self.ensp2[ID]\n",
    "            y[i] = self.labels[ID]\n",
    "            \n",
    "            prot1emb_tmp = prot2emb[p1]\n",
    "            X_go1[i,:prot1emb_tmp.shape[0]] = prot1emb_tmp\n",
    "            \n",
    "            prot2emb_tmp = prot2emb[p2]\n",
    "            X_go2[i,:prot2emb_tmp.shape[0]] = prot2emb_tmp\n",
    "            \n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            \n",
    "            \n",
    "  \n",
    "        return [X_go1,X_go2,   X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 512, 20)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 512, 20)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 512, 16)      336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 512, 16)      336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 512, 16)      336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 512, 16)      336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 512, 16)      1296        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 512, 16)      784         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 512, 16)      976         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 512, 16)      336         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 512, 16)      1296        conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 512, 16)      784         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 512, 16)      976         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 512, 16)      336         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 256, 768)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512, 64)      0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 512, 128)     33024       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 512, 64)      0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 512, 128)     33024       input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 768)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 768)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 768)          1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 768)          0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 768)          0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 768)          1024        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512, 64)      0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512, 128)     0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 512, 64)      0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512, 128)     0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2304)         0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 2304)         0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 64)           576         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_5 (Glo (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 128)          640         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 64)           0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 64)           576         dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_8 (Glo (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 128)          640         dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2360320     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1024)         2360320     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 576)          0           global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 attention_3[0][0]                \n",
      "                                                                 global_average_pooling1d_5[0][0] \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 attention_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 576)          0           global_average_pooling1d_6[0][0] \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 attention_6[0][0]                \n",
      "                                                                 global_average_pooling1d_8[0][0] \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          262400      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          262400      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 256)          147712      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 256)          147712      concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1024)         0           dense_2[0][0]                    \n",
      "                                                                 dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "                                                                 dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1024)         1049600     concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1024)         0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1024)         1049600     dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1024)         0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 512)          524800      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 512)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 7)            3591        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 7)            0           dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,247,111\n",
      "Trainable params: 8,247,111\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, dot, Flatten, CuDNNLSTM, Add\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam,  RMSprop\n",
    "from keras import regularizers\n",
    "from keras_radam import RAdam\n",
    "from keras_lookahead import Lookahead\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\",    padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\",   padding='same')(y)\n",
    "#     x1 = Dropout(0.3)(x1)\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\",   padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\",   padding='same')(y)\n",
    "#     x2 = Dropout(0.3)(x2)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\",  padding='same')(input_tensor)\n",
    "#     x3 = Dropout(0.3)(x3)\n",
    "\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\",  padding='same')(input_tensor)\n",
    "#     x4 = Dropout(0.3)(x4)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(2)(y)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    " \n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "    x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    \n",
    "#     x_conv = Concatenate()([x_a, x_b,  x_c])\n",
    "#     x_conv = Dense(128, activation='relu')(x_conv)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "    x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "#     x_gru = Concatenate()([x_gru_a, x_gru_b,  x_gru_c])\n",
    "#     x_gru = Dense(128, activation='relu')(x_gru)\n",
    "#     x = Concatenate()([x_conv, x_gru])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Concatenate()([x_a, x_b, x_c, x_gru_a, x_gru_b, x_gru_c])\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     x_conv = Concatenate()([x_a, x_b, x_c, ])\n",
    "#     x_gru =  Concatenate()([x_gru_a, x_gru_b, x_gru_c ])\n",
    "#     x = BatchNormalization()(x)\n",
    "    \n",
    "#     x_conv = Dense(128, activation = 'relu')(x_conv)\n",
    "#     x_gru = Dense(128)(x_conv)\n",
    "    return x\n",
    "\n",
    "\n",
    "def build_fc_model(input_x):\n",
    "     \n",
    "    \n",
    "    x_a = GlobalAveragePooling1D()(input_x)\n",
    "    x_b = GlobalMaxPooling1D()(input_x)\n",
    "    x_c = Attention()(input_x)\n",
    "    x = Concatenate()([x_a, x_b, x_c])\n",
    "    x = Dense(1024)(x)\n",
    "    x = Dense(256)(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    right_input_go = Input(shape=(max_go_len,feature_len))\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "    left_x_go = build_fc_model(left_input_go)\n",
    "    right_x_go = build_fc_model(right_input_go)\n",
    "    \n",
    "#     left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "#     right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    \n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters//4, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters//4, gru_units)\n",
    "    \n",
    "      \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go,   left_x_seq, right_x_seq])\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(7)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "    output = Activation('softmax')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go,    left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model)\n",
    "#     adam = Adam(lr=0.01, amsgrad=True, epsilon=1e-5)\n",
    "    optimizer = Lookahead(RAdam())\n",
    "\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "# siamese_a = create_share_model()\n",
    "# siamese_a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/100\n",
      "85/85 [==============================] - 87s 1s/step - loss: 1.8143 - acc: 0.2265 - val_loss: 1.6425 - val_acc: 0.3108\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.31076, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 2/100\n",
      "85/85 [==============================] - 76s 898ms/step - loss: 1.6040 - acc: 0.3040 - val_loss: 1.4665 - val_acc: 0.3720\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.31076 to 0.37196, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 3/100\n",
      "85/85 [==============================] - 85s 999ms/step - loss: 1.5142 - acc: 0.3390 - val_loss: 1.4330 - val_acc: 0.3863\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.37196 to 0.38628, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 4/100\n",
      "85/85 [==============================] - 78s 914ms/step - loss: 1.4335 - acc: 0.3720 - val_loss: 1.3654 - val_acc: 0.4171\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.38628 to 0.41710, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 5/100\n",
      "85/85 [==============================] - 82s 964ms/step - loss: 1.3945 - acc: 0.3929 - val_loss: 1.4231 - val_acc: 0.3798\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.41710\n",
      "Epoch 6/100\n",
      "85/85 [==============================] - 78s 918ms/step - loss: 1.4061 - acc: 0.3875 - val_loss: 1.4352 - val_acc: 0.3824\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.41710\n",
      "Epoch 7/100\n",
      "85/85 [==============================] - 80s 936ms/step - loss: 1.3123 - acc: 0.4175 - val_loss: 1.4326 - val_acc: 0.3789\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.41710\n",
      "Epoch 8/100\n",
      "85/85 [==============================] - 77s 902ms/step - loss: 1.2952 - acc: 0.4297 - val_loss: 1.3130 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.41710 to 0.42318, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 9/100\n",
      "85/85 [==============================] - 77s 910ms/step - loss: 1.2508 - acc: 0.4372 - val_loss: 1.2091 - val_acc: 0.4609\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.42318 to 0.46094, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 10/100\n",
      "85/85 [==============================] - 79s 928ms/step - loss: 1.2589 - acc: 0.4364 - val_loss: 1.2328 - val_acc: 0.4640\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.46094 to 0.46398, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 11/100\n",
      "85/85 [==============================] - 82s 961ms/step - loss: 1.1862 - acc: 0.4636 - val_loss: 1.2227 - val_acc: 0.4614\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.46398\n",
      "Epoch 12/100\n",
      "85/85 [==============================] - 78s 923ms/step - loss: 1.2305 - acc: 0.4506 - val_loss: 1.2676 - val_acc: 0.4214\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.46398\n",
      "Epoch 13/100\n",
      "85/85 [==============================] - 80s 939ms/step - loss: 1.2364 - acc: 0.4465 - val_loss: 1.1563 - val_acc: 0.4753\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.46398 to 0.47526, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 14/100\n",
      "85/85 [==============================] - 75s 887ms/step - loss: 1.1475 - acc: 0.4783 - val_loss: 1.1761 - val_acc: 0.4748\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.47526\n",
      "Epoch 15/100\n",
      "85/85 [==============================] - 72s 848ms/step - loss: 1.1397 - acc: 0.4819 - val_loss: 1.1671 - val_acc: 0.4692\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.47526\n",
      "Epoch 16/100\n",
      "85/85 [==============================] - 64s 751ms/step - loss: 1.2141 - acc: 0.4597 - val_loss: 1.2369 - val_acc: 0.4080\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.47526\n",
      "Epoch 17/100\n",
      "85/85 [==============================] - 67s 786ms/step - loss: 1.1467 - acc: 0.4756 - val_loss: 1.1300 - val_acc: 0.4931\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.47526 to 0.49306, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 18/100\n",
      "85/85 [==============================] - 71s 836ms/step - loss: 1.1175 - acc: 0.4921 - val_loss: 1.1364 - val_acc: 0.4835\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.49306\n",
      "Epoch 19/100\n",
      "85/85 [==============================] - 68s 805ms/step - loss: 1.0776 - acc: 0.5004 - val_loss: 1.1593 - val_acc: 0.4661\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.49306\n",
      "Epoch 20/100\n",
      "85/85 [==============================] - 69s 809ms/step - loss: 1.1579 - acc: 0.4772 - val_loss: 1.1019 - val_acc: 0.4896\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.49306\n",
      "Epoch 21/100\n",
      "85/85 [==============================] - 62s 731ms/step - loss: 1.0752 - acc: 0.5085 - val_loss: 1.0888 - val_acc: 0.4931\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.49306\n",
      "Epoch 22/100\n",
      "85/85 [==============================] - 66s 774ms/step - loss: 1.0462 - acc: 0.5147 - val_loss: 1.0776 - val_acc: 0.4931\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.49306\n",
      "Epoch 23/100\n",
      "85/85 [==============================] - 67s 785ms/step - loss: 1.0734 - acc: 0.5061 - val_loss: 1.0578 - val_acc: 0.5091\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.49306 to 0.50911, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 24/100\n",
      "85/85 [==============================] - 80s 941ms/step - loss: 1.0935 - acc: 0.5006 - val_loss: 1.3216 - val_acc: 0.4219\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.50911\n",
      "Epoch 25/100\n",
      "85/85 [==============================] - 80s 939ms/step - loss: 1.1082 - acc: 0.4942 - val_loss: 1.0636 - val_acc: 0.5035\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.50911\n",
      "Epoch 26/100\n",
      "85/85 [==============================] - 76s 900ms/step - loss: 1.0243 - acc: 0.5274 - val_loss: 1.0526 - val_acc: 0.5082\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.50911\n",
      "Epoch 27/100\n",
      "85/85 [==============================] - 72s 842ms/step - loss: 0.9979 - acc: 0.5356 - val_loss: 1.0192 - val_acc: 0.5269\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.50911 to 0.52691, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 28/100\n",
      "85/85 [==============================] - 79s 929ms/step - loss: 1.0117 - acc: 0.5284 - val_loss: 1.0328 - val_acc: 0.5178\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.52691\n",
      "Epoch 29/100\n",
      "85/85 [==============================] - 74s 868ms/step - loss: 1.0065 - acc: 0.5357 - val_loss: 1.0640 - val_acc: 0.5109\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.52691\n",
      "Epoch 30/100\n",
      "85/85 [==============================] - 74s 876ms/step - loss: 0.9835 - acc: 0.5380 - val_loss: 1.0547 - val_acc: 0.5113\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.52691\n",
      "Epoch 31/100\n",
      "85/85 [==============================] - 76s 895ms/step - loss: 0.9786 - acc: 0.5423 - val_loss: 1.0517 - val_acc: 0.4931\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.52691\n",
      "Epoch 32/100\n",
      "85/85 [==============================] - 69s 815ms/step - loss: 0.9753 - acc: 0.5417 - val_loss: 1.0208 - val_acc: 0.5135\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.52691\n",
      "Epoch 33/100\n",
      "85/85 [==============================] - 70s 826ms/step - loss: 1.4423 - acc: 0.3938 - val_loss: 1.8655 - val_acc: 0.1693\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.52691\n",
      "Epoch 34/100\n",
      "85/85 [==============================] - 72s 843ms/step - loss: 1.7825 - acc: 0.2213 - val_loss: 1.6454 - val_acc: 0.3056\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.52691\n",
      "Epoch 35/100\n",
      "85/85 [==============================] - 70s 821ms/step - loss: 1.4242 - acc: 0.3710 - val_loss: 1.1476 - val_acc: 0.4679\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.52691\n",
      "Epoch 36/100\n",
      "85/85 [==============================] - 70s 827ms/step - loss: 1.1035 - acc: 0.4911 - val_loss: 1.0541 - val_acc: 0.5043\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.52691\n",
      "Epoch 37/100\n",
      "85/85 [==============================] - 70s 824ms/step - loss: 0.9920 - acc: 0.5358 - val_loss: 1.0108 - val_acc: 0.5312\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.52691 to 0.53125, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 38/100\n",
      "85/85 [==============================] - 70s 818ms/step - loss: 0.9700 - acc: 0.5402 - val_loss: 1.0372 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.53125\n",
      "Epoch 39/100\n",
      "85/85 [==============================] - 69s 813ms/step - loss: 0.9719 - acc: 0.5416 - val_loss: 1.0029 - val_acc: 0.5226\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.53125\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 70s 819ms/step - loss: 0.9415 - acc: 0.5531 - val_loss: 0.9944 - val_acc: 0.5369\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.53125 to 0.53689, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 41/100\n",
      "85/85 [==============================] - 71s 830ms/step - loss: 0.9670 - acc: 0.5485 - val_loss: 1.1042 - val_acc: 0.4905\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.53689\n",
      "Epoch 42/100\n",
      "85/85 [==============================] - 70s 822ms/step - loss: 0.9646 - acc: 0.5450 - val_loss: 1.0024 - val_acc: 0.5321\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.53689\n",
      "Epoch 43/100\n",
      "85/85 [==============================] - 71s 831ms/step - loss: 0.9618 - acc: 0.5465 - val_loss: 1.0381 - val_acc: 0.5148\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.53689\n",
      "Epoch 44/100\n",
      "85/85 [==============================] - 71s 837ms/step - loss: 0.9113 - acc: 0.5597 - val_loss: 1.0516 - val_acc: 0.5208\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.53689\n",
      "Epoch 45/100\n",
      "85/85 [==============================] - 71s 833ms/step - loss: 0.9165 - acc: 0.5608 - val_loss: 1.0506 - val_acc: 0.5182\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.53689\n",
      "Epoch 46/100\n",
      "85/85 [==============================] - 70s 825ms/step - loss: 0.9145 - acc: 0.5608 - val_loss: 0.9687 - val_acc: 0.5460\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.53689 to 0.54601, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 47/100\n",
      "85/85 [==============================] - 71s 835ms/step - loss: 0.9428 - acc: 0.5522 - val_loss: 1.0033 - val_acc: 0.5130\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.54601\n",
      "Epoch 48/100\n",
      "85/85 [==============================] - 71s 837ms/step - loss: 0.9242 - acc: 0.5603 - val_loss: 0.9769 - val_acc: 0.5447\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.54601\n",
      "Epoch 49/100\n",
      "85/85 [==============================] - 71s 830ms/step - loss: 0.8969 - acc: 0.5674 - val_loss: 0.9762 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.54601\n",
      "Epoch 50/100\n",
      "85/85 [==============================] - 71s 835ms/step - loss: 0.8983 - acc: 0.5678 - val_loss: 1.0229 - val_acc: 0.5178\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.54601\n",
      "Epoch 51/100\n",
      "85/85 [==============================] - 70s 818ms/step - loss: 0.8936 - acc: 0.5689 - val_loss: 0.9474 - val_acc: 0.5330\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.54601\n",
      "Epoch 52/100\n",
      "85/85 [==============================] - 71s 839ms/step - loss: 0.8820 - acc: 0.5713 - val_loss: 0.9619 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.54601\n",
      "Epoch 53/100\n",
      "85/85 [==============================] - 71s 831ms/step - loss: 0.8767 - acc: 0.5722 - val_loss: 0.9765 - val_acc: 0.5369\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.54601\n",
      "Epoch 54/100\n",
      "85/85 [==============================] - 74s 865ms/step - loss: 0.8810 - acc: 0.5722 - val_loss: 0.9714 - val_acc: 0.5386\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.54601\n",
      "Epoch 55/100\n",
      "85/85 [==============================] - 72s 852ms/step - loss: 0.9225 - acc: 0.5597 - val_loss: 0.9790 - val_acc: 0.5404\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.54601\n",
      "Epoch 56/100\n",
      "85/85 [==============================] - 71s 836ms/step - loss: 0.8926 - acc: 0.5712 - val_loss: 1.0087 - val_acc: 0.5339\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.54601\n",
      "Epoch 57/100\n",
      "85/85 [==============================] - 72s 842ms/step - loss: 0.8734 - acc: 0.5767 - val_loss: 1.0009 - val_acc: 0.5230\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.54601\n",
      "Epoch 58/100\n",
      "85/85 [==============================] - 71s 835ms/step - loss: 0.8685 - acc: 0.5792 - val_loss: 0.9870 - val_acc: 0.5386\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.54601\n",
      "Epoch 59/100\n",
      "85/85 [==============================] - 72s 846ms/step - loss: 0.8617 - acc: 0.5796 - val_loss: 0.9795 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.54601 to 0.54905, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 60/100\n",
      "85/85 [==============================] - 72s 843ms/step - loss: 0.8587 - acc: 0.5802 - val_loss: 0.9789 - val_acc: 0.5378\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.54905\n",
      "Epoch 61/100\n",
      "85/85 [==============================] - 71s 838ms/step - loss: 0.8930 - acc: 0.5695 - val_loss: 0.9594 - val_acc: 0.5512\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.54905 to 0.55122, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 62/100\n",
      "85/85 [==============================] - 72s 845ms/step - loss: 0.8439 - acc: 0.5837 - val_loss: 0.9538 - val_acc: 0.5573\n",
      "\n",
      "Epoch 00062: val_acc improved from 0.55122 to 0.55729, saving model to CV/27_fc_go_seq_rand0-0.hdf5\n",
      "Epoch 63/100\n",
      "85/85 [==============================] - 71s 839ms/step - loss: 0.8392 - acc: 0.5874 - val_loss: 0.9742 - val_acc: 0.5399\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.55729\n",
      "Epoch 64/100\n",
      "85/85 [==============================] - 71s 832ms/step - loss: 0.8542 - acc: 0.5823 - val_loss: 0.9755 - val_acc: 0.5373\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.55729\n",
      "Epoch 65/100\n",
      "85/85 [==============================] - 71s 836ms/step - loss: 0.8687 - acc: 0.5784 - val_loss: 0.9710 - val_acc: 0.5265\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.55729\n",
      "Epoch 66/100\n",
      "85/85 [==============================] - 71s 837ms/step - loss: 0.9157 - acc: 0.5673 - val_loss: 0.9810 - val_acc: 0.5356\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.55729\n",
      "Epoch 67/100\n",
      "85/85 [==============================] - 70s 826ms/step - loss: 0.8654 - acc: 0.5785 - val_loss: 0.9614 - val_acc: 0.5473\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.55729\n",
      "Epoch 68/100\n",
      "85/85 [==============================] - 71s 839ms/step - loss: 0.8360 - acc: 0.5895 - val_loss: 0.9592 - val_acc: 0.5560\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.55729\n",
      "Epoch 69/100\n",
      "85/85 [==============================] - 71s 829ms/step - loss: 0.8369 - acc: 0.5902 - val_loss: 0.9709 - val_acc: 0.5343\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.55729\n",
      "Epoch 70/100\n",
      "85/85 [==============================] - 71s 838ms/step - loss: 0.8696 - acc: 0.5788 - val_loss: 0.9759 - val_acc: 0.5421\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.55729\n",
      "Epoch 71/100\n",
      "85/85 [==============================] - 72s 850ms/step - loss: 0.8434 - acc: 0.5883 - val_loss: 0.9862 - val_acc: 0.5508\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.55729\n",
      "Epoch 72/100\n",
      "85/85 [==============================] - 70s 822ms/step - loss: 0.8439 - acc: 0.5858 - val_loss: 0.9664 - val_acc: 0.5499\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.55729\n",
      "Epoch 73/100\n",
      "85/85 [==============================] - 71s 834ms/step - loss: 0.8471 - acc: 0.5850 - val_loss: 0.9562 - val_acc: 0.5516\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.55729\n",
      "Epoch 74/100\n",
      "85/85 [==============================] - 72s 846ms/step - loss: 0.8402 - acc: 0.5909 - val_loss: 0.9675 - val_acc: 0.5490\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.55729\n",
      "Epoch 75/100\n",
      "85/85 [==============================] - 71s 835ms/step - loss: 0.8232 - acc: 0.5940 - val_loss: 0.9823 - val_acc: 0.5356\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.55729\n",
      "Epoch 76/100\n",
      "85/85 [==============================] - 73s 854ms/step - loss: 0.8806 - acc: 0.5779 - val_loss: 0.9750 - val_acc: 0.5447\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.55729\n",
      "Epoch 77/100\n",
      "85/85 [==============================] - 72s 845ms/step - loss: 0.8346 - acc: 0.5923 - val_loss: 0.9549 - val_acc: 0.5443\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.55729\n",
      "Epoch 78/100\n",
      "85/85 [==============================] - 73s 859ms/step - loss: 0.8161 - acc: 0.5963 - val_loss: 0.9711 - val_acc: 0.5516\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.55729\n",
      "Epoch 79/100\n",
      "85/85 [==============================] - 71s 840ms/step - loss: 0.8149 - acc: 0.5974 - val_loss: 0.9615 - val_acc: 0.5547\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.55729\n",
      "Epoch 80/100\n",
      "85/85 [==============================] - 71s 840ms/step - loss: 0.8558 - acc: 0.5864 - val_loss: 0.9798 - val_acc: 0.5443\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.55729\n",
      "Epoch 81/100\n",
      "85/85 [==============================] - 71s 832ms/step - loss: 0.8304 - acc: 0.5905 - val_loss: 0.9521 - val_acc: 0.5564\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.55729\n",
      "Epoch 82/100\n",
      "85/85 [==============================] - 71s 834ms/step - loss: 0.8174 - acc: 0.5996 - val_loss: 0.9877 - val_acc: 0.5469\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.55729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5929499072356216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e69aab59ee5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0mtrain_pairs_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mtest_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0mtest_pairs_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mvalid_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0mvalid_pairs_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e838d5238122>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, ppi_pair_file, batch_size)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotein2onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monehot_seqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppi_pair_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-e838d5238122>\u001b[0m in \u001b[0;36monehot_seqs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0monehot_seqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotein2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotein2onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mprotein_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seqlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c74e1706a8dc>\u001b[0m in \u001b[0;36mprotein_one_hot\u001b[0;34m(protein_sequence, MAX_SEQ_LEN)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mone_hot_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_SEQ_LEN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mone_hot_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mletter_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mone_hot_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c74e1706a8dc>\u001b[0m in \u001b[0;36mletter_one_hot\u001b[0;34m(aa)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mletter_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mletter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malphabet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maa\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mletter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mone_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for rep in range(1):\n",
    "    n_splits = 10\n",
    "     \n",
    "    count = 0\n",
    "    num_total = 0.\n",
    "    num_hit = 0.\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'CV/'+ dataset_name + '_train'+str(split)+'.npz'\n",
    "        test_pairs_file = 'CV/' + dataset_name +'_test'+str(split)+'.npz'\n",
    "        valid_pairs_file = 'CV/'+ dataset_name +'_valid'+str(split)+'.npz'\n",
    "        \n",
    "         \n",
    "\n",
    "        batch_size = 256\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "        model = build_model()\n",
    "        \n",
    "\n",
    "        save_model_name = 'CV/27_fc_go_seq_rand'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=20, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True,verbose=1, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "#         validation_data = valid_generator,\n",
    "#                                  callbacks=[earlyStopping, save_checkpoint]\n",
    "         \n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                   epochs = 100,verbose=1, validation_data = valid_generator, callbacks=[earlyStopping, save_checkpoint])\n",
    "#         model.save_weights(save_model_name)\n",
    "    \n",
    "\n",
    "        model.load_weights(save_model_name)\n",
    "        data = np.load(test_pairs_file)\n",
    "         \n",
    "\n",
    "        test_len = data['labels'].shape[0]\n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "        y_true = np.zeros((test_len,7))\n",
    "        y_pred_prob = np.zeros((test_len,7))\n",
    "        for index in range(int(np.floor(test_len/batch_size)) +1):\n",
    "            start_ind = index * batch_size\n",
    "            if (index+1)* batch_size >= test_len:\n",
    "                end_ind = test_len\n",
    "            else:\n",
    "                end_ind = (index+1)* batch_size\n",
    "            indexes = list_IDs_temp[start_ind:end_ind]\n",
    "            test_x, y_test = test_generator.all_data(indexes)\n",
    "            y_pred_prob_bs = model.predict(test_x)\n",
    "            y_true[start_ind:end_ind] = y_test.reshape(len(indexes),7)\n",
    "            y_pred_prob[start_ind:end_ind] = y_pred_prob_bs\n",
    "            del test_x\n",
    "            del y_test\n",
    "#         np.save('27_results'+str(split), y_pred_prob)\n",
    "        y_test = y_true\n",
    "\n",
    "#         test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "#         results = model.evaluate(test_x, y_test, batch_size=128)\n",
    "#         print('test loss, test acc:', results)\n",
    "#         y_pred_prob = model.predict(test_x)\n",
    "        \n",
    "        for i in range( y_test.shape[0]  ):\n",
    "            num_total += 1\n",
    "            if np.argmax(y_test[i]) == np.argmax(y_pred_prob[i]):\n",
    "                num_hit += 1\n",
    "        \n",
    "        print(num_hit / num_total)\n",
    "        del model\n",
    "        del train_generator\n",
    "        del valid_generator\n",
    "        del test_generator\n",
    "        del y_test\n",
    "    accuracy = num_hit / num_total\n",
    "    print (accuracy)\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del train_generator\n",
    "# del valid_generator\n",
    "# del test_generator\n",
    "# del test_x\n",
    "# del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true2 = np.argmax(y_true, axis=1)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "confusion_matrix(y_true2, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
