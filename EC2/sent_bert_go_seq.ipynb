{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/xhh/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers import  GlobalAveragePooling1D, Input, Activation, MaxPooling1D, BatchNormalization, Dense, Dropout, Conv1D,GlobalMaxPooling1D\n",
    "from keras.layers import GRU,AveragePooling1D,CuDNNGRU\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.models import Model \n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "KTF.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "alphabet = np.array(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "                     'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "def label_sequence(line, MAX_SEQ_LEN, smi_ch_ind):\n",
    "\tX = np.zeros(MAX_SEQ_LEN)\n",
    "\n",
    "\tfor i, ch in enumerate(line[:MAX_SEQ_LEN]):\n",
    "\t\tX[i] = smi_ch_ind[ch]\n",
    "\n",
    "\treturn X #.tolist()\n",
    "\n",
    "def letter_one_hot(aa):\n",
    "    one_hot = np.zeros(20)\n",
    "    for idx, letter in enumerate(alphabet):\n",
    "        if aa == letter:\n",
    "            one_hot[idx] = 1\n",
    "            return one_hot\n",
    "\n",
    "\n",
    "# Convert an entire protein to one-hot representation.\n",
    "def protein_one_hot(protein_sequence, MAX_SEQ_LEN):\n",
    "    #  Remove non-specific AA codes (very few are actually present in this dataset)\n",
    "    protein_sequence = protein_sequence.replace('B', '')\n",
    "    protein_sequence = protein_sequence.replace('J', '')\n",
    "    protein_sequence = protein_sequence.replace('O', '')\n",
    "    protein_sequence = protein_sequence.replace('U', '')\n",
    "    protein_sequence = protein_sequence.replace('X', '')\n",
    "    protein_sequence = protein_sequence.replace('Z', '')\n",
    "    one_hot_seq = np.zeros( (MAX_SEQ_LEN, 20))\n",
    "    for idx, aa in enumerate(protein_sequence[:MAX_SEQ_LEN]):\n",
    "        one_hot_seq[idx, :] = letter_one_hot(aa)\n",
    "    return one_hot_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f05d00d83f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_packed_sequence as unpack\n",
    "from torch.nn.utils.rnn import pack_padded_sequence as pack\n",
    "seed = 777\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embedding(words4sent, max_seq_len, feature_dim,   to_reverse=0):\n",
    "    length = []\n",
    "    output = []\n",
    "    \n",
    "    for words in words4sent:\n",
    "        if to_reverse:\n",
    "            words = np.flip(words, 0)\n",
    "        length.append( words.shape[0])\n",
    "        if  words.shape[0] < max_seq_len:\n",
    "            wordList = np.concatenate([words,np.zeros([max_seq_len - words.shape[0],feature_dim])])\n",
    "        output.append(wordList)\n",
    "    return np.array(output),np.array(length) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pool(x, lengths):\n",
    "    out = torch.FloatTensor(x.size(1), x.size(2)).zero_() # BxF\n",
    "    for i in range(x.size(1)):\n",
    "        out[i] = torch.mean(x[:lengths[i],i,:], 0)\n",
    "    return out\n",
    "\n",
    "\n",
    "class RandLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_layers, output_dim,  bidirectional=False):\n",
    "        super(RandLSTM, self).__init__()\n",
    "        \n",
    "\n",
    "        self.bidirectional = bidirectional\n",
    "        self.max_seq_len = 128\n",
    "        self.input_dim = input_dim\n",
    "         \n",
    "\n",
    "        self.e_hid_init = torch.zeros(1, 1, output_dim)\n",
    "        self.e_cell_init = torch.zeros(1, 1, output_dim)\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lm = nn.LSTM(input_dim, output_dim, num_layers=num_layers,\n",
    "                          bidirectional= self.bidirectional, batch_first=True)\n",
    "\n",
    "        self.bidirectional += 1\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "   \n",
    "\n",
    "    def lstm(self, inputs, lengths):\n",
    "        bsz, max_len, _ = inputs.size()\n",
    "        in_embs = inputs\n",
    "        lens, indices = torch.sort(lengths, 0, True)\n",
    "\n",
    "        e_hid_init = self.e_hid_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        e_cell_init = self.e_cell_init.expand(1*self.num_layers*self.bidirectional, bsz, self.output_dim).contiguous()\n",
    "        all_hids, (enc_last_hid, _) = self.lm(pack(in_embs[indices],\n",
    "                                                        lens.tolist(), batch_first=True), (e_hid_init, e_cell_init))\n",
    "        _, _indices = torch.sort(indices, 0)\n",
    "        all_hids = unpack(all_hids, batch_first=True)[0][_indices]\n",
    "\n",
    "        return all_hids\n",
    "\n",
    "    def forward(self, words4sent):\n",
    "        \n",
    "        out, lengths = gen_embedding(words4sent, self.max_seq_len, self.input_dim)\n",
    "        out = torch.from_numpy(out).float()\n",
    "        lengths = torch.from_numpy(np.array(lengths))\n",
    "        out = self.lstm(out, lengths)\n",
    "#         print(\"output size:\",out.size())\n",
    "        out = out.transpose(1,0)\n",
    "        out = mean_pool(out, lengths)\n",
    "        return out\n",
    "\n",
    "    def encode(self, batch):\n",
    "        return self.forward(batch).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from six.moves import cPickle as pickle #for performance\n",
    "\n",
    " \n",
    "def save_dict(di_, filename_):\n",
    "    with open(filename_, 'wb') as f:\n",
    "        pickle.dump(di_, f)\n",
    "\n",
    "def load_dict(filename_):\n",
    "    with open(filename_, 'rb') as f:\n",
    "        ret_di = pickle.load(f)\n",
    "    return ret_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract w2v model\n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "w2vmodel =  Word2Vec.load('/home/xhh/PMC_model/PMC_model.txt')\n",
    "def vector_name(name): \n",
    "    s = name.split(' ')\n",
    "    vectors = [] \n",
    "    for word in s: \n",
    "        if w2vmodel.wv.__contains__(word):   \n",
    "            vectors.append(w2vmodel.wv[word]) \n",
    "    else: \n",
    "        clear_words = re.sub('[^A-Za-z0-9]+', ' ', word) \n",
    "        clear_words = clear_words.lstrip().rstrip().split(' ')\n",
    "        for w in clear_words: \n",
    "            if w2vmodel.wv.__contains__(w): \n",
    "                vectors.append(w2vmodel.wv[w]) \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read go.obo obtain ontology type\n",
    " \n",
    "obo_file = '../cross-species/go.obo'\n",
    "fp=open(obo_file,'r')\n",
    "obo_txt=fp.read()\n",
    "fp.close()\n",
    "obo_txt=obo_txt[obo_txt.find(\"[Term]\")-1:]\n",
    "obo_txt=obo_txt[:obo_txt.find(\"[Typedef]\")]\n",
    "# obo_dict=parse_obo_txt(obo_txt)\n",
    "id_name_dicts = {}\n",
    "for Term_txt in obo_txt.split(\"[Term]\\n\"):\n",
    "    if not Term_txt.strip():\n",
    "        continue\n",
    "    name = ''\n",
    "    ids = []\n",
    "    for line in Term_txt.splitlines():\n",
    "        if   line.startswith(\"id: \"):\n",
    "            ids.append(line[len(\"id: \"):])     \n",
    "        elif line.startswith(\"name: \"):\n",
    "             name=line[len(\"name: \"):]\n",
    "        elif line.startswith(\"alt_id: \"):\n",
    "            ids.append(line[len(\"alt_id: \"):])\n",
    "    \n",
    "    for t_id in ids:\n",
    "        id_name_dicts[t_id] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "protein2go =  load_dict('EC2prot2go.pkl')\n",
    "prot2emb_w2v = {}\n",
    "project_dim = 2048\n",
    "num_layers = 1\n",
    "max_go_len = 1024\n",
    "max_protlen = 0\n",
    "w2vlstm = RandLSTM(200,num_layers,  project_dim, bidirectional = False)\n",
    "\n",
    "\n",
    "for key, value in protein2go.items(): \n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    words4sent = []\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.array(vector_name(id_name_dicts[go]))\n",
    "            if feature.shape[0] > 0:\n",
    "                words4sent.append(feature)\n",
    "            \n",
    "        count += feature.shape[0]\n",
    "    if len(words4sent) > 0:\n",
    "        sent_embedding = w2vlstm.encode(words4sent)\n",
    "    else:\n",
    "        sent_embedding = np.zeros((1, project_dim))\n",
    "    if max_protlen < sent_embedding.shape[0]:\n",
    "        max_protlen = sent_embedding.shape[0]\n",
    "    prot2emb_w2v[key] = sent_embedding \n",
    "\n",
    "del w2vmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(max_protlen)\n",
    "w2v_len = max_protlen\n",
    "# w2v_len = 211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot2emb_bert = {}\n",
    "max_protlen = 0\n",
    "input_dim = 768\n",
    " \n",
    "bertlstm = RandLSTM(input_dim,num_layers,  project_dim, bidirectional = False)\n",
    "for key, value in protein2go.items():\n",
    "     \n",
    "    allgos = value.split(',') \n",
    "    allgos = list(set(allgos))\n",
    "    count = 0\n",
    "    words4sent = []\n",
    "    for  go in  allgos:\n",
    "        if len(go) > 2:\n",
    "            feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "            words4sent.append(feature)\n",
    "        count += feature.shape[0] \n",
    "    if len(words4sent) > 0:\n",
    "        sent_embedding = bertlstm.encode(words4sent)\n",
    "    else:\n",
    "        sent_embedding = np.zeros((1, project_dim))\n",
    "    if max_protlen < sent_embedding.shape[0]:\n",
    "        max_protlen = sent_embedding.shape[0]\n",
    "    prot2emb_bert[key] = sent_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(max_protlen)\n",
    "bert_len = max_protlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "feature_len = 768\n",
    "\n",
    "max_seq_len = 1000\n",
    "# max_protlen = 32\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self,  ppi_pair_file, batch_size=128):\n",
    "        'Initialization' \n",
    "        self.batch_size = batch_size\n",
    "        self.ppi_pair_file = ppi_pair_file\n",
    "        input_dim = 768\n",
    "        num_layers = 1\n",
    "         \n",
    "        \n",
    "        \n",
    "        self.projection_dim = project_dim\n",
    "        self.bert_len = bert_len\n",
    "        self.w2v_len = w2v_len\n",
    "        self.max_seqlen = max_seq_len\n",
    "        self.protein2seq = load_dict('EC2prot2seq.pkl')\n",
    "        self.read_ppi()\n",
    "        self.protein2onehot = {}\n",
    "        self.onehot_seqs()\n",
    "        self.prot2emb_bert =  prot2emb_bert\n",
    "        self.prot2emb_w2v = prot2emb_w2v\n",
    "         \n",
    "#         self.prot2embedding() \n",
    "         \n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def read_ppi(self):\n",
    "        with open(self.ppi_pair_file, 'r') as f:\n",
    "            self.ppi_pairs  =  f.readlines()\n",
    "    \n",
    "    def onehot_seqs(self):\n",
    "        for key, value in self.protein2seq.items():\n",
    "            self.protein2onehot[key] =  protein_one_hot(value, self.max_seqlen) \n",
    "    \n",
    "#     def prot2embedding(self):\n",
    "#         for key, value in self.protein2go.items():\n",
    "#             X_go1 =  np.zeros((1,768))\n",
    "#             allgos = value.split(',') \n",
    "#             allgos = list(set(allgos))\n",
    "#             count = 0\n",
    "#             words4sent = []\n",
    "#             for  go in  allgos:\n",
    "#                 if len(go) > 2:\n",
    "#                     feature = np.load('../ncbi_allfeatures4go/'+go+'_0.npy')[1:-1]\n",
    "#                     words4sent.append(feature)\n",
    "#                 if count + feature.shape[0] > max_go_len:\n",
    "#                     break    \n",
    "#                 count += feature.shape[0] \n",
    "#             if len(words4sent) > 0:\n",
    "#                 sent_embedding = self.bertlstm.encode(words4sent)\n",
    "#             else:\n",
    "#                 sent_embedding = np.zeros((1, self.projection_dim))\n",
    "#             self.prot2emb[key] = sent_embedding \n",
    "    \n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.ppi_pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.ppi_pairs))\n",
    "         \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        X_seq1 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        X_seq2 = np.empty((self.batch_size, self.max_seqlen,20))\n",
    "        y = np.empty((self.batch_size))\n",
    "        X_go1 = np.empty((self.batch_size, self.bert_len + self.w2v_len,self.projection_dim))\n",
    "        X_go2 = np.empty((self.batch_size, self.bert_len + self.w2v_len,self.projection_dim))\n",
    "\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "            \n",
    "            prot1emb_w2v = self.prot2emb_w2v[p1]\n",
    "            X_go1[i,prot1emb_bert.shape[0]:prot1emb_w2v.shape[0] + prot1emb_bert.shape[0]] = prot1emb_w2v\n",
    "            \n",
    "            prot2emb_w2v = self.prot2emb_w2v[p2]\n",
    "            X_go2[i,prot2emb_bert.shape[0]:prot2emb_bert.shape[0] + prot2emb_w2v.shape[0] ] = prot2emb_w2v\n",
    "            \n",
    "             \n",
    "            \n",
    "            \n",
    "        return [X_go1, X_go2,  X_seq1, X_seq2] ,  y\n",
    "\n",
    "\n",
    "\n",
    "    def all_data(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "\n",
    "         \n",
    "        X_seq1 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "\n",
    "         \n",
    "        X_seq2 = np.empty((len(list_IDs_temp), self.max_seqlen,20))\n",
    "        y = np.empty((len(list_IDs_temp)))\n",
    "        X_go1 = np.empty((len(list_IDs_temp), self.bert_len + self.w2v_len,self.projection_dim))\n",
    "        X_go2 = np.empty((len(list_IDs_temp), self.bert_len + self.w2v_len,self.projection_dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            ppi_pair = self.ppi_pairs[ID]\n",
    "            p1, p2, label = ppi_pair.rstrip().split('\\t')\n",
    "            if label == '+':\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "            X_seq1[i] =  self.protein2onehot[p1]\n",
    "            X_seq2[i] =  self.protein2onehot[p2]\n",
    "            \n",
    "            prot1emb_bert = self.prot2emb_bert[p1]\n",
    "            X_go1[i,:prot1emb_bert.shape[0]] = prot1emb_bert\n",
    "            \n",
    "            prot2emb_bert = self.prot2emb_bert[p2]\n",
    "            X_go2[i,:prot2emb_bert.shape[0]] = prot2emb_bert\n",
    "            \n",
    "            prot1emb_w2v = self.prot2emb_w2v[p1]\n",
    "            X_go1[i,prot1emb_bert.shape[0]:prot1emb_w2v.shape[0] + prot1emb_bert.shape[0]] = prot1emb_w2v\n",
    "            \n",
    "            prot2emb_w2v = self.prot2emb_w2v[p2]\n",
    "            X_go2[i,prot2emb_bert.shape[0]:prot2emb_bert.shape[0] + prot2emb_w2v.shape[0] ] = prot2emb_w2v\n",
    "            \n",
    "           \n",
    "        return [X_go1, X_go2,  X_seq1, X_seq2] ,  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_54\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_213 (InputLayer)          (None, 44, 2048)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_214 (InputLayer)          (None, 44, 2048)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_215 (InputLayer)          (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_216 (InputLayer)          (None, 1000, 20)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1273 (Conv1D)            (None, 44, 64)       393280      input_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1275 (Conv1D)            (None, 44, 64)       131136      input_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1279 (Conv1D)            (None, 44, 64)       393280      input_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1281 (Conv1D)            (None, 44, 64)       131136      input_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1285 (Conv1D)            (None, 1000, 16)     976         input_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1287 (Conv1D)            (None, 1000, 16)     336         input_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1291 (Conv1D)            (None, 1000, 16)     976         input_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1293 (Conv1D)            (None, 1000, 16)     336         input_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1274 (Conv1D)            (None, 44, 64)       20544       conv1d_1273[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1276 (Conv1D)            (None, 44, 64)       12352       conv1d_1275[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1277 (Conv1D)            (None, 44, 64)       393280      input_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1278 (Conv1D)            (None, 44, 64)       131136      input_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1280 (Conv1D)            (None, 44, 64)       20544       conv1d_1279[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1282 (Conv1D)            (None, 44, 64)       12352       conv1d_1281[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1283 (Conv1D)            (None, 44, 64)       393280      input_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1284 (Conv1D)            (None, 44, 64)       131136      input_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1286 (Conv1D)            (None, 1000, 16)     1296        conv1d_1285[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1288 (Conv1D)            (None, 1000, 16)     784         conv1d_1287[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1289 (Conv1D)            (None, 1000, 16)     976         input_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1290 (Conv1D)            (None, 1000, 16)     336         input_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1292 (Conv1D)            (None, 1000, 16)     1296        conv1d_1291[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1294 (Conv1D)            (None, 1000, 16)     784         conv1d_1293[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1295 (Conv1D)            (None, 1000, 16)     976         input_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1296 (Conv1D)            (None, 1000, 16)     336         input_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_410 (Concatenate)   (None, 44, 256)      0           conv1d_1274[0][0]                \n",
      "                                                                 conv1d_1276[0][0]                \n",
      "                                                                 conv1d_1277[0][0]                \n",
      "                                                                 conv1d_1278[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_77 (Bidirectional (None, 44, 128)      811776      input_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_412 (Concatenate)   (None, 44, 256)      0           conv1d_1280[0][0]                \n",
      "                                                                 conv1d_1282[0][0]                \n",
      "                                                                 conv1d_1283[0][0]                \n",
      "                                                                 conv1d_1284[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_78 (Bidirectional (None, 44, 128)      811776      input_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_414 (Concatenate)   (None, 1000, 64)     0           conv1d_1286[0][0]                \n",
      "                                                                 conv1d_1288[0][0]                \n",
      "                                                                 conv1d_1289[0][0]                \n",
      "                                                                 conv1d_1290[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_79 (Bidirectional (None, 1000, 128)    33024       input_215[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_416 (Concatenate)   (None, 1000, 64)     0           conv1d_1292[0][0]                \n",
      "                                                                 conv1d_1294[0][0]                \n",
      "                                                                 conv1d_1295[0][0]                \n",
      "                                                                 conv1d_1296[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_80 (Bidirectional (None, 1000, 128)    33024       input_216[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_395 (Dropout)           (None, 44, 256)      0           concatenate_410[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_396 (Dropout)           (None, 44, 128)      0           bidirectional_77[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_397 (Dropout)           (None, 44, 256)      0           concatenate_412[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_398 (Dropout)           (None, 44, 128)      0           bidirectional_78[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_399 (Dropout)           (None, 1000, 64)     0           concatenate_414[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_400 (Dropout)           (None, 1000, 128)    0           bidirectional_79[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_401 (Dropout)           (None, 1000, 64)     0           concatenate_416[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_402 (Dropout)           (None, 1000, 128)    0           bidirectional_80[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_217 (G (None, 256)          0           dropout_395[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_269 (Attention)       (None, 256)          300         dropout_395[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_270 (Attention)       (None, 128)          172         dropout_396[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_218 (G (None, 128)          0           dropout_396[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_219 (G (None, 256)          0           dropout_397[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_271 (Attention)       (None, 256)          300         dropout_397[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_272 (Attention)       (None, 128)          172         dropout_398[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_220 (G (None, 128)          0           dropout_398[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_221 (G (None, 64)           0           dropout_399[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_273 (Attention)       (None, 64)           1064        dropout_399[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_274 (Attention)       (None, 128)          1128        dropout_400[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_222 (G (None, 128)          0           dropout_400[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_223 (G (None, 64)           0           dropout_401[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_275 (Attention)       (None, 64)           1064        dropout_401[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_276 (Attention)       (None, 128)          1128        dropout_402[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_224 (G (None, 128)          0           dropout_402[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_411 (Concatenate)   (None, 768)          0           global_average_pooling1d_217[0][0\n",
      "                                                                 attention_269[0][0]              \n",
      "                                                                 attention_270[0][0]              \n",
      "                                                                 global_average_pooling1d_218[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_413 (Concatenate)   (None, 768)          0           global_average_pooling1d_219[0][0\n",
      "                                                                 attention_271[0][0]              \n",
      "                                                                 attention_272[0][0]              \n",
      "                                                                 global_average_pooling1d_220[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_415 (Concatenate)   (None, 384)          0           global_average_pooling1d_221[0][0\n",
      "                                                                 attention_273[0][0]              \n",
      "                                                                 attention_274[0][0]              \n",
      "                                                                 global_average_pooling1d_222[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_417 (Concatenate)   (None, 384)          0           global_average_pooling1d_223[0][0\n",
      "                                                                 attention_275[0][0]              \n",
      "                                                                 attention_276[0][0]              \n",
      "                                                                 global_average_pooling1d_224[0][0\n",
      "__________________________________________________________________________________________________\n",
      "concatenate_418 (Concatenate)   (None, 2304)         0           concatenate_411[0][0]            \n",
      "                                                                 concatenate_413[0][0]            \n",
      "                                                                 concatenate_415[0][0]            \n",
      "                                                                 concatenate_417[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_341 (Dense)               (None, 1024)         2360320     concatenate_418[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_403 (Dropout)           (None, 1024)         0           dense_341[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_342 (Dense)               (None, 1024)         1049600     dropout_403[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_404 (Dropout)           (None, 1024)         0           dense_342[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_343 (Dense)               (None, 512)          524800      dropout_404[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_344 (Dense)               (None, 1)            513         dense_343[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 1)            0           dense_344[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,803,025\n",
      "Trainable params: 7,803,025\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import   Embedding\n",
    "from keras.layers import  GRU, Bidirectional, CuDNNGRU, Lambda, Flatten\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "def inception_block(input_tensor, output_size):\n",
    "    \"\"\"\"\"\"\n",
    "    con1d_filters = int(output_size/4)\n",
    "    y = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x1 = Conv1D(con1d_filters, 5, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    y = Conv1D(con1d_filters, 1, activation=\"relu\", padding='valid')(input_tensor)\n",
    "    x2 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(y)\n",
    "\n",
    "    x3 = Conv1D(con1d_filters, 3, activation=\"relu\", padding='same')(input_tensor)\n",
    "    x4 = Conv1D(con1d_filters, 1, activation=\"relu\", padding='same')(input_tensor)\n",
    "\n",
    "    y = Concatenate()([x1, x2, x3, x4])\n",
    "#     y = MaxPooling1D(4)(mix0)\n",
    "    # y = AveragePooling1D()(mix0)\n",
    "#     y = BatchNormalization()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "def build_cnn_gru_model(input_x, con_filters, gru_units):\n",
    "    x = inception_block(input_x,con_filters )\n",
    "    x = Dropout(0.3)(x)\n",
    "    x_gru = Bidirectional(CuDNNGRU(gru_units, return_sequences=True))(input_x)\n",
    "    x_gru = Dropout(0.3)(x_gru)\n",
    "     \n",
    "    x_a = GlobalAveragePooling1D()(x)\n",
    "#     x_b = GlobalMaxPooling1D()(x)\n",
    "    x_c = Attention()(x)\n",
    "    x_gru_a = GlobalAveragePooling1D()(x_gru)\n",
    "#     x_gru_b = GlobalMaxPooling1D()(x_gru)\n",
    "    x_gru_c = Attention()(x_gru)\n",
    "    x = Concatenate()([ x_a, x_c, x_gru_c,   x_gru_a])\n",
    "    return x\n",
    " \n",
    "\n",
    "def build_model():\n",
    "    con_filters = 256\n",
    "    gru_units = 64\n",
    "    left_input_go = Input(shape=(bert_len+w2v_len,project_dim))\n",
    "    right_input_go = Input(shape=(bert_len+w2v_len,project_dim))\n",
    "    \n",
    "    \n",
    "    left_input_seq = Input(shape=(max_seq_len,20))\n",
    "    right_input_seq = Input(shape=(max_seq_len,20))\n",
    "    \n",
    "    left_x_go = build_cnn_gru_model(left_input_go, con_filters, gru_units)\n",
    "    right_x_go = build_cnn_gru_model(right_input_go, con_filters,gru_units)\n",
    "    left_x_seq = build_cnn_gru_model(left_input_seq, con_filters//4, gru_units)\n",
    "    right_x_seq = build_cnn_gru_model(right_input_seq, con_filters//4, gru_units)\n",
    "\n",
    "   \n",
    "   \n",
    "    x =   Concatenate()([left_x_go  , right_x_go, left_x_seq, right_x_seq])\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "  \n",
    "     \n",
    "    x = Dense(1)(x)\n",
    "    output = Activation('sigmoid')(x)\n",
    "    # model = Model([left_input_go, right_input_go], output)\n",
    "  \n",
    "    model = Model([left_input_go, right_input_go, left_input_seq, right_input_seq], output)\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 52s 2s/step - loss: 0.5863 - acc: 0.7107 - val_loss: 0.4418 - val_acc: 0.7708\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 276ms/step - loss: 0.4376 - acc: 0.8044 - val_loss: 0.4471 - val_acc: 0.7865\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.3974 - acc: 0.8190 - val_loss: 0.4137 - val_acc: 0.7969\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.3774 - acc: 0.8351 - val_loss: 0.3789 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.3658 - acc: 0.8405 - val_loss: 0.3958 - val_acc: 0.7917\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.3415 - acc: 0.8502 - val_loss: 0.4011 - val_acc: 0.8177\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.3196 - acc: 0.8540 - val_loss: 0.3764 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.3905 - acc: 0.8314 - val_loss: 0.3827 - val_acc: 0.8229\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 8s 281ms/step - loss: 0.3289 - acc: 0.8513 - val_loss: 0.4199 - val_acc: 0.8125\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.3037 - acc: 0.8540 - val_loss: 0.3064 - val_acc: 0.8542\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 0.3088 - acc: 0.8534 - val_loss: 0.3391 - val_acc: 0.8073\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.2832 - acc: 0.8653 - val_loss: 0.4115 - val_acc: 0.7656\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 0.2789 - acc: 0.8648 - val_loss: 0.2983 - val_acc: 0.8490\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.2902 - acc: 0.8561 - val_loss: 0.5594 - val_acc: 0.8073\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 8s 278ms/step - loss: 0.2929 - acc: 0.8798 - val_loss: 0.3213 - val_acc: 0.8438\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.2807 - acc: 0.8664 - val_loss: 0.3084 - val_acc: 0.8438\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.2694 - acc: 0.8772 - val_loss: 0.3218 - val_acc: 0.8542\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.2463 - acc: 0.8922 - val_loss: 0.3433 - val_acc: 0.8385\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.2605 - acc: 0.8766 - val_loss: 0.2532 - val_acc: 0.8854\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.2428 - acc: 0.8815 - val_loss: 0.3722 - val_acc: 0.8438\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.2096 - acc: 0.9014 - val_loss: 0.2649 - val_acc: 0.8958\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.2176 - acc: 0.9030 - val_loss: 0.2629 - val_acc: 0.8750\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.2130 - acc: 0.9138 - val_loss: 0.3123 - val_acc: 0.8854\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.1634 - acc: 0.9256 - val_loss: 0.3505 - val_acc: 0.8594\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.1593 - acc: 0.9262 - val_loss: 0.2714 - val_acc: 0.9010\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.1628 - acc: 0.9273 - val_loss: 0.3038 - val_acc: 0.8542\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.1770 - acc: 0.9197 - val_loss: 0.4298 - val_acc: 0.8646\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 8s 279ms/step - loss: 0.2043 - acc: 0.9116 - val_loss: 0.3594 - val_acc: 0.8490\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.1737 - acc: 0.9267 - val_loss: 0.5203 - val_acc: 0.8073\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.1778 - acc: 0.9332 - val_loss: 0.3132 - val_acc: 0.8854\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.0922 - acc: 0.9596 - val_loss: 0.5576 - val_acc: 0.8333\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.1109 - acc: 0.9591 - val_loss: 0.4089 - val_acc: 0.8958\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 0.1245 - acc: 0.9494 - val_loss: 0.3451 - val_acc: 0.8490\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.1231 - acc: 0.9499 - val_loss: 0.3385 - val_acc: 0.8750\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.0909 - acc: 0.9634 - val_loss: 0.3496 - val_acc: 0.8906\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.976393\n",
      "ACC: 0.914530\n",
      "MCC : 0.828392\n",
      "TPR:0.927419\n",
      "FPR:0.100000\n",
      "Pre:0.912698\n",
      "F1:0.920000\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 51s 2s/step - loss: 0.4965 - acc: 0.7500 - val_loss: 0.4699 - val_acc: 0.7865\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.5900 - acc: 0.6832 - val_loss: 0.6774 - val_acc: 0.6198\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.4992 - acc: 0.7721 - val_loss: 0.4772 - val_acc: 0.7969\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.4046 - acc: 0.8211 - val_loss: 0.5396 - val_acc: 0.7604\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.4163 - acc: 0.8082 - val_loss: 0.4247 - val_acc: 0.8073\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 8s 281ms/step - loss: 0.3688 - acc: 0.8367 - val_loss: 0.4044 - val_acc: 0.8125\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 8s 286ms/step - loss: 0.3571 - acc: 0.8362 - val_loss: 0.3724 - val_acc: 0.8177\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 8s 281ms/step - loss: 0.3552 - acc: 0.8389 - val_loss: 0.3820 - val_acc: 0.8281\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 0.3832 - acc: 0.8303 - val_loss: 0.4330 - val_acc: 0.8021\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.3446 - acc: 0.8357 - val_loss: 0.3784 - val_acc: 0.8177\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 8s 279ms/step - loss: 0.3402 - acc: 0.8394 - val_loss: 0.3606 - val_acc: 0.8385\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 282ms/step - loss: 0.3297 - acc: 0.8448 - val_loss: 0.4561 - val_acc: 0.8177\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.3406 - acc: 0.8351 - val_loss: 0.3657 - val_acc: 0.8333\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 8s 275ms/step - loss: 0.3112 - acc: 0.8556 - val_loss: 0.3820 - val_acc: 0.8229\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.3189 - acc: 0.8486 - val_loss: 0.3251 - val_acc: 0.8333\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.3195 - acc: 0.8432 - val_loss: 0.3458 - val_acc: 0.8229\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.2847 - acc: 0.8610 - val_loss: 0.3695 - val_acc: 0.8333\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.2868 - acc: 0.8685 - val_loss: 0.3327 - val_acc: 0.8490\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.2621 - acc: 0.8777 - val_loss: 0.3740 - val_acc: 0.8542\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 8s 282ms/step - loss: 0.2735 - acc: 0.8793 - val_loss: 0.4123 - val_acc: 0.7917\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.2842 - acc: 0.8675 - val_loss: 0.4135 - val_acc: 0.8229\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 276ms/step - loss: 0.2770 - acc: 0.8793 - val_loss: 0.3916 - val_acc: 0.8281\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 8s 279ms/step - loss: 0.2911 - acc: 0.8653 - val_loss: 0.3610 - val_acc: 0.8281\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.2443 - acc: 0.8917 - val_loss: 0.3448 - val_acc: 0.8385\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 0.2578 - acc: 0.8750 - val_loss: 0.3739 - val_acc: 0.8438\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.2085 - acc: 0.9089 - val_loss: 0.3630 - val_acc: 0.8542\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.1986 - acc: 0.9133 - val_loss: 0.3621 - val_acc: 0.8281\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 0.1950 - acc: 0.9186 - val_loss: 0.3723 - val_acc: 0.8594\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.1839 - acc: 0.9176 - val_loss: 0.4171 - val_acc: 0.8542\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 0.2116 - acc: 0.9116 - val_loss: 0.3257 - val_acc: 0.8750\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.2186 - acc: 0.9057 - val_loss: 0.3495 - val_acc: 0.8646\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.1703 - acc: 0.9278 - val_loss: 0.3886 - val_acc: 0.8385\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.1430 - acc: 0.9391 - val_loss: 0.3864 - val_acc: 0.8490\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.1146 - acc: 0.9558 - val_loss: 0.5536 - val_acc: 0.8281\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.1396 - acc: 0.9445 - val_loss: 0.4114 - val_acc: 0.8333\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.1884 - acc: 0.9305 - val_loss: 0.4657 - val_acc: 0.8438\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.1140 - acc: 0.9510 - val_loss: 0.4228 - val_acc: 0.8646\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.1141 - acc: 0.9542 - val_loss: 0.4370 - val_acc: 0.8698\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.1212 - acc: 0.9461 - val_loss: 0.3573 - val_acc: 0.8750\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.1141 - acc: 0.9526 - val_loss: 0.6562 - val_acc: 0.8177\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.964944\n",
      "ACC: 0.910256\n",
      "MCC : 0.817181\n",
      "TPR:0.882353\n",
      "FPR:0.068182\n",
      "Pre:0.909091\n",
      "F1:0.895522\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 54s 2s/step - loss: 0.5335 - acc: 0.7338 - val_loss: 0.4600 - val_acc: 0.7969\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.4370 - acc: 0.8120 - val_loss: 0.4002 - val_acc: 0.8177\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.3957 - acc: 0.8281 - val_loss: 0.3962 - val_acc: 0.8229\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.3823 - acc: 0.8287 - val_loss: 0.3906 - val_acc: 0.8229\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.3830 - acc: 0.8238 - val_loss: 0.3883 - val_acc: 0.8281\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.3603 - acc: 0.8394 - val_loss: 0.4172 - val_acc: 0.8438\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 0.3523 - acc: 0.8416 - val_loss: 0.3758 - val_acc: 0.8281\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.3511 - acc: 0.8346 - val_loss: 0.3674 - val_acc: 0.8281\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.3334 - acc: 0.8464 - val_loss: 0.4306 - val_acc: 0.8333\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.3745 - acc: 0.8249 - val_loss: 0.3718 - val_acc: 0.8750\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.3430 - acc: 0.8389 - val_loss: 0.4142 - val_acc: 0.8333\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.3355 - acc: 0.8416 - val_loss: 0.4525 - val_acc: 0.8229\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.3265 - acc: 0.8427 - val_loss: 0.3656 - val_acc: 0.8542\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.3237 - acc: 0.8508 - val_loss: 0.3514 - val_acc: 0.8594\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.3018 - acc: 0.8578 - val_loss: 0.3476 - val_acc: 0.8698\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.2853 - acc: 0.8691 - val_loss: 0.3243 - val_acc: 0.8646\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.2786 - acc: 0.8658 - val_loss: 0.3080 - val_acc: 0.8750\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 8s 277ms/step - loss: 0.2759 - acc: 0.8648 - val_loss: 0.3203 - val_acc: 0.8802\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.2663 - acc: 0.8782 - val_loss: 0.3266 - val_acc: 0.8698\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.3688 - acc: 0.8529 - val_loss: 0.3595 - val_acc: 0.8177\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.2812 - acc: 0.8777 - val_loss: 0.3382 - val_acc: 0.8750\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.2482 - acc: 0.8917 - val_loss: 0.3326 - val_acc: 0.8594\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2494 - acc: 0.8912 - val_loss: 0.3831 - val_acc: 0.8229\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.2189 - acc: 0.8987 - val_loss: 0.3912 - val_acc: 0.8490\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.2095 - acc: 0.9057 - val_loss: 0.3628 - val_acc: 0.8750\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 0.2157 - acc: 0.9036 - val_loss: 0.3510 - val_acc: 0.8594\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.1990 - acc: 0.9025 - val_loss: 0.3399 - val_acc: 0.8906\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.1746 - acc: 0.9203 - val_loss: 0.4061 - val_acc: 0.8854\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.1574 - acc: 0.9300 - val_loss: 0.3851 - val_acc: 0.9010\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.1372 - acc: 0.9386 - val_loss: 0.4407 - val_acc: 0.9010\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.1397 - acc: 0.9440 - val_loss: 0.4229 - val_acc: 0.8594\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.1509 - acc: 0.9332 - val_loss: 0.4955 - val_acc: 0.8958\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.1152 - acc: 0.9520 - val_loss: 0.5516 - val_acc: 0.8490\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.1395 - acc: 0.9423 - val_loss: 0.5767 - val_acc: 0.8854\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.1498 - acc: 0.9386 - val_loss: 0.4539 - val_acc: 0.8854\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.1218 - acc: 0.9494 - val_loss: 0.5030 - val_acc: 0.8698\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.1038 - acc: 0.9634 - val_loss: 0.5201 - val_acc: 0.8698\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 0.0822 - acc: 0.9628 - val_loss: 0.6405 - val_acc: 0.8698\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.0968 - acc: 0.9617 - val_loss: 0.6037 - val_acc: 0.8750\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.974649\n",
      "ACC: 0.944444\n",
      "MCC : 0.891592\n",
      "TPR:0.906780\n",
      "FPR:0.017241\n",
      "Pre:0.981651\n",
      "F1:0.942731\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 54s 2s/step - loss: 0.5168 - acc: 0.7295 - val_loss: 0.4158 - val_acc: 0.8021\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.4159 - acc: 0.8071 - val_loss: 0.4178 - val_acc: 0.8073\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.4264 - acc: 0.8055 - val_loss: 0.4570 - val_acc: 0.8229\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.3735 - acc: 0.8411 - val_loss: 0.3379 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 0.3795 - acc: 0.8297 - val_loss: 0.3634 - val_acc: 0.8281\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 8s 260ms/step - loss: 0.3431 - acc: 0.8470 - val_loss: 0.3770 - val_acc: 0.8385\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.3348 - acc: 0.8497 - val_loss: 0.3634 - val_acc: 0.8438\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 0.3082 - acc: 0.8626 - val_loss: 0.4325 - val_acc: 0.8333\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.3167 - acc: 0.8578 - val_loss: 0.3887 - val_acc: 0.8490\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.2968 - acc: 0.8718 - val_loss: 0.3550 - val_acc: 0.8490\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.2834 - acc: 0.8696 - val_loss: 0.3537 - val_acc: 0.8542\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.2986 - acc: 0.8594 - val_loss: 0.3569 - val_acc: 0.8490\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.2760 - acc: 0.8696 - val_loss: 0.4288 - val_acc: 0.8177\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.2834 - acc: 0.8669 - val_loss: 0.4229 - val_acc: 0.8542\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2837 - acc: 0.8739 - val_loss: 0.3601 - val_acc: 0.8646\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.2719 - acc: 0.8728 - val_loss: 0.3714 - val_acc: 0.8438\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.2322 - acc: 0.8955 - val_loss: 0.3445 - val_acc: 0.8385\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.2553 - acc: 0.8766 - val_loss: 0.3092 - val_acc: 0.8802\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2127 - acc: 0.9073 - val_loss: 0.3776 - val_acc: 0.8542\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.2103 - acc: 0.9068 - val_loss: 0.4512 - val_acc: 0.8542\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2427 - acc: 0.8858 - val_loss: 0.4112 - val_acc: 0.8177\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2146 - acc: 0.8998 - val_loss: 0.3702 - val_acc: 0.8385\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 0.2463 - acc: 0.8804 - val_loss: 0.3587 - val_acc: 0.8438\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2171 - acc: 0.9019 - val_loss: 0.3971 - val_acc: 0.7865\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 7s 251ms/step - loss: 0.1728 - acc: 0.9251 - val_loss: 0.3499 - val_acc: 0.8854\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.1445 - acc: 0.9413 - val_loss: 0.3431 - val_acc: 0.8542\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.1410 - acc: 0.9375 - val_loss: 0.4069 - val_acc: 0.8646\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.1283 - acc: 0.9423 - val_loss: 0.4685 - val_acc: 0.8646\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.1401 - acc: 0.9359 - val_loss: 0.3473 - val_acc: 0.8802\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.1619 - acc: 0.9337 - val_loss: 0.3362 - val_acc: 0.8698\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.1074 - acc: 0.9569 - val_loss: 0.4426 - val_acc: 0.8594\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.1058 - acc: 0.9607 - val_loss: 0.4504 - val_acc: 0.8385\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.1122 - acc: 0.9520 - val_loss: 0.3883 - val_acc: 0.8490\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.0968 - acc: 0.9596 - val_loss: 0.5372 - val_acc: 0.8542\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.0572 - acc: 0.9822 - val_loss: 0.7121 - val_acc: 0.8750\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.986291\n",
      "ACC: 0.923077\n",
      "MCC : 0.845166\n",
      "TPR:0.886792\n",
      "FPR:0.046875\n",
      "Pre:0.940000\n",
      "F1:0.912621\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 55s 2s/step - loss: 0.5273 - acc: 0.7333 - val_loss: 0.5863 - val_acc: 0.7292\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.4008 - acc: 0.8254 - val_loss: 0.5280 - val_acc: 0.7969\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3885 - acc: 0.8270 - val_loss: 0.4042 - val_acc: 0.7917\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.3476 - acc: 0.8481 - val_loss: 0.4477 - val_acc: 0.7865\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.3648 - acc: 0.8324 - val_loss: 0.4159 - val_acc: 0.7708\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.3607 - acc: 0.8394 - val_loss: 0.4302 - val_acc: 0.7344\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.3283 - acc: 0.8524 - val_loss: 0.3906 - val_acc: 0.8021\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.3222 - acc: 0.8610 - val_loss: 0.4565 - val_acc: 0.7865\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.3108 - acc: 0.8648 - val_loss: 0.4154 - val_acc: 0.7969\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.3082 - acc: 0.8653 - val_loss: 0.5446 - val_acc: 0.7604\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.2981 - acc: 0.8712 - val_loss: 0.6100 - val_acc: 0.7760\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.2787 - acc: 0.8739 - val_loss: 0.4648 - val_acc: 0.7969\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.2907 - acc: 0.8631 - val_loss: 0.4728 - val_acc: 0.7708\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.3239 - acc: 0.8513 - val_loss: 0.3958 - val_acc: 0.7865\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.3045 - acc: 0.8658 - val_loss: 0.3996 - val_acc: 0.7969\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2518 - acc: 0.8906 - val_loss: 0.4209 - val_acc: 0.8229\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 7s 251ms/step - loss: 0.2279 - acc: 0.8960 - val_loss: 0.3904 - val_acc: 0.8229\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2517 - acc: 0.8809 - val_loss: 0.3739 - val_acc: 0.8177\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.2517 - acc: 0.8890 - val_loss: 0.4105 - val_acc: 0.8073\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 7s 250ms/step - loss: 0.2644 - acc: 0.8707 - val_loss: 0.4832 - val_acc: 0.7760\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.2293 - acc: 0.8917 - val_loss: 0.4927 - val_acc: 0.7448\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2940 - acc: 0.8718 - val_loss: 0.3764 - val_acc: 0.8177\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.2284 - acc: 0.8939 - val_loss: 0.4437 - val_acc: 0.7500\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.2272 - acc: 0.8960 - val_loss: 0.4472 - val_acc: 0.8073\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2087 - acc: 0.9106 - val_loss: 0.4606 - val_acc: 0.8229\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.2049 - acc: 0.9079 - val_loss: 0.3426 - val_acc: 0.8542\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.1654 - acc: 0.9310 - val_loss: 0.3934 - val_acc: 0.8177\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.1567 - acc: 0.9321 - val_loss: 0.4023 - val_acc: 0.8438\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.1587 - acc: 0.9321 - val_loss: 0.4421 - val_acc: 0.8385\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.1327 - acc: 0.9434 - val_loss: 0.3614 - val_acc: 0.8594\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: 0.1180 - acc: 0.9499 - val_loss: 0.3332 - val_acc: 0.9062\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.1050 - acc: 0.9531 - val_loss: 0.4218 - val_acc: 0.8698\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 7s 251ms/step - loss: 0.0934 - acc: 0.9639 - val_loss: 0.4338 - val_acc: 0.8854\n",
      "Epoch 34/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.1071 - acc: 0.9558 - val_loss: 0.5635 - val_acc: 0.8229\n",
      "Epoch 35/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.0841 - acc: 0.9655 - val_loss: 0.4776 - val_acc: 0.8594\n",
      "Epoch 36/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.0913 - acc: 0.9634 - val_loss: 0.4509 - val_acc: 0.8594\n",
      "Epoch 37/100\n",
      "29/29 [==============================] - 7s 250ms/step - loss: 0.0696 - acc: 0.9752 - val_loss: 0.4651 - val_acc: 0.8385\n",
      "Epoch 38/100\n",
      "29/29 [==============================] - 7s 250ms/step - loss: 0.0877 - acc: 0.9677 - val_loss: 0.5315 - val_acc: 0.8542\n",
      "Epoch 39/100\n",
      "29/29 [==============================] - 7s 251ms/step - loss: 0.1349 - acc: 0.9456 - val_loss: 0.5181 - val_acc: 0.8490\n",
      "Epoch 40/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.0912 - acc: 0.9650 - val_loss: 0.3960 - val_acc: 0.8802\n",
      "Epoch 41/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.0475 - acc: 0.9817 - val_loss: 0.6540 - val_acc: 0.8698\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.992383\n",
      "ACC: 0.970085\n",
      "MCC : 0.940074\n",
      "TPR:0.967480\n",
      "FPR:0.027027\n",
      "Pre:0.975410\n",
      "F1:0.971429\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 56s 2s/step - loss: 0.5667 - acc: 0.7247 - val_loss: 0.4593 - val_acc: 0.8073\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.4321 - acc: 0.8055 - val_loss: 0.4491 - val_acc: 0.7917\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.3929 - acc: 0.8249 - val_loss: 0.4511 - val_acc: 0.7812\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.4239 - acc: 0.8066 - val_loss: 0.3964 - val_acc: 0.8229\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: 0.3534 - acc: 0.8384 - val_loss: 0.4557 - val_acc: 0.7969\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.3729 - acc: 0.8297 - val_loss: 0.4113 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3551 - acc: 0.8367 - val_loss: 0.5138 - val_acc: 0.7812\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3453 - acc: 0.8405 - val_loss: 0.3761 - val_acc: 0.8438\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.3919 - acc: 0.8157 - val_loss: 0.5687 - val_acc: 0.7865\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 8s 273ms/step - loss: 0.3405 - acc: 0.8411 - val_loss: 0.6199 - val_acc: 0.7292\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.3128 - acc: 0.8540 - val_loss: 0.4887 - val_acc: 0.7917\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.3200 - acc: 0.8481 - val_loss: 0.3855 - val_acc: 0.8281\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 272ms/step - loss: 0.2952 - acc: 0.8648 - val_loss: 0.4003 - val_acc: 0.8490\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2942 - acc: 0.8675 - val_loss: 0.4288 - val_acc: 0.8229\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3160 - acc: 0.8578 - val_loss: 0.3897 - val_acc: 0.8229\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.2927 - acc: 0.8658 - val_loss: 0.4094 - val_acc: 0.8333\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 7s 250ms/step - loss: 0.2947 - acc: 0.8680 - val_loss: 0.5044 - val_acc: 0.8438\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 7s 251ms/step - loss: 0.3539 - acc: 0.8529 - val_loss: 0.3765 - val_acc: 0.8281\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.2674 - acc: 0.8804 - val_loss: 0.3961 - val_acc: 0.8177\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2441 - acc: 0.8825 - val_loss: 0.4036 - val_acc: 0.8542\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2502 - acc: 0.8901 - val_loss: 0.4147 - val_acc: 0.8646\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.2220 - acc: 0.9046 - val_loss: 0.4106 - val_acc: 0.8594\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.2099 - acc: 0.9122 - val_loss: 0.4092 - val_acc: 0.8490\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.1887 - acc: 0.9149 - val_loss: 0.3902 - val_acc: 0.8229\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.1856 - acc: 0.9143 - val_loss: 0.4646 - val_acc: 0.8125\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.2297 - acc: 0.8852 - val_loss: 0.4106 - val_acc: 0.8229\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.1928 - acc: 0.9106 - val_loss: 0.3583 - val_acc: 0.8438\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.1613 - acc: 0.9246 - val_loss: 0.4353 - val_acc: 0.8281\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.1386 - acc: 0.9391 - val_loss: 0.4226 - val_acc: 0.8490\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.1332 - acc: 0.9429 - val_loss: 0.4473 - val_acc: 0.8542\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.2176 - acc: 0.9122 - val_loss: 0.4262 - val_acc: 0.8281\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.937742\n",
      "ACC: 0.888889\n",
      "MCC : 0.785581\n",
      "TPR:0.823529\n",
      "FPR:0.043478\n",
      "Pre:0.951456\n",
      "F1:0.882883\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 57s 2s/step - loss: 0.6198 - acc: 0.6821 - val_loss: 0.6435 - val_acc: 0.6354\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.4741 - acc: 0.7883 - val_loss: 0.4608 - val_acc: 0.7760\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: 0.4043 - acc: 0.8179 - val_loss: 0.4031 - val_acc: 0.8125\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.3654 - acc: 0.8384 - val_loss: 0.4420 - val_acc: 0.7812\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 7s 250ms/step - loss: 0.3844 - acc: 0.8244 - val_loss: 0.3810 - val_acc: 0.8125\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 7s 247ms/step - loss: 0.3585 - acc: 0.8367 - val_loss: 0.4082 - val_acc: 0.8073\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.3525 - acc: 0.8416 - val_loss: 0.3466 - val_acc: 0.8490\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.3375 - acc: 0.8513 - val_loss: 0.3631 - val_acc: 0.8385\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 7s 250ms/step - loss: 0.3227 - acc: 0.8518 - val_loss: 0.3379 - val_acc: 0.8646\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 8s 263ms/step - loss: 0.2947 - acc: 0.8615 - val_loss: 0.3807 - val_acc: 0.8385\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2850 - acc: 0.8728 - val_loss: 0.3347 - val_acc: 0.8438\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2749 - acc: 0.8755 - val_loss: 0.3319 - val_acc: 0.8438\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.2820 - acc: 0.8734 - val_loss: 0.3878 - val_acc: 0.8125\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.2876 - acc: 0.8723 - val_loss: 0.3258 - val_acc: 0.8438\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.2713 - acc: 0.8772 - val_loss: 0.3441 - val_acc: 0.8177\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.2834 - acc: 0.8745 - val_loss: 0.3963 - val_acc: 0.8177\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.2406 - acc: 0.8949 - val_loss: 0.3274 - val_acc: 0.8542\n",
      "Epoch 18/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 7s 252ms/step - loss: 0.2639 - acc: 0.8879 - val_loss: 0.3338 - val_acc: 0.8438\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.2396 - acc: 0.8901 - val_loss: 0.3188 - val_acc: 0.8542\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.952233\n",
      "ACC: 0.871795\n",
      "MCC : 0.762126\n",
      "TPR:0.790698\n",
      "FPR:0.028571\n",
      "Pre:0.971429\n",
      "F1:0.871795\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 57s 2s/step - loss: 0.5510 - acc: 0.7360 - val_loss: 0.4451 - val_acc: 0.8021\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.4157 - acc: 0.8109 - val_loss: 0.4009 - val_acc: 0.8385\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.3847 - acc: 0.8244 - val_loss: 0.4794 - val_acc: 0.8229\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.3696 - acc: 0.8378 - val_loss: 0.5197 - val_acc: 0.8438\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.3597 - acc: 0.8330 - val_loss: 0.3804 - val_acc: 0.8542\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3456 - acc: 0.8400 - val_loss: 0.4934 - val_acc: 0.8333\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.3461 - acc: 0.8400 - val_loss: 0.4177 - val_acc: 0.8438\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3270 - acc: 0.8448 - val_loss: 0.4507 - val_acc: 0.8490\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3166 - acc: 0.8518 - val_loss: 0.4770 - val_acc: 0.8594\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.3179 - acc: 0.8513 - val_loss: 0.3963 - val_acc: 0.8750\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.2954 - acc: 0.8621 - val_loss: 0.3700 - val_acc: 0.8542\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.3054 - acc: 0.8675 - val_loss: 0.4261 - val_acc: 0.8281\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.2815 - acc: 0.8734 - val_loss: 0.4501 - val_acc: 0.8021\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2700 - acc: 0.8718 - val_loss: 0.4357 - val_acc: 0.8646\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3459 - acc: 0.8529 - val_loss: 0.4654 - val_acc: 0.8177\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 7s 248ms/step - loss: 0.2792 - acc: 0.8755 - val_loss: 0.4199 - val_acc: 0.8594\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.2621 - acc: 0.8836 - val_loss: 0.3792 - val_acc: 0.8594\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.2442 - acc: 0.8949 - val_loss: 0.4438 - val_acc: 0.8438\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: 0.2553 - acc: 0.8815 - val_loss: 0.3253 - val_acc: 0.8646\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 7s 249ms/step - loss: 0.2214 - acc: 0.9095 - val_loss: 0.3852 - val_acc: 0.8646\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.934894\n",
      "ACC: 0.836207\n",
      "MCC : 0.690524\n",
      "TPR:0.726496\n",
      "FPR:0.052174\n",
      "Pre:0.934066\n",
      "F1:0.817308\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 59s 2s/step - loss: 0.6142 - acc: 0.6816 - val_loss: 0.6148 - val_acc: 0.7500\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 274ms/step - loss: 0.4455 - acc: 0.7996 - val_loss: 0.5163 - val_acc: 0.7188\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 8s 267ms/step - loss: 0.4049 - acc: 0.8200 - val_loss: 0.4159 - val_acc: 0.7656\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.3689 - acc: 0.8362 - val_loss: 0.4580 - val_acc: 0.7552\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.3697 - acc: 0.8314 - val_loss: 0.4333 - val_acc: 0.7708\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.3438 - acc: 0.8464 - val_loss: 0.3875 - val_acc: 0.8177\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.3553 - acc: 0.8346 - val_loss: 0.4065 - val_acc: 0.7760\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.3398 - acc: 0.8481 - val_loss: 0.4442 - val_acc: 0.7865\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.3288 - acc: 0.8540 - val_loss: 0.3710 - val_acc: 0.8125\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.3970 - acc: 0.8330 - val_loss: 0.4602 - val_acc: 0.7604\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 7s 257ms/step - loss: 0.3509 - acc: 0.8448 - val_loss: 0.4183 - val_acc: 0.7812\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.2985 - acc: 0.8621 - val_loss: 0.3571 - val_acc: 0.8177\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 7s 259ms/step - loss: 0.2980 - acc: 0.8621 - val_loss: 0.3699 - val_acc: 0.8281\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.3125 - acc: 0.8524 - val_loss: 0.3852 - val_acc: 0.7917\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 7s 248ms/step - loss: 0.2861 - acc: 0.8691 - val_loss: 0.3682 - val_acc: 0.8438\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.2711 - acc: 0.8766 - val_loss: 0.3786 - val_acc: 0.8385\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2697 - acc: 0.8825 - val_loss: 0.4237 - val_acc: 0.8073\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: 0.2591 - acc: 0.8815 - val_loss: 0.3709 - val_acc: 0.8438\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2726 - acc: 0.8739 - val_loss: 0.4271 - val_acc: 0.8073\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 7s 259ms/step - loss: 0.2743 - acc: 0.8637 - val_loss: 0.4513 - val_acc: 0.7396\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.2357 - acc: 0.8922 - val_loss: 0.4258 - val_acc: 0.8177\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.3050 - acc: 0.8605 - val_loss: 0.3735 - val_acc: 0.7969\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 8s 265ms/step - loss: 0.2316 - acc: 0.8944 - val_loss: 0.3801 - val_acc: 0.8229\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 269ms/step - loss: 0.1995 - acc: 0.9106 - val_loss: 0.3887 - val_acc: 0.8438\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.2099 - acc: 0.9089 - val_loss: 0.3935 - val_acc: 0.8333\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.940476\n",
      "ACC: 0.862069\n",
      "MCC : 0.728683\n",
      "TPR:0.816667\n",
      "FPR:0.089286\n",
      "Pre:0.907407\n",
      "F1:0.859649\n",
      "--------------------------\n",
      "\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 61s 2s/step - loss: 0.5521 - acc: 0.7252 - val_loss: 0.5145 - val_acc: 0.7656\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.4204 - acc: 0.8130 - val_loss: 0.3953 - val_acc: 0.8125\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.4178 - acc: 0.8103 - val_loss: 0.3811 - val_acc: 0.8333\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.3804 - acc: 0.8297 - val_loss: 0.3828 - val_acc: 0.8333\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3774 - acc: 0.8319 - val_loss: 0.3485 - val_acc: 0.8333\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.3965 - acc: 0.8222 - val_loss: 0.3579 - val_acc: 0.8281\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 7s 255ms/step - loss: 0.3730 - acc: 0.8335 - val_loss: 0.3410 - val_acc: 0.8333\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 7s 252ms/step - loss: 0.3633 - acc: 0.8314 - val_loss: 0.3749 - val_acc: 0.8281\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.3366 - acc: 0.8416 - val_loss: 0.3310 - val_acc: 0.8542\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 8s 268ms/step - loss: 0.3259 - acc: 0.8518 - val_loss: 0.3174 - val_acc: 0.8542\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.3124 - acc: 0.8561 - val_loss: 0.4352 - val_acc: 0.8385\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 8s 270ms/step - loss: 0.3151 - acc: 0.8529 - val_loss: 0.3269 - val_acc: 0.8594\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.3114 - acc: 0.8588 - val_loss: 0.3366 - val_acc: 0.8490\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.2794 - acc: 0.8707 - val_loss: 0.3064 - val_acc: 0.8750\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2780 - acc: 0.8631 - val_loss: 0.3174 - val_acc: 0.8802\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2715 - acc: 0.8750 - val_loss: 0.3103 - val_acc: 0.8646\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.2595 - acc: 0.8761 - val_loss: 0.3139 - val_acc: 0.8854\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2536 - acc: 0.8895 - val_loss: 0.3657 - val_acc: 0.8438\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.2756 - acc: 0.8739 - val_loss: 0.3021 - val_acc: 0.8750\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.2303 - acc: 0.8955 - val_loss: 0.4723 - val_acc: 0.8333\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2205 - acc: 0.8836 - val_loss: 0.2973 - val_acc: 0.8958\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 8s 264ms/step - loss: 0.2972 - acc: 0.8702 - val_loss: 0.3855 - val_acc: 0.8229\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 7s 258ms/step - loss: 0.3022 - acc: 0.8540 - val_loss: 0.4244 - val_acc: 0.8229\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 8s 260ms/step - loss: 0.2502 - acc: 0.8798 - val_loss: 0.3215 - val_acc: 0.8906\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 7s 256ms/step - loss: 0.2554 - acc: 0.8879 - val_loss: 0.3398 - val_acc: 0.8542\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.2184 - acc: 0.9014 - val_loss: 0.4212 - val_acc: 0.8594\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 7s 254ms/step - loss: 0.2015 - acc: 0.9089 - val_loss: 0.3695 - val_acc: 0.8802\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 8s 259ms/step - loss: 0.1758 - acc: 0.9106 - val_loss: 0.4816 - val_acc: 0.8802\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 7s 253ms/step - loss: 0.1425 - acc: 0.9370 - val_loss: 0.4527 - val_acc: 0.8542\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 7s 251ms/step - loss: 0.1679 - acc: 0.9192 - val_loss: 0.4389 - val_acc: 0.8802\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 8s 266ms/step - loss: 0.1603 - acc: 0.9310 - val_loss: 0.4291 - val_acc: 0.8594\n",
      "--------------------------\n",
      "\n",
      "AUC: 0.966808\n",
      "ACC: 0.909483\n",
      "MCC : 0.827976\n",
      "TPR:0.816514\n",
      "FPR:0.008130\n",
      "Pre:0.988889\n",
      "F1:0.894472\n",
      "--------------------------\n",
      "\n",
      "mean AUC: 0.962681\n",
      "mean ACC: 0.903084\n",
      "mean MCC : 0.811730\n",
      "mean TPR:0.854473\n",
      "mean FPR:0.048096\n",
      "mean Pre:0.947210\n",
      "mean F1:0.896841\n",
      "Epoch 1/100\n",
      "29/29 [==============================] - 62s 2s/step - loss: 0.5916 - acc: 0.7188 - val_loss: 0.3574 - val_acc: 0.8438\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 8s 271ms/step - loss: 0.4469 - acc: 0.7920 - val_loss: 0.3609 - val_acc: 0.8542\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 8s 262ms/step - loss: 0.4370 - acc: 0.8017 - val_loss: 0.2783 - val_acc: 0.8802\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 8s 261ms/step - loss: 0.4183 - acc: 0.8136 - val_loss: 0.3424 - val_acc: 0.8490\n",
      "Epoch 5/100\n",
      "26/29 [=========================>....] - ETA: 0s - loss: 0.3833 - acc: 0.8245"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "dataset_name = 'EC2'\n",
    "for rep in range(3):\n",
    "    n_splits = 10\n",
    "    TPRs =  np.zeros(n_splits)\n",
    "    FPRs = np.zeros(n_splits)\n",
    "    Precs = np.zeros(n_splits)\n",
    "    ACCs = np.zeros(n_splits)\n",
    "    F1s = np.zeros(n_splits)\n",
    "    MCCs = np.zeros(n_splits)\n",
    "    AUCs = np.zeros(n_splits)\n",
    "     \n",
    "    count = 0\n",
    "    for split in range(n_splits):\n",
    "        train_pairs_file = 'CV/train'+str(rep)+'-'+str(split)\n",
    "        test_pairs_file = 'CV/test'+str(rep)+'-'+str(split)\n",
    "        valid_pairs_file = 'CV/valid'+str(rep)+'-'+str(split)\n",
    "\n",
    "        batch_size = 64\n",
    "        train_generator = DataGenerator(   train_pairs_file,batch_size = batch_size )\n",
    "        test_generator = DataGenerator(   test_pairs_file,batch_size = batch_size)\n",
    "        valid_generator = DataGenerator(   valid_pairs_file,batch_size = batch_size)\n",
    "         \n",
    "        # model = build_model_without_att()\n",
    "        model = build_model()\n",
    "        save_model_name = 'CV/fusion_sent_GoplusSeq'+str(rep)+'-'+str(split) + '.hdf5'\n",
    "        \n",
    "        earlyStopping = EarlyStopping(monitor='val_acc', patience=10, verbose=0, mode='max')\n",
    "        save_checkpoint = ModelCheckpoint(save_model_name, save_best_only=True, monitor='val_acc', mode='max', save_weights_only=True)\n",
    "\n",
    "         \n",
    "        # validation_data = (valid_X, valid_Y),  verbose=1,callbacks=[earlyStopping, save_checkpoint]\n",
    "        #  max_queue_size=16, workers=8, use_multiprocessing=True,\n",
    "        # validation_data=valid_generator,callbacks=[earlyStopping, save_checkpoint] \n",
    "        hist = model.fit_generator(generator=train_generator,\n",
    "                    epochs = 100,verbose=1,validation_data=valid_generator,callbacks=[earlyStopping, save_checkpoint])\n",
    "         \n",
    "        \n",
    "        # model = load_model(save_model_name)\n",
    "        model.load_weights(save_model_name)\n",
    "        with open(test_pairs_file, 'r') as f:\n",
    "            test_ppi_pairs  =  f.readlines()\n",
    "\n",
    "        test_len = len(test_ppi_pairs) \n",
    "        list_IDs_temp = np.arange(test_len)\n",
    "\n",
    "        test_x, y_test = test_generator.all_data(list_IDs_temp)\n",
    "\n",
    "        y_pred_prob = model.predict(test_x)\n",
    "\n",
    "       \n",
    "        y_pred = (y_pred_prob > 0.5)\n",
    "        auc = metrics.roc_auc_score(y_test, y_pred_prob) \n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        pre = precision_score(y_test, y_pred)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        precision, recall, _thresholds = metrics.precision_recall_curve(y_test, y_pred_prob)\n",
    "        pr_auc = metrics.auc(recall, precision)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        total=tn+fp+fn+tp\n",
    "        sen = float(tp)/float(tp+fn)\n",
    "        sps = float(tn)/float((tn+fp))\n",
    "\n",
    "        tpr = float(tp)/float(tp+fn)\n",
    "        fpr = float(fp)/float((tn+fp))\n",
    "        print('--------------------------\\n')\n",
    "        print ('AUC: %f' % auc)\n",
    "        print ('ACC: %f' % acc) \n",
    "        # print(\"PRAUC: %f\" % pr_auc)\n",
    "        print ('MCC : %f' % mcc)\n",
    "        # print ('SEN: %f' % sen)\n",
    "        # print ('SEP: %f' % sps)\n",
    "        print('TPR:%f'%tpr)\n",
    "        print('FPR:%f'%fpr)\n",
    "        print('Pre:%f'%pre)\n",
    "        print('F1:%f'%f1)\n",
    "        print('--------------------------\\n')\n",
    "        TPRs[count] = tpr\n",
    "        FPRs[count] = fpr\n",
    "        Precs[count] =pre\n",
    "        ACCs[count] =acc\n",
    "        F1s[count] =f1\n",
    "        MCCs[count] =mcc\n",
    "        AUCs[count] =auc\n",
    "        count += 1\n",
    "        del test_x\n",
    "        del y_test\n",
    "    print ('mean AUC: %f' % np.mean(AUCs))\n",
    "    print ('mean ACC: %f' % np.mean(ACCs)) \n",
    "    print ('mean MCC : %f' % np.mean(MCCs))\n",
    "    print('mean TPR:%f'% np.mean(TPRs))\n",
    "    print('mean FPR:%f'% np.mean(FPRs))\n",
    "    print('mean Pre:%f'% np.mean(Precs))\n",
    "    print('mean F1:%f'% np.mean(F1s))\n",
    "    np.savez('fusion_sent_seq_and_go__incep_'+str(rep), AUCs=AUCs, ACCs=ACCs, MCCs=MCCs, TPRs = TPRs, FPRs=FPRs, Precs=Precs, F1s=F1s)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.969969127009348 0.9177411828273897 0.8424223155260361 0.8577609821879515 0.02400262365122591 0.9730605667321175 0.9099493945703878\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "aucs = 0\n",
    "accs = 0\n",
    "mccs = 0\n",
    "tprs = 0\n",
    "fprs = 0\n",
    "pres = 0\n",
    "f1s  = 0\n",
    "for i in range(3):\n",
    "    for j in range(10):\n",
    "        results_file = 'CV/sent_fusion_'+str(i)+'-'+str(j)+'.npz'\n",
    "        results = np.load(results_file)\n",
    "        aucs += results['AUCs']\n",
    "        accs += results['ACCs']\n",
    "        mccs += results['MCCs']\n",
    "        tprs += results['TPRs']\n",
    "        fprs += results['FPRs']\n",
    "        pres += results['Precs']\n",
    "        f1s += results['F1s']\n",
    "        \n",
    "print(aucs/30, accs/30, mccs/30, tprs/30, fprs/30, pres/30, f1s/30)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
